#!/usr/bin/env bash
set -euo pipefail

PROJECT_ROOT="/mnt/extremessd10tb/borisiuk/open-unlearning"
cd "${PROJECT_ROOT}"

export MASTER_PORT=$(python - <<'PYCODE'
import socket
s = socket.socket(); s.bind(('', 0))
print(s.getsockname()[1])
s.close()
PYCODE
)

########################################
# 2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ —Å–ø–ª–∏—Ç–æ–≤
########################################
model="Llama-3.1-8B-Instruct"
orig_model_path="${PROJECT_ROOT}/saves/finetune/llama3.1-8b_full_2ep_ft_popqa"

# —Å–ø–ª–∏—Ç—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å unlearning
orig_splits=("rare_forget10" "popular_forget10" ) #
# –∞–ª–≥–æ—Ä–∏—Ç–º—ã unlearning
trainers=("GradAscent" "GradDiff" "NPO" "RMU" ) #
# —Å–ø–ª–∏—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
eval_splits=(
  "duplicate_rare_forget10"
  "duplicate_popular_forget10"
  "rare_forget10"
  "popular_forget10"
  "retain_intersection90"
)

########################################
# 3. –§–∞–π–ª –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
########################################
metrics_dir="${PROJECT_ROOT}/saves/eval/metrics_unlear_${model}"
mkdir -p "${metrics_dir}"
metrics_file="${metrics_dir}/res.txt"

# --- —Å–±—Ä–æ—Å–∏–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª ---
{
  echo "==== Metrics for ${model} ===="
  echo ""
} > "${metrics_file}"

########################################
# 4. Eval –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π ft-–º–æ–¥–µ–ª–∏
########################################
echo
echo ">>> EVAL ORIGINAL FT MODEL"
echo "==== ${model}_Original ====" >> "${metrics_file}"
for split in "${eval_splits[@]}"; do
  echo "--- on split: ${split} ---"
  echo "*** ${split} ***" >> "${metrics_file}"

  out_dir="${PROJECT_ROOT}/saves/eval/eval_final_experiments/${model}/Original/on_${split}"
  mkdir -p "${out_dir}"

  set +e
  CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
    --config-name=eval.yaml \
    experiment=eval/popqa/default \
    forget_split="${split}" \
    task_name="popqa_${model}_Original" \
    paths.output_dir="${out_dir}" \
    model.model_args.pretrained_model_name_or_path="${orig_model_path}" \
    2>&1 | tee "${out_dir}/eval_results.txt"
  exit_code=$?
  set -e

  if [ $exit_code -ne 0 ]; then
    msg="üö® Original eval failed on split ${split} (code ${exit_code})"
    echo "${msg}"
    echo "${msg}" >> "${metrics_file}"
  else
    if grep -q "^Result for metric" "${out_dir}/eval_results.txt"; then
      grep "^Result for metric" "${out_dir}/eval_results.txt" >> "${metrics_file}"
    else
      msg="‚ö†Ô∏è  No metrics found for Original on split ${split}"
      echo "${msg}"
      echo "${msg}" >> "${metrics_file}"
    fi
  fi

  echo "" >> "${metrics_file}"
done

########################################
# 5. Eval fine-tuned (unlearned) –º–æ–¥–µ–ª–µ–π
########################################
for orig in "${orig_splits[@]}"; do
  for trainer in "${trainers[@]}"; do
    task_name="popqa_${model}_${orig}_${trainer}"
    model_path="${PROJECT_ROOT}/saves/unlearn/${task_name}"

    if [ ! -d "${model_path}" ]; then
      echo "‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—é ${task_name}: –ø–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
      continue
    fi

    echo
    echo ">>> EVAL ${task_name}"
    echo "==== ${task_name} ====" >> "${metrics_file}"

    for split in "${eval_splits[@]}"; do
      echo "--- on split: ${split} ---"
      echo "*** ${split} ***" >> "${metrics_file}"

      out_dir="${PROJECT_ROOT}/saves/eval/eval_final_experiments/${model}/${orig}_${trainer}/on_${split}"
      mkdir -p "${out_dir}"

      set +e
      CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
        --config-name=eval.yaml \
        experiment=eval/popqa/default \
        forget_split="${split}" \
        task_name="${task_name}" \
        paths.output_dir="${out_dir}" \
        model.model_args.pretrained_model_name_or_path="${model_path}" \
        2>&1 | tee "${out_dir}/eval_results.txt"
      exit_code=$?
      set -e

      if [ $exit_code -ne 0 ]; then
        msg="üö® Eval failed for ${task_name} on split ${split} (code ${exit_code})"
        echo "${msg}"
        echo "${msg}" >> "${metrics_file}"
      else
        if grep -q "^Result for metric" "${out_dir}/eval_results.txt"; then
          grep "^Result for metric" "${out_dir}/eval_results.txt" >> "${metrics_file}"
        else
          msg="‚ö†Ô∏è  No metrics found for ${task_name} on split ${split}"
          echo "${msg}"
          echo "${msg}" >> "${metrics_file}"
        fi
      fi

      echo "" >> "${metrics_file}"
    done
  done
done

# #!/usr/bin/env bash
# set -euo pipefail

# ########################################
# # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–æ—Ä—Ç –¥–ª—è accelerate
# ########################################
# PROJECT_ROOT="/mnt/extremessd10tb/borisiuk/open-unlearning"
# cd "${PROJECT_ROOT}"

# export MASTER_PORT=$(python - <<'PYCODE'
# import socket
# s = socket.socket(); s.bind(('', 0))
# print(s.getsockname()[1])
# s.close()
# PYCODE
# )

# ########################################
# # 2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏ —Å–ø–ª–∏—Ç–æ–≤
# ########################################
# model="Llama-3.2-1B-Instruct"
# orig_model_path="${PROJECT_ROOT}/saves/finetune/llama3.2-1b_full_5ep_ft_popqa"

# orig_splits=( "rare_forget10" "popular_forget10" )
# trainers=( "GradAscent" "GradDiff" "NPO" "RMU" )
# eval_splits=(
#   "duplicate_rare_forget10"
#   "duplicate_popular_forget10"
#   "retain_intersection90"
#   # "rare_forget10"
#   # "popular_forget10"
# )

# ########################################
# # 3. –§–∞–π–ª –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
# ########################################
# metrics_dir="${PROJECT_ROOT}/saves/eval/metrics_unlear_${model}"
# mkdir -p "${metrics_dir}"
# metrics_file="${metrics_dir}/res.txt"

# # –°–±—Ä–æ—Å —Å—Ç–∞—Ä–æ–≥–æ
# {
#   echo "==== Metrics for ${model} ===="
#   echo ""
# } > "${metrics_file}"

# ########################################
# # 4. Eval fine-tuned (unlearned) –º–æ–¥–µ–ª–µ–π
# ########################################
# for orig in "${orig_splits[@]}"; do
#   for trainer in "${trainers[@]}"; do
#     task_name="popqa_${model}_${orig}_${trainer}"
#     model_path="${PROJECT_ROOT}/saves/unlearn/${task_name}"

#     if [ ! -d "${model_path}" ]; then
#       echo "‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞—é ${task_name}: –ø–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
#       continue
#     fi

#     echo
#     echo ">>> EVAL ${task_name}"
#     echo "==== ${task_name} ====" >> "${metrics_file}"

#     for split in "${eval_splits[@]}"; do
#       echo "--- on split: ${split} ---"
#       echo "*** ${split} ***" >> "${metrics_file}"

#       out_dir="${PROJECT_ROOT}/saves/eval/eval_final_experiments/${model}/${orig}_${trainer}/on_${split}"
#       mkdir -p "${out_dir}"

#       # –∑–∞–ø—É—Å–∫–∞–µ–º —É—Å–∫–æ—Ä–µ–Ω–Ω–æ, –ª–æ–≤–∏–º —Å—Ç–∞—Ç—É—Å
#       set +e
#       CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
#         --config-name=eval.yaml \
#         experiment=eval/popqa/default \
#         forget_split="${split}" \
#         task_name="${task_name}" \
#         paths.output_dir="${out_dir}" \
#         model.model_args.pretrained_model_name_or_path="${model_path}" \
#         2>&1 | tee "${out_dir}/eval_results.txt"
#       exit_code=$?
#       set -e

#       if [ $exit_code -ne 0 ]; then
#         err_msg="üö® Eval failed for ${task_name} on split ${split}, exit code ${exit_code}"
#         echo "${err_msg}"
#         echo "${err_msg}" >> "${metrics_file}"
#       else
#         # –ø–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ ‚Äî –Ω–µ –ø–∞–¥–∞–µ–º, –¥–∞–∂–µ –µ—Å–ª–∏ grep –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥—ë—Ç
#         if grep -q "^Result for metric" "${out_dir}/eval_results.txt"; then
#           grep "^Result for metric" "${out_dir}/eval_results.txt" >> "${metrics_file}"
#         else
#           no_msg="‚ö†Ô∏è  No metrics found for ${task_name} on split ${split}"
#           echo "${no_msg}"
#           echo "${no_msg}" >> "${metrics_file}"
#         fi
#       fi

#       echo "" >> "${metrics_file}"
#     done
#   done
# done









# #!/usr/bin/env bash
# set -euo pipefail

# #
# # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–æ—Ä—Ç –¥–ª—è accelerate
# #
# PROJECT_ROOT="/mnt/extremessd10tb/borisiuk/open-unlearning"
# cd "${PROJECT_ROOT}"

# export MASTER_PORT=$(python - <<'PYCODE'
# import socket
# s = socket.socket(); s.bind(('', 0))
# print(s.getsockname()[1])
# s.close()
# PYCODE
# )

# #
# # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π –∏ —Å–ø–ª–∏—Ç–æ–≤
# #
# model="Llama-3.2-1B-Instruct"
# orig_model_path="${PROJECT_ROOT}/saves/finetune/llama3.2-1b_full_5ep_ft_popqa"

# # unlearning sets (—Ç–æ–ª—å–∫–æ —ç—Ç–∏ –¥–≤–µ)
# orig_splits=( "rare_forget10" "popular_forget10" )

# # –≤—Å–µ —Å–ø–ª–∏—Ç—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–µ—Ä–∏—Ç—å
# eval_splits=(
#   "rare_forget10"
#   "popular_forget10"
#   "duplicate_rare_forget10"
#   "duplicate_popular_forget10"
#   "retain_intersection90"
# )

# # Unlearning –∞–ª–≥–æ—Ä–∏—Ç–º—ã
# trainers=( "GradAscent" "GradDiff" "NPO" "RMU" )

# #
# # –§–∞–π–ª –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# #
# metrics_dir="${PROJECT_ROOT}/saves/eval/metrics_unlear_${model}"
# mkdir -p "${metrics_dir}"
# metrics_file="${metrics_dir}/res.txt"

# # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Ñ–∞–π–ª–µ
# # echo "==== ${model} ORIGINAL MODELS ====" > "${metrics_file}"

# # #
# # 1) Eval –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (rare_forget10, popular_forget10) –Ω–∞ –≤—Å–µ—Ö —Å–ø–ª–∏—Ç–∞—Ö
# #
# for orig in "${orig_splits[@]}"; do
#   echo "*** MODEL: popqa_${model}_${orig}_Original ***" | tee -a "${metrics_file}"
#   for split in "${eval_splits[@]}"; do
#     echo "*** eval on split ${split} ***" | tee -a "${metrics_file}"

#     out_dir="${PROJECT_ROOT}/saves/eval/eval_final_experiments/${model}/${orig}/Original_on_${split}"
#     mkdir -p "${out_dir}"

#     CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
#       --config-name=eval.yaml \
#       experiment=eval/popqa/default \
#       forget_split="${split}" \
#       task_name="popqa_${model}_${orig}_Original" \
#       paths.output_dir="${out_dir}" \
#       model.model_args.pretrained_model_name_or_path="${orig_model_path}" \
#       2>&1 | tee "${out_dir}/eval_results.txt" | tee -a "${metrics_file}"

#     echo "" | tee -a "${metrics_file}"
#   done
# done

# #
# # 2) Eval unlearn-–º–æ–¥–µ–ª–µ–π (–∫–∞–∫ –±—ã–ª–æ —Ä–∞–Ω—å—à–µ)
# #
# for split in "${eval_splits[@]}"; do
#   out_root="${PROJECT_ROOT}/saves/eval/eval_final_experiments/${model}/${split}"
#   mkdir -p "${out_root}"

#   for trainer in "${trainers[@]}"; do
#     task_name="popqa_${model}_${split}_${trainer}"
#     model_path="${PROJECT_ROOT}/saves/unlearn/${task_name}"
#     eval_dir="${out_root}/${task_name}"
#     mkdir -p "${eval_dir}"

#     echo "=== EVAL ${trainer}: ${task_name} on split ${split} ==="
#     CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
#       --config-name=eval.yaml \
#       experiment=eval/popqa/default \
#       forget_split="${split}" \
#       task_name="${task_name}" \
#       paths.output_dir="${eval_dir}" \
#       model.model_args.pretrained_model_name_or_path="${model_path}" \
#       2>&1 | tee "${eval_dir}/eval_results.txt"
#     echo
#   done
# done









# #!/usr/bin/env bash
# set -euo pipefail

# # —Å–ª—É—á–∞–π–Ω—ã–π —Å–≤–æ–±–æ–¥–Ω—ã–π –ø–æ—Ä—Ç –¥–ª—è accelerate (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
# export MASTER_PORT=$(python - <<'PYCODE'
# import socket
# s = socket.socket(); s.bind(('', 0))
# print(s.getsockname()[1])
# s.close()
# PYCODE
# )

# model="Llama-3.2-1B-Instruct" #"Llama-3.1-8B-Instruct"
# orig_model_path="/mnt/extremessd10tb/borisiuk/open-unlearning/saves/finetune/llama3.2-1b_full_5ep_ft_popqa"

# # Unlearning algorithms (–∏–º–µ–Ω–∞ –ø–∞–ø–æ–∫ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å task_name)
# trainers=( "GradAscent" "GradDiff" "NPO" "RMU" )

# # –í—Å–µ —Å–ø–ª–∏—Ç—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ö–æ—Ç–∏–º –¥–µ–ª–∞—Ç—å eval
# eval_splits=(
#   "rare_forget10"
#   "popular_forget10"
#   "duplicate_rare_forget10"
#   "duplicate_popular_forget10"
#   "retain_intersection90"
# )

# for split in "${eval_splits[@]}"; do
#   out_root="saves/eval/eval_final_experiments/${model}/${split}"
#   mkdir -p "${out_root}"

#   ### 1) Eval –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π finetuned –º–æ–¥–µ–ª–∏ ###
#   task_name="popqa_${model}_${split}_Original"
#   eval_dir="${out_root}/${task_name}"
#   mkdir -p "${eval_dir}"

#   echo "=== EVAL ORIGINAL: $task_name on split $split ==="
#   CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
#     --config-name=eval.yaml \
#     experiment=eval/popqa/default \
#     forget_split=${split} \
#     task_name=${task_name} \
#     paths.output_dir=${eval_dir} \
#     model.model_args.pretrained_model_name_or_path=${orig_model_path} \
#     2>&1 | tee "${eval_dir}/${model}_eval_results.txt"
#   echo

#   ### 2) Eval –∫–∞–∂–¥–æ–π unlearn-–º–æ–¥–µ–ª–∏ ###
#   for trainer in "${trainers[@]}"; do
#     task_name="popqa_${model}_${split}_${trainer}"
#     model_path="saves/unlearn/${task_name}"
#     eval_dir="${out_root}/${task_name}"
#     mkdir -p "${eval_dir}"

#     echo "=== EVAL ${trainer}: $task_name on split $split ==="
#     CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes=1 src/eval.py \
#       --config-name=eval.yaml \
#       experiment=eval/popqa/default \
#       forget_split=${split} \
#       task_name=${task_name} \
#       paths.output_dir=${eval_dir} \
#       model.model_args.pretrained_model_name_or_path=${model_path} \
#       2>&1 | tee "${eval_dir}/${model}_${trainer}_eval_results.txt"
#     echo
#   done
# done