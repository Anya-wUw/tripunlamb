{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dafcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolve labels → QID: 100%|██████████| 25972/25972 [1:55:12<00:00,  3.76it/s]  \n",
      "Fetch sitelinks: 100%|██████████| 25857/25857 [3:22:42<00:00,  2.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved subject labels to QIDs: 99.9%\n",
      "Resolved object  labels to QIDs: 99.9%\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.csv\n",
      "ℹ️ Caches: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.label2qid.json, /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.sitelinks_cache.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ===== Paths =====\n",
    "INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics.csv\"\n",
    "OUTPUT_CSV = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.csv\"\n",
    "\n",
    "# Кэши рядом с OUTPUT\n",
    "CACHE_LABEL2QID = Path(OUTPUT_CSV).with_suffix(\".label2qid.json\")\n",
    "CACHE_SITELINKS = Path(OUTPUT_CSV).with_suffix(\".sitelinks_cache.json\")\n",
    "\n",
    "# ===== Settings =====\n",
    "LANGS = [\"en\"]   # порядок поиска меток , \"ru\", \"de\"\n",
    "SLEEP_BETWEEN = 0.05         # чтобы не ловить 429\n",
    "TIMEOUT = 20\n",
    "\n",
    "QID_RE = re.compile(r\"Q\\d+\")\n",
    "\n",
    "# ===== HTTP session =====\n",
    "def build_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"UNLamb-Wikidata/1.0 (contact: youremail@example.com)\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    })\n",
    "    return sess\n",
    "\n",
    "# ===== Caching helpers =====\n",
    "def load_json(path):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_json(path, data):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ===== QID / sitelinks =====\n",
    "def extract_qid(val):\n",
    "    \"\"\"Если в строке уже есть QID — вернём его, иначе None.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    s = str(val)\n",
    "    m = QID_RE.search(s)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def search_wikidata_qid(label, session, langs=LANGS, timeout=TIMEOUT):\n",
    "    \"\"\"\n",
    "    Ищем QID по текстовому label через wbsearchentities.\n",
    "    1) пробуем найти точное совпадение label (case-insensitive)\n",
    "    2) иначе берём первый результат.\n",
    "    Возвращает QID или None.\n",
    "    \"\"\"\n",
    "    label_stripped = str(label).strip()\n",
    "    if not label_stripped:\n",
    "        return None\n",
    "\n",
    "    # частый кейс: в строке прямо QID\n",
    "    q = extract_qid(label_stripped)\n",
    "    if q:\n",
    "        return q\n",
    "\n",
    "    for lang in langs:\n",
    "        params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"language\": lang,\n",
    "            \"uselang\": lang,\n",
    "            \"type\": \"item\",\n",
    "            \"search\": label_stripped,\n",
    "            \"limit\": 5,\n",
    "        }\n",
    "        try:\n",
    "            r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            results = data.get(\"search\", []) or []\n",
    "\n",
    "            if not results:\n",
    "                continue\n",
    "\n",
    "            # 1) ищем точное совпадение по label\n",
    "            lower = label_stripped.lower()\n",
    "            for item in results:\n",
    "                if item.get(\"label\", \"\").lower() == lower:\n",
    "                    return item.get(\"id\")\n",
    "\n",
    "            # 2) иначе берём первый\n",
    "            return results[0].get(\"id\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_sitelinks_count(qid, session, timeout=TIMEOUT):\n",
    "    \"\"\"Количество sitelinks по QID.\"\"\"\n",
    "    if not qid:\n",
    "        return 0\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=timeout)\n",
    "        if r.status_code == 404:\n",
    "            return 0\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        ent = data.get(\"entities\", {}).get(qid, {})\n",
    "        return len(ent.get(\"sitelinks\", {}))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ===== Main =====\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    if \"subject\" not in df.columns or \"object\" not in df.columns:\n",
    "        raise ValueError(\"Ожидаю колонки 'subject' и 'object' в датасете.\")\n",
    "\n",
    "    session = build_session()\n",
    "\n",
    "    # Соберём уникальные строки\n",
    "    subj_vals = df[\"subject\"].fillna(\"\").astype(str)\n",
    "    obj_vals  = df[\"object\"].fillna(\"\").astype(str)\n",
    "    unique_labels = sorted(set(subj_vals.tolist() + obj_vals.tolist()))\n",
    "\n",
    "    # Загрузим кэши\n",
    "    label2qid = load_json(CACHE_LABEL2QID)\n",
    "    qid2sitelinks = load_json(CACHE_SITELINKS)\n",
    "\n",
    "    # 1) Разрешаем label → QID\n",
    "    to_resolve = [lbl for lbl in unique_labels if lbl and lbl not in label2qid]\n",
    "    if to_resolve:\n",
    "        for lbl in tqdm(to_resolve, desc=\"Resolve labels → QID\"):\n",
    "            qid = search_wikidata_qid(lbl, session)\n",
    "            label2qid[lbl] = qid or \"\"  # пустая строка если не нашли\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        save_json(CACHE_LABEL2QID, label2qid)\n",
    "\n",
    "    # Добавим вспомогательные колонки с QID\n",
    "    df[\"subject_qid\"] = subj_vals.map(lambda s: label2qid.get(s, \"\") or extract_qid(s) or \"\")\n",
    "    df[\"object_qid\"]  = obj_vals.map(lambda s: label2qid.get(s, \"\") or extract_qid(s) or \"\")\n",
    "\n",
    "    # 2) QID → sitelinks\n",
    "    qids = sorted({q for q in pd.concat([df[\"subject_qid\"], df[\"object_qid\"]]).tolist() if q})\n",
    "    to_fetch = [q for q in qids if q not in qid2sitelinks]\n",
    "    if to_fetch:\n",
    "        for q in tqdm(to_fetch, desc=\"Fetch sitelinks\"):\n",
    "            qid2sitelinks[q] = get_sitelinks_count(q, session)\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        save_json(CACHE_SITELINKS, qid2sitelinks)\n",
    "\n",
    "    # 3) Преобразуем в счётчики\n",
    "    df[\"subject_popularity_sitelinks\"] = df[\"subject_qid\"].map(lambda q: int(qid2sitelinks.get(q, 0)))\n",
    "    df[\"object_popularity_sitelinks\"]  = df[\"object_qid\"].map(lambda q: int(qid2sitelinks.get(q, 0)))\n",
    "    df[\"popularity_sitelinks_sum\"]     = df[\"subject_popularity_sitelinks\"] + df[\"object_popularity_sitelinks\"]\n",
    "\n",
    "    # Немного статистики в консоль\n",
    "    resolved_subject = (df[\"subject_qid\"] != \"\").mean() * 100\n",
    "    resolved_object  = (df[\"object_qid\"]  != \"\").mean() * 100\n",
    "    print(f\"Resolved subject labels to QIDs: {resolved_subject:.1f}%\")\n",
    "    print(f\"Resolved object  labels to QIDs: {resolved_object:.1f}%\")\n",
    "\n",
    "    # Сохраняем\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Saved: {OUTPUT_CSV}\")\n",
    "    print(f\"ℹ️ Caches: {CACHE_LABEL2QID}, {CACHE_SITELINKS}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca514ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>PPL_Llama3_1-8B_Instruct</th>\n",
       "      <th>best_gen_Llama_8b_Instract</th>\n",
       "      <th>gen_recall_Llama_8b_Instract</th>\n",
       "      <th>bert_sim_Llama_8b_Instract</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_sim_Gemma_7b_IT</th>\n",
       "      <th>PPL_Zephyr_7B_Beta</th>\n",
       "      <th>best_gen_Zephyr_7b_Beta</th>\n",
       "      <th>gen_recall_Zephyr_7b_Beta</th>\n",
       "      <th>bert_sim_Zephyr_7b_Beta</th>\n",
       "      <th>subject_qid</th>\n",
       "      <th>object_qid</th>\n",
       "      <th>subject_popularity_sitelinks</th>\n",
       "      <th>object_popularity_sitelinks</th>\n",
       "      <th>popularity_sitelinks_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What does JavaScript has use?</td>\n",
       "      <td>web development</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>has use</td>\n",
       "      <td>web development</td>\n",
       "      <td>35865.451403</td>\n",
       "      <td>Front-end web development and scripting language</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665370</td>\n",
       "      <td>7.220331e+09</td>\n",
       "      <td>JavaScript is a scripting language used for cr...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417127</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q386275</td>\n",
       "      <td>157</td>\n",
       "      <td>38</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>aspect-oriented programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>aspect-oriented programming</td>\n",
       "      <td>200.512601</td>\n",
       "      <td>Object-Oriented, Functional programming</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.639641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197448</td>\n",
       "      <td>1.977540e+06</td>\n",
       "      <td>JavaScript is a multiparadigm programming</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.225238</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q30267</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>event-driven programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>event-driven programming</td>\n",
       "      <td>236.456111</td>\n",
       "      <td>Object-oriented, event-driven, and scripting l...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.786737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416374</td>\n",
       "      <td>8.271734e+07</td>\n",
       "      <td>JavaScript follows a multiparadigm programming</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.198329</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q1135914</td>\n",
       "      <td>157</td>\n",
       "      <td>29</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>imperative programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>imperative programming</td>\n",
       "      <td>42.875039</td>\n",
       "      <td>Multi-paradigm: OOP, imperative,</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378432</td>\n",
       "      <td>2.660150e+08</td>\n",
       "      <td>JavaScript's programming paradigm is dynamic</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.404129</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q275596</td>\n",
       "      <td>157</td>\n",
       "      <td>50</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>generic programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>generic programming</td>\n",
       "      <td>386.563954</td>\n",
       "      <td>Multithreaded object-oriented imperative progr...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.439199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541070</td>\n",
       "      <td>2.157036e+10</td>\n",
       "      <td>JavaScript is a multi-paradigm</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332423</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q1051282</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55350</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the location of Moscow Jewish Film Fes...</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>location</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1499.950819</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600137</td>\n",
       "      <td>1.235002e+11</td>\n",
       "      <td>The location of the Moscow Jewish Film Festiva...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540264</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q649</td>\n",
       "      <td>3</td>\n",
       "      <td>332</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55351</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the location of Moscow Jewish Film Fes...</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>location</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1499.950819</td>\n",
       "      <td>Tchaikovsky Passage, Moscow, Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548567</td>\n",
       "      <td>1.235002e+11</td>\n",
       "      <td>The location of the Moscow Jewish Film Festiva...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540264</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q649</td>\n",
       "      <td>3</td>\n",
       "      <td>332</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55352</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the official language of Huesca Intern...</td>\n",
       "      <td>English</td>\n",
       "      <td>Huesca International Film Festival</td>\n",
       "      <td>official language</td>\n",
       "      <td>English</td>\n",
       "      <td>815.949818</td>\n",
       "      <td>Spanish, English</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.337788e+13</td>\n",
       "      <td>The official language of Huesca International ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349974</td>\n",
       "      <td>Q59590889</td>\n",
       "      <td>Q1860</td>\n",
       "      <td>5</td>\n",
       "      <td>392</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55353</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the official language of Huesca Intern...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Huesca International Film Festival</td>\n",
       "      <td>official language</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>2148.108094</td>\n",
       "      <td>Spanish, Aragonese</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698930</td>\n",
       "      <td>1.346292e+09</td>\n",
       "      <td>The official language of Huesca International ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370240</td>\n",
       "      <td>Q59590889</td>\n",
       "      <td>Q1321</td>\n",
       "      <td>5</td>\n",
       "      <td>349</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55354</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the country of Moscow Jewish Film Fest...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>country</td>\n",
       "      <td>Russia</td>\n",
       "      <td>11076.616601</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.581914e+06</td>\n",
       "      <td>The country of Moscow Jewish Film Festival is ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.467451</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q159</td>\n",
       "      <td>3</td>\n",
       "      <td>410</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55355 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file  \\\n",
       "0      technology_programming_language.csv   \n",
       "1      technology_programming_language.csv   \n",
       "2      technology_programming_language.csv   \n",
       "3      technology_programming_language.csv   \n",
       "4      technology_programming_language.csv   \n",
       "...                                    ...   \n",
       "55350                       event_film.csv   \n",
       "55351                       event_film.csv   \n",
       "55352                       event_film.csv   \n",
       "55353                       event_film.csv   \n",
       "55354                       event_film.csv   \n",
       "\n",
       "                                                question  \\\n",
       "0                          What does JavaScript has use?   \n",
       "1        What is the programming paradigm of JavaScript?   \n",
       "2        What is the programming paradigm of JavaScript?   \n",
       "3        What is the programming paradigm of JavaScript?   \n",
       "4        What is the programming paradigm of JavaScript?   \n",
       "...                                                  ...   \n",
       "55350  What is the location of Moscow Jewish Film Fes...   \n",
       "55351  What is the location of Moscow Jewish Film Fes...   \n",
       "55352  What is the official language of Huesca Intern...   \n",
       "55353  What is the official language of Huesca Intern...   \n",
       "55354  What is the country of Moscow Jewish Film Fest...   \n",
       "\n",
       "                            answer                             subject  \\\n",
       "0                  web development                          JavaScript   \n",
       "1      aspect-oriented programming                          JavaScript   \n",
       "2         event-driven programming                          JavaScript   \n",
       "3           imperative programming                          JavaScript   \n",
       "4              generic programming                          JavaScript   \n",
       "...                            ...                                 ...   \n",
       "55350                       Moscow         Moscow Jewish Film Festival   \n",
       "55351                       Moscow         Moscow Jewish Film Festival   \n",
       "55352                      English  Huesca International Film Festival   \n",
       "55353                      Spanish  Huesca International Film Festival   \n",
       "55354                       Russia         Moscow Jewish Film Festival   \n",
       "\n",
       "                   relation                       object  \\\n",
       "0                   has use              web development   \n",
       "1      programming paradigm  aspect-oriented programming   \n",
       "2      programming paradigm     event-driven programming   \n",
       "3      programming paradigm       imperative programming   \n",
       "4      programming paradigm          generic programming   \n",
       "...                     ...                          ...   \n",
       "55350              location                       Moscow   \n",
       "55351              location                       Moscow   \n",
       "55352     official language                      English   \n",
       "55353     official language                      Spanish   \n",
       "55354               country                       Russia   \n",
       "\n",
       "       PPL_Llama3_1-8B_Instruct  \\\n",
       "0                  35865.451403   \n",
       "1                    200.512601   \n",
       "2                    236.456111   \n",
       "3                     42.875039   \n",
       "4                    386.563954   \n",
       "...                         ...   \n",
       "55350               1499.950819   \n",
       "55351               1499.950819   \n",
       "55352                815.949818   \n",
       "55353               2148.108094   \n",
       "55354              11076.616601   \n",
       "\n",
       "                              best_gen_Llama_8b_Instract  \\\n",
       "0       Front-end web development and scripting language   \n",
       "1                Object-Oriented, Functional programming   \n",
       "2      Object-oriented, event-driven, and scripting l...   \n",
       "3                       Multi-paradigm: OOP, imperative,   \n",
       "4      Multithreaded object-oriented imperative progr...   \n",
       "...                                                  ...   \n",
       "55350                                             Moscow   \n",
       "55351                Tchaikovsky Passage, Moscow, Russia   \n",
       "55352                                   Spanish, English   \n",
       "55353                                 Spanish, Aragonese   \n",
       "55354                                             Russia   \n",
       "\n",
       "       gen_recall_Llama_8b_Instract  bert_sim_Llama_8b_Instract  ...  \\\n",
       "0                          1.000000                    0.699027  ...   \n",
       "1                          0.666667                    0.639641  ...   \n",
       "2                          0.666667                    0.786737  ...   \n",
       "3                          0.500000                    0.574319  ...   \n",
       "4                          0.500000                    0.439199  ...   \n",
       "...                             ...                         ...  ...   \n",
       "55350                      1.000000                    1.000000  ...   \n",
       "55351                      1.000000                    0.577317  ...   \n",
       "55352                      1.000000                    0.811516  ...   \n",
       "55353                      1.000000                    0.714963  ...   \n",
       "55354                      1.000000                    1.000000  ...   \n",
       "\n",
       "       bert_sim_Gemma_7b_IT PPL_Zephyr_7B_Beta  \\\n",
       "0                  0.665370       7.220331e+09   \n",
       "1                  0.197448       1.977540e+06   \n",
       "2                  0.416374       8.271734e+07   \n",
       "3                  0.378432       2.660150e+08   \n",
       "4                  0.541070       2.157036e+10   \n",
       "...                     ...                ...   \n",
       "55350              0.600137       1.235002e+11   \n",
       "55351              0.548567       1.235002e+11   \n",
       "55352              1.000000       5.337788e+13   \n",
       "55353              0.698930       1.346292e+09   \n",
       "55354              1.000000       7.581914e+06   \n",
       "\n",
       "                                 best_gen_Zephyr_7b_Beta  \\\n",
       "0      JavaScript is a scripting language used for cr...   \n",
       "1              JavaScript is a multiparadigm programming   \n",
       "2         JavaScript follows a multiparadigm programming   \n",
       "3           JavaScript's programming paradigm is dynamic   \n",
       "4                         JavaScript is a multi-paradigm   \n",
       "...                                                  ...   \n",
       "55350  The location of the Moscow Jewish Film Festiva...   \n",
       "55351  The location of the Moscow Jewish Film Festiva...   \n",
       "55352  The official language of Huesca International ...   \n",
       "55353  The official language of Huesca International ...   \n",
       "55354  The country of Moscow Jewish Film Festival is ...   \n",
       "\n",
       "       gen_recall_Zephyr_7b_Beta  bert_sim_Zephyr_7b_Beta subject_qid  \\\n",
       "0                       0.000000                 0.417127       Q2005   \n",
       "1                       0.333333                 0.225238       Q2005   \n",
       "2                       0.333333                 0.198329       Q2005   \n",
       "3                       0.500000                 0.404129       Q2005   \n",
       "4                       0.000000                 0.332423       Q2005   \n",
       "...                          ...                      ...         ...   \n",
       "55350                   1.000000                 0.540264   Q30124890   \n",
       "55351                   1.000000                 0.540264   Q30124890   \n",
       "55352                   0.000000                 0.349974   Q59590889   \n",
       "55353                   0.000000                 0.370240   Q59590889   \n",
       "55354                   1.000000                 0.467451   Q30124890   \n",
       "\n",
       "       object_qid  subject_popularity_sitelinks  object_popularity_sitelinks  \\\n",
       "0         Q386275                           157                           38   \n",
       "1          Q30267                           157                           33   \n",
       "2        Q1135914                           157                           29   \n",
       "3         Q275596                           157                           50   \n",
       "4        Q1051282                           157                           33   \n",
       "...           ...                           ...                          ...   \n",
       "55350        Q649                             3                          332   \n",
       "55351        Q649                             3                          332   \n",
       "55352       Q1860                             5                          392   \n",
       "55353       Q1321                             5                          349   \n",
       "55354        Q159                             3                          410   \n",
       "\n",
       "      popularity_sitelinks_sum  \n",
       "0                          195  \n",
       "1                          190  \n",
       "2                          186  \n",
       "3                          207  \n",
       "4                          190  \n",
       "...                        ...  \n",
       "55350                      335  \n",
       "55351                      335  \n",
       "55352                      397  \n",
       "55353                      354  \n",
       "55354                      413  \n",
       "\n",
       "[55355 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d54afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment method: index\n",
      "Filled PPL_Phi3_5_mini_Instruct: 100.0% rows\n",
      "Filled best_gen_Phi3_5_mini_Instruct: 99.9% rows\n",
      "Filled gen_recall_Phi3_5_mini_Instruct: 100.0% rows\n",
      "Filled bert_sim_Phi3_5_mini_Instruct: 100.0% rows\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_WITH_PHI35_cols.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>PPL_Llama3_1-8B_Instruct</th>\n",
       "      <th>best_gen_Llama_8b_Instract</th>\n",
       "      <th>gen_recall_Llama_8b_Instract</th>\n",
       "      <th>bert_sim_Llama_8b_Instract</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_sim_Zephyr_7b_Beta</th>\n",
       "      <th>subject_qid</th>\n",
       "      <th>object_qid</th>\n",
       "      <th>subject_popularity_sitelinks</th>\n",
       "      <th>object_popularity_sitelinks</th>\n",
       "      <th>popularity_sitelinks_sum</th>\n",
       "      <th>PPL_Phi3_5_mini_Instruct</th>\n",
       "      <th>best_gen_Phi3_5_mini_Instruct</th>\n",
       "      <th>gen_recall_Phi3_5_mini_Instruct</th>\n",
       "      <th>bert_sim_Phi3_5_mini_Instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What does JavaScript has use?</td>\n",
       "      <td>web development</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>has use</td>\n",
       "      <td>web development</td>\n",
       "      <td>35865.451403</td>\n",
       "      <td>Front-end web development and scripting language</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417127</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q386275</td>\n",
       "      <td>157</td>\n",
       "      <td>38</td>\n",
       "      <td>195</td>\n",
       "      <td>1.109592e+14</td>\n",
       "      <td>Semicolons (;) as statements terminators</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>aspect-oriented programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>aspect-oriented programming</td>\n",
       "      <td>200.512601</td>\n",
       "      <td>Object-Oriented, Functional programming</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.639641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225238</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q30267</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "      <td>4.402190e+09</td>\n",
       "      <td>Object-Oriented Programming (OOP)</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.630127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>event-driven programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>event-driven programming</td>\n",
       "      <td>236.456111</td>\n",
       "      <td>Object-oriented, event-driven, and scripting l...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.786737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198329</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q1135914</td>\n",
       "      <td>157</td>\n",
       "      <td>29</td>\n",
       "      <td>186</td>\n",
       "      <td>8.118268e+05</td>\n",
       "      <td>Object-Oriented Programming (OOP)</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.484787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>imperative programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>imperative programming</td>\n",
       "      <td>42.875039</td>\n",
       "      <td>Multi-paradigm: OOP, imperative,</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.574319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404129</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q275596</td>\n",
       "      <td>157</td>\n",
       "      <td>50</td>\n",
       "      <td>207</td>\n",
       "      <td>1.513656e+11</td>\n",
       "      <td>Multi-paradigm: imperative,</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.566366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology_programming_language.csv</td>\n",
       "      <td>What is the programming paradigm of JavaScript?</td>\n",
       "      <td>generic programming</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>programming paradigm</td>\n",
       "      <td>generic programming</td>\n",
       "      <td>386.563954</td>\n",
       "      <td>Multithreaded object-oriented imperative progr...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.439199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332423</td>\n",
       "      <td>Q2005</td>\n",
       "      <td>Q1051282</td>\n",
       "      <td>157</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "      <td>2.520896e+16</td>\n",
       "      <td>Object-Oriented Programming (OOP)</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.507858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55350</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the location of Moscow Jewish Film Fes...</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>location</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1499.950819</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540264</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q649</td>\n",
       "      <td>3</td>\n",
       "      <td>332</td>\n",
       "      <td>335</td>\n",
       "      <td>6.292632e+17</td>\n",
       "      <td>Moscow, Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55351</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the location of Moscow Jewish Film Fes...</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>location</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>1499.950819</td>\n",
       "      <td>Tchaikovsky Passage, Moscow, Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540264</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q649</td>\n",
       "      <td>3</td>\n",
       "      <td>332</td>\n",
       "      <td>335</td>\n",
       "      <td>6.292632e+17</td>\n",
       "      <td>Moscow, Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55352</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the official language of Huesca Intern...</td>\n",
       "      <td>English</td>\n",
       "      <td>Huesca International Film Festival</td>\n",
       "      <td>official language</td>\n",
       "      <td>English</td>\n",
       "      <td>815.949818</td>\n",
       "      <td>Spanish, English</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349974</td>\n",
       "      <td>Q59590889</td>\n",
       "      <td>Q1860</td>\n",
       "      <td>5</td>\n",
       "      <td>392</td>\n",
       "      <td>397</td>\n",
       "      <td>2.912514e+17</td>\n",
       "      <td>Spanish is the official language used during t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55353</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the official language of Huesca Intern...</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Huesca International Film Festival</td>\n",
       "      <td>official language</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>2148.108094</td>\n",
       "      <td>Spanish, Aragonese</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370240</td>\n",
       "      <td>Q59590889</td>\n",
       "      <td>Q1321</td>\n",
       "      <td>5</td>\n",
       "      <td>349</td>\n",
       "      <td>354</td>\n",
       "      <td>1.603046e+17</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55354</th>\n",
       "      <td>event_film.csv</td>\n",
       "      <td>What is the country of Moscow Jewish Film Fest...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Moscow Jewish Film Festival</td>\n",
       "      <td>country</td>\n",
       "      <td>Russia</td>\n",
       "      <td>11076.616601</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467451</td>\n",
       "      <td>Q30124890</td>\n",
       "      <td>Q159</td>\n",
       "      <td>3</td>\n",
       "      <td>410</td>\n",
       "      <td>413</td>\n",
       "      <td>8.961113e+16</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55355 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file  \\\n",
       "0      technology_programming_language.csv   \n",
       "1      technology_programming_language.csv   \n",
       "2      technology_programming_language.csv   \n",
       "3      technology_programming_language.csv   \n",
       "4      technology_programming_language.csv   \n",
       "...                                    ...   \n",
       "55350                       event_film.csv   \n",
       "55351                       event_film.csv   \n",
       "55352                       event_film.csv   \n",
       "55353                       event_film.csv   \n",
       "55354                       event_film.csv   \n",
       "\n",
       "                                                question  \\\n",
       "0                          What does JavaScript has use?   \n",
       "1        What is the programming paradigm of JavaScript?   \n",
       "2        What is the programming paradigm of JavaScript?   \n",
       "3        What is the programming paradigm of JavaScript?   \n",
       "4        What is the programming paradigm of JavaScript?   \n",
       "...                                                  ...   \n",
       "55350  What is the location of Moscow Jewish Film Fes...   \n",
       "55351  What is the location of Moscow Jewish Film Fes...   \n",
       "55352  What is the official language of Huesca Intern...   \n",
       "55353  What is the official language of Huesca Intern...   \n",
       "55354  What is the country of Moscow Jewish Film Fest...   \n",
       "\n",
       "                            answer                             subject  \\\n",
       "0                  web development                          JavaScript   \n",
       "1      aspect-oriented programming                          JavaScript   \n",
       "2         event-driven programming                          JavaScript   \n",
       "3           imperative programming                          JavaScript   \n",
       "4              generic programming                          JavaScript   \n",
       "...                            ...                                 ...   \n",
       "55350                       Moscow         Moscow Jewish Film Festival   \n",
       "55351                       Moscow         Moscow Jewish Film Festival   \n",
       "55352                      English  Huesca International Film Festival   \n",
       "55353                      Spanish  Huesca International Film Festival   \n",
       "55354                       Russia         Moscow Jewish Film Festival   \n",
       "\n",
       "                   relation                       object  \\\n",
       "0                   has use              web development   \n",
       "1      programming paradigm  aspect-oriented programming   \n",
       "2      programming paradigm     event-driven programming   \n",
       "3      programming paradigm       imperative programming   \n",
       "4      programming paradigm          generic programming   \n",
       "...                     ...                          ...   \n",
       "55350              location                       Moscow   \n",
       "55351              location                       Moscow   \n",
       "55352     official language                      English   \n",
       "55353     official language                      Spanish   \n",
       "55354               country                       Russia   \n",
       "\n",
       "       PPL_Llama3_1-8B_Instruct  \\\n",
       "0                  35865.451403   \n",
       "1                    200.512601   \n",
       "2                    236.456111   \n",
       "3                     42.875039   \n",
       "4                    386.563954   \n",
       "...                         ...   \n",
       "55350               1499.950819   \n",
       "55351               1499.950819   \n",
       "55352                815.949818   \n",
       "55353               2148.108094   \n",
       "55354              11076.616601   \n",
       "\n",
       "                              best_gen_Llama_8b_Instract  \\\n",
       "0       Front-end web development and scripting language   \n",
       "1                Object-Oriented, Functional programming   \n",
       "2      Object-oriented, event-driven, and scripting l...   \n",
       "3                       Multi-paradigm: OOP, imperative,   \n",
       "4      Multithreaded object-oriented imperative progr...   \n",
       "...                                                  ...   \n",
       "55350                                             Moscow   \n",
       "55351                Tchaikovsky Passage, Moscow, Russia   \n",
       "55352                                   Spanish, English   \n",
       "55353                                 Spanish, Aragonese   \n",
       "55354                                             Russia   \n",
       "\n",
       "       gen_recall_Llama_8b_Instract  bert_sim_Llama_8b_Instract  ...  \\\n",
       "0                          1.000000                    0.699027  ...   \n",
       "1                          0.666667                    0.639641  ...   \n",
       "2                          0.666667                    0.786737  ...   \n",
       "3                          0.500000                    0.574319  ...   \n",
       "4                          0.500000                    0.439199  ...   \n",
       "...                             ...                         ...  ...   \n",
       "55350                      1.000000                    1.000000  ...   \n",
       "55351                      1.000000                    0.577317  ...   \n",
       "55352                      1.000000                    0.811516  ...   \n",
       "55353                      1.000000                    0.714963  ...   \n",
       "55354                      1.000000                    1.000000  ...   \n",
       "\n",
       "       bert_sim_Zephyr_7b_Beta subject_qid  object_qid  \\\n",
       "0                     0.417127       Q2005     Q386275   \n",
       "1                     0.225238       Q2005      Q30267   \n",
       "2                     0.198329       Q2005    Q1135914   \n",
       "3                     0.404129       Q2005     Q275596   \n",
       "4                     0.332423       Q2005    Q1051282   \n",
       "...                        ...         ...         ...   \n",
       "55350                 0.540264   Q30124890        Q649   \n",
       "55351                 0.540264   Q30124890        Q649   \n",
       "55352                 0.349974   Q59590889       Q1860   \n",
       "55353                 0.370240   Q59590889       Q1321   \n",
       "55354                 0.467451   Q30124890        Q159   \n",
       "\n",
       "       subject_popularity_sitelinks  object_popularity_sitelinks  \\\n",
       "0                               157                           38   \n",
       "1                               157                           33   \n",
       "2                               157                           29   \n",
       "3                               157                           50   \n",
       "4                               157                           33   \n",
       "...                             ...                          ...   \n",
       "55350                             3                          332   \n",
       "55351                             3                          332   \n",
       "55352                             5                          392   \n",
       "55353                             5                          349   \n",
       "55354                             3                          410   \n",
       "\n",
       "      popularity_sitelinks_sum  PPL_Phi3_5_mini_Instruct  \\\n",
       "0                          195              1.109592e+14   \n",
       "1                          190              4.402190e+09   \n",
       "2                          186              8.118268e+05   \n",
       "3                          207              1.513656e+11   \n",
       "4                          190              2.520896e+16   \n",
       "...                        ...                       ...   \n",
       "55350                      335              6.292632e+17   \n",
       "55351                      335              6.292632e+17   \n",
       "55352                      397              2.912514e+17   \n",
       "55353                      354              1.603046e+17   \n",
       "55354                      413              8.961113e+16   \n",
       "\n",
       "                           best_gen_Phi3_5_mini_Instruct  \\\n",
       "0               Semicolons (;) as statements terminators   \n",
       "1                      Object-Oriented Programming (OOP)   \n",
       "2                      Object-Oriented Programming (OOP)   \n",
       "3                            Multi-paradigm: imperative,   \n",
       "4                      Object-Oriented Programming (OOP)   \n",
       "...                                                  ...   \n",
       "55350                                     Moscow, Russia   \n",
       "55351                                     Moscow, Russia   \n",
       "55352  Spanish is the official language used during t...   \n",
       "55353                                            Spanish   \n",
       "55354                                             Russia   \n",
       "\n",
       "       gen_recall_Phi3_5_mini_Instruct bert_sim_Phi3_5_mini_Instruct  \n",
       "0                             0.000000                      0.071977  \n",
       "1                             0.666667                      0.630127  \n",
       "2                             0.333333                      0.484787  \n",
       "3                             0.500000                      0.566366  \n",
       "4                             0.500000                      0.507858  \n",
       "...                                ...                           ...  \n",
       "55350                         1.000000                      0.941657  \n",
       "55351                         1.000000                      0.941657  \n",
       "55352                         0.000000                      0.339224  \n",
       "55353                         1.000000                      1.000000  \n",
       "55354                         1.000000                      1.000000  \n",
       "\n",
       "[55355 rows x 35 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- входные файлы ---\n",
    "BASE_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_with_popularity.csv\"\n",
    "DONOR_CSV = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/phi3_5_mini_instruct_all_questions_with_metrics.csv\"\n",
    "OUT_CSV   = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_WITH_PHI35_cols.csv\"\n",
    "\n",
    "# --- какие колонки забираем из DONOR ---\n",
    "PHI_COLS = [\n",
    "    \"PPL_Phi3_5_mini_Instruct\",\n",
    "    \"best_gen_Phi3_5_mini_Instruct\",\n",
    "    \"gen_recall_Phi3_5_mini_Instruct\",\n",
    "    \"bert_sim_Phi3_5_mini_Instruct\",\n",
    "]\n",
    "\n",
    "# ==== load ====\n",
    "base  = pd.read_csv(BASE_CSV)\n",
    "donor = pd.read_csv(DONOR_CSV)\n",
    "\n",
    "# Проверка наличия колонок\n",
    "for c in PHI_COLS:\n",
    "    if c not in donor.columns:\n",
    "        raise ValueError(f\"В донорском файле нет колонки: {c}\")\n",
    "if \"question\" not in base.columns or \"answer\" not in base.columns:\n",
    "    raise ValueError(\"В базовом файле ожидаются колонки 'question' и 'answer'.\")\n",
    "if \"question\" not in donor.columns or \"answer\" not in donor.columns:\n",
    "    raise ValueError(\"В донорском файле ожидаются колонки 'question' и 'answer'.\")\n",
    "\n",
    "# ==== сначала пробуем прямое выравнивание по индексу ====\n",
    "use_index_align = False\n",
    "if len(base) == len(donor):\n",
    "    # на всякий случай проверим долю совпадений question по позициям\n",
    "    same_q = (base[\"question\"].astype(str).fillna(\"\") == donor[\"question\"].astype(str).fillna(\"\")).mean()\n",
    "    # если совпадает >= 95% — считаем, что порядок одинаковый\n",
    "    if same_q >= 0.95:\n",
    "        use_index_align = True\n",
    "\n",
    "if use_index_align:\n",
    "    # выравнивание по индексу\n",
    "    for c in PHI_COLS:\n",
    "        base[c] = donor[c].values\n",
    "    method = \"index\"\n",
    "else:\n",
    "    # безопасный merge по (question, answer)\n",
    "    # Сужаем donor до ключей + нужных колонок\n",
    "    donor_slim = donor[[\"question\", \"answer\"] + PHI_COLS].copy()\n",
    "\n",
    "    # Если в donor есть дубликаты по ключу, возьмем первое вхождение\n",
    "    if donor_slim.duplicated([\"question\", \"answer\"]).any():\n",
    "        donor_slim = donor_slim.drop_duplicates([\"question\", \"answer\"], keep=\"first\")\n",
    "\n",
    "    merged = base.merge(donor_slim, on=[\"question\", \"answer\"], how=\"left\", suffixes=(\"\", \"_donor\"))\n",
    "\n",
    "    # На случай, если какие-то из PHI_COLS уже есть в base — перезапишем их одноимёнными из donor\n",
    "    for c in PHI_COLS:\n",
    "        if c in merged.columns:\n",
    "            # уже пришёл из donor с точным именем\n",
    "            pass\n",
    "        elif c + \"_donor\" in merged.columns:\n",
    "            merged[c] = merged[c + \"_donor\"]\n",
    "            merged.drop(columns=[c + \"_donor\"], inplace=True)\n",
    "        else:\n",
    "            # не найдено после merge — создаём пустую колонку\n",
    "            merged[c] = pd.NA\n",
    "\n",
    "    base = merged\n",
    "    method = \"merge(question,answer)\"\n",
    "\n",
    "# ==== отчёт и сохранение ====\n",
    "added_info = {c: base[c].notna().mean()*100 for c in PHI_COLS}\n",
    "print(f\"Alignment method: {method}\")\n",
    "for c, pct in added_info.items():\n",
    "    print(f\"Filled {c}: {pct:.1f}% rows\")\n",
    "\n",
    "# Сохраняем\n",
    "Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "base.to_csv(OUT_CSV, index=False)\n",
    "print(f\"✅ Saved: {OUT_CSV}\")\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef037d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ВХОД/ВЫХОД — при необходимости поменяй путь на свой файл\n",
    "INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_WITH_PHI35_cols.csv\"\n",
    "OUTPUT_CSV = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "\n",
    "# Какие колонки унести в конец\n",
    "TAIL_COLS = [\n",
    "    \"subject_qid\",\n",
    "    \"object_qid\",\n",
    "    \"subject_popularity_sitelinks\",\n",
    "    \"object_popularity_sitelinks\",\n",
    "    \"popularity_sitelinks_sum\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Оставляем только реально существующие из списка (на случай, если чего-то нет)\n",
    "tail_existing = [c for c in TAIL_COLS if c in df.columns]\n",
    "\n",
    "# Новый порядок: все остальные + хвост\n",
    "new_cols = [c for c in df.columns if c not in tail_existing] + tail_existing\n",
    "df = df.reindex(columns=new_cols)\n",
    "\n",
    "# Сортировка по возрастанию popularity_sitelinks_sum (если колонка есть)\n",
    "if \"popularity_sitelinks_sum\" in df.columns:\n",
    "    df = df.sort_values(\"popularity_sitelinks_sum\", ascending=True, kind=\"mergesort\")  # стабильная\n",
    "\n",
    "# Сохранение\n",
    "Path(OUTPUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Saved: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4750bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f603c2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>PPL_Llama3_1-8B_Instruct</th>\n",
       "      <th>best_gen_Llama_8b_Instract</th>\n",
       "      <th>gen_recall_Llama_8b_Instract</th>\n",
       "      <th>bert_sim_Llama_8b_Instract</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_sim_Zephyr_7b_Beta</th>\n",
       "      <th>PPL_Phi3_5_mini_Instruct</th>\n",
       "      <th>best_gen_Phi3_5_mini_Instruct</th>\n",
       "      <th>gen_recall_Phi3_5_mini_Instruct</th>\n",
       "      <th>bert_sim_Phi3_5_mini_Instruct</th>\n",
       "      <th>subject_qid</th>\n",
       "      <th>object_qid</th>\n",
       "      <th>subject_popularity_sitelinks</th>\n",
       "      <th>object_popularity_sitelinks</th>\n",
       "      <th>popularity_sitelinks_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>business_industry.csv</td>\n",
       "      <td>What did book industry said to be the same as?</td>\n",
       "      <td>book publishing</td>\n",
       "      <td>book industry</td>\n",
       "      <td>said to be the same as</td>\n",
       "      <td>book publishing</td>\n",
       "      <td>1865.235713</td>\n",
       "      <td>Publishing was said to be like running a river</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.512184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468156</td>\n",
       "      <td>1.587546e+17</td>\n",
       "      <td>Digital distribution platforms Book industry o...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.615013</td>\n",
       "      <td>Q56560668</td>\n",
       "      <td>Q112165919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>places_city.csv</td>\n",
       "      <td>What did Calais twinned administrative body?</td>\n",
       "      <td>Xiangtan</td>\n",
       "      <td>Calais</td>\n",
       "      <td>twinned administrative body</td>\n",
       "      <td>Xiangtan</td>\n",
       "      <td>46.282780</td>\n",
       "      <td>France – via Lille and Kortrijk (</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016905</td>\n",
       "      <td>1.862347e+15</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.205666</td>\n",
       "      <td>Q87300250</td>\n",
       "      <td>Q113491656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13126</th>\n",
       "      <td>places_city.csv</td>\n",
       "      <td>What is the located in the administrative terr...</td>\n",
       "      <td>Tlajomulco de Zúñiga Municipality</td>\n",
       "      <td>San Agustín (Jalisco)</td>\n",
       "      <td>located in the administrative territorial entity</td>\n",
       "      <td>Tlajomulco de Zúñiga Municipality</td>\n",
       "      <td>3.066345</td>\n",
       "      <td>Mina Grande de los Alamos municipality in N</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.356967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324772</td>\n",
       "      <td>2.733490e+00</td>\n",
       "      <td>Plaza de Armas San Agustín,</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.311132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15089</th>\n",
       "      <td>health_symptom.csv</td>\n",
       "      <td>What is the drug or therapy used for treatment...</td>\n",
       "      <td>propantheline</td>\n",
       "      <td>spasm</td>\n",
       "      <td>drug or therapy used for treatment</td>\n",
       "      <td>propantheline</td>\n",
       "      <td>5.538411</td>\n",
       "      <td>Examples include baclofen, dantrolene</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355749</td>\n",
       "      <td>4.429714e+12</td>\n",
       "      <td>Beta-blockers, muscle relaxants</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.342819</td>\n",
       "      <td>Q65632660</td>\n",
       "      <td>Q95594627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16992</th>\n",
       "      <td>health_disease.csv</td>\n",
       "      <td>What is the health specialty of sleeprunning?</td>\n",
       "      <td>sleep medicine</td>\n",
       "      <td>sleeprunning</td>\n",
       "      <td>health specialty</td>\n",
       "      <td>sleep medicine</td>\n",
       "      <td>2457.903565</td>\n",
       "      <td>Sleep Medicine</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653341</td>\n",
       "      <td>5.246328e+16</td>\n",
       "      <td>Sleep Medicine</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q15762248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27854</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the country of Japan?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>country</td>\n",
       "      <td>Japan</td>\n",
       "      <td>12709.168794</td>\n",
       "      <td>Japan is a country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.744425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636226</td>\n",
       "      <td>4.784381e+14</td>\n",
       "      <td>Japan is a country in East Asia</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.697189</td>\n",
       "      <td>Q17</td>\n",
       "      <td>Q17</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32683</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the country of Russia?</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>country</td>\n",
       "      <td>Russia</td>\n",
       "      <td>11398.500244</td>\n",
       "      <td>Russia is a country in Eastern Europe and Nort...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.672289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595697</td>\n",
       "      <td>8.400571e+15</td>\n",
       "      <td>Russia is a transcontinental country in Northe...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.605403</td>\n",
       "      <td>Q159</td>\n",
       "      <td>Q159</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32742</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Russia?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Russia</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Japan</td>\n",
       "      <td>1651.651733</td>\n",
       "      <td>Russia has relations with:</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202271</td>\n",
       "      <td>7.640358e+17</td>\n",
       "      <td>Multifaceted and complex, with partners</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.091473</td>\n",
       "      <td>Q159</td>\n",
       "      <td>Q17</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30641</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Turkey?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Japan</td>\n",
       "      <td>3224.247831</td>\n",
       "      <td>Turkey has diplomatic relations with 195 UN re...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>2.014042e+18</td>\n",
       "      <td>Relations vary with each country; please speci...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.371126</td>\n",
       "      <td>Q43</td>\n",
       "      <td>Q17</td>\n",
       "      <td>414</td>\n",
       "      <td>410</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30654</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Turkey?</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1523.578146</td>\n",
       "      <td>Turkey has a non-NATO ally relationship with R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268692</td>\n",
       "      <td>5.048042e+17</td>\n",
       "      <td>Friendly, though sometimes strained, due to</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.211534</td>\n",
       "      <td>Q43</td>\n",
       "      <td>Q159</td>\n",
       "      <td>414</td>\n",
       "      <td>410</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55355 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file  \\\n",
       "932    business_industry.csv   \n",
       "2751         places_city.csv   \n",
       "13126        places_city.csv   \n",
       "15089     health_symptom.csv   \n",
       "16992     health_disease.csv   \n",
       "...                      ...   \n",
       "27854     places_country.csv   \n",
       "32683     places_country.csv   \n",
       "32742     places_country.csv   \n",
       "30641     places_country.csv   \n",
       "30654     places_country.csv   \n",
       "\n",
       "                                                question  \\\n",
       "932       What did book industry said to be the same as?   \n",
       "2751        What did Calais twinned administrative body?   \n",
       "13126  What is the located in the administrative terr...   \n",
       "15089  What is the drug or therapy used for treatment...   \n",
       "16992      What is the health specialty of sleeprunning?   \n",
       "...                                                  ...   \n",
       "27854                      What is the country of Japan?   \n",
       "32683                     What is the country of Russia?   \n",
       "32742         What is the diplomatic relation of Russia?   \n",
       "30641         What is the diplomatic relation of Turkey?   \n",
       "30654         What is the diplomatic relation of Turkey?   \n",
       "\n",
       "                                  answer                subject  \\\n",
       "932                      book publishing          book industry   \n",
       "2751                            Xiangtan                 Calais   \n",
       "13126  Tlajomulco de Zúñiga Municipality  San Agustín (Jalisco)   \n",
       "15089                      propantheline                  spasm   \n",
       "16992                     sleep medicine           sleeprunning   \n",
       "...                                  ...                    ...   \n",
       "27854                              Japan                  Japan   \n",
       "32683                             Russia                 Russia   \n",
       "32742                              Japan                 Russia   \n",
       "30641                              Japan                 Turkey   \n",
       "30654                             Russia                 Turkey   \n",
       "\n",
       "                                               relation  \\\n",
       "932                              said to be the same as   \n",
       "2751                        twinned administrative body   \n",
       "13126  located in the administrative territorial entity   \n",
       "15089                drug or therapy used for treatment   \n",
       "16992                                  health specialty   \n",
       "...                                                 ...   \n",
       "27854                                           country   \n",
       "32683                                           country   \n",
       "32742                               diplomatic relation   \n",
       "30641                               diplomatic relation   \n",
       "30654                               diplomatic relation   \n",
       "\n",
       "                                  object  PPL_Llama3_1-8B_Instruct  \\\n",
       "932                      book publishing               1865.235713   \n",
       "2751                            Xiangtan                 46.282780   \n",
       "13126  Tlajomulco de Zúñiga Municipality                  3.066345   \n",
       "15089                      propantheline                  5.538411   \n",
       "16992                     sleep medicine               2457.903565   \n",
       "...                                  ...                       ...   \n",
       "27854                              Japan              12709.168794   \n",
       "32683                             Russia              11398.500244   \n",
       "32742                              Japan               1651.651733   \n",
       "30641                              Japan               3224.247831   \n",
       "30654                             Russia               1523.578146   \n",
       "\n",
       "                              best_gen_Llama_8b_Instract  \\\n",
       "932       Publishing was said to be like running a river   \n",
       "2751                   France – via Lille and Kortrijk (   \n",
       "13126        Mina Grande de los Alamos municipality in N   \n",
       "15089              Examples include baclofen, dantrolene   \n",
       "16992                                     Sleep Medicine   \n",
       "...                                                  ...   \n",
       "27854                                 Japan is a country   \n",
       "32683  Russia is a country in Eastern Europe and Nort...   \n",
       "32742                         Russia has relations with:   \n",
       "30641  Turkey has diplomatic relations with 195 UN re...   \n",
       "30654  Turkey has a non-NATO ally relationship with R...   \n",
       "\n",
       "       gen_recall_Llama_8b_Instract  bert_sim_Llama_8b_Instract  ...  \\\n",
       "932                             0.5                    0.512184  ...   \n",
       "2751                            0.0                    0.260455  ...   \n",
       "13126                           0.5                    0.356967  ...   \n",
       "15089                           0.0                    0.279883  ...   \n",
       "16992                           1.0                    1.000000  ...   \n",
       "...                             ...                         ...  ...   \n",
       "27854                           1.0                    0.744425  ...   \n",
       "32683                           1.0                    0.672289  ...   \n",
       "32742                           0.0                    0.302459  ...   \n",
       "30641                           0.0                    0.119216  ...   \n",
       "30654                           1.0                    0.424669  ...   \n",
       "\n",
       "       bert_sim_Zephyr_7b_Beta PPL_Phi3_5_mini_Instruct  \\\n",
       "932                   0.468156             1.587546e+17   \n",
       "2751                 -0.016905             1.862347e+15   \n",
       "13126                 0.324772             2.733490e+00   \n",
       "15089                 0.355749             4.429714e+12   \n",
       "16992                 0.653341             5.246328e+16   \n",
       "...                        ...                      ...   \n",
       "27854                 0.636226             4.784381e+14   \n",
       "32683                 0.595697             8.400571e+15   \n",
       "32742                 0.202271             7.640358e+17   \n",
       "30641                 0.057623             2.014042e+18   \n",
       "30654                 0.268692             5.048042e+17   \n",
       "\n",
       "                           best_gen_Phi3_5_mini_Instruct  \\\n",
       "932    Digital distribution platforms Book industry o...   \n",
       "2751                                      Not applicable   \n",
       "13126                        Plaza de Armas San Agustín,   \n",
       "15089                    Beta-blockers, muscle relaxants   \n",
       "16992                                     Sleep Medicine   \n",
       "...                                                  ...   \n",
       "27854                    Japan is a country in East Asia   \n",
       "32683  Russia is a transcontinental country in Northe...   \n",
       "32742            Multifaceted and complex, with partners   \n",
       "30641  Relations vary with each country; please speci...   \n",
       "30654        Friendly, though sometimes strained, due to   \n",
       "\n",
       "       gen_recall_Phi3_5_mini_Instruct  bert_sim_Phi3_5_mini_Instruct  \\\n",
       "932                               0.50                       0.615013   \n",
       "2751                              0.00                       0.205666   \n",
       "13126                             0.25                       0.311132   \n",
       "15089                             0.00                       0.342819   \n",
       "16992                             1.00                       1.000000   \n",
       "...                                ...                            ...   \n",
       "27854                             1.00                       0.697189   \n",
       "32683                             1.00                       0.605403   \n",
       "32742                             0.00                       0.091473   \n",
       "30641                             0.00                       0.371126   \n",
       "30654                             0.00                       0.211534   \n",
       "\n",
       "      subject_qid  object_qid  subject_popularity_sitelinks  \\\n",
       "932     Q56560668  Q112165919                             0   \n",
       "2751    Q87300250  Q113491656                             0   \n",
       "13126         NaN         NaN                             0   \n",
       "15089   Q65632660   Q95594627                             0   \n",
       "16992         NaN   Q15762248                             0   \n",
       "...           ...         ...                           ...   \n",
       "27854         Q17         Q17                           410   \n",
       "32683        Q159        Q159                           410   \n",
       "32742        Q159         Q17                           410   \n",
       "30641         Q43         Q17                           414   \n",
       "30654         Q43        Q159                           414   \n",
       "\n",
       "       object_popularity_sitelinks popularity_sitelinks_sum  \n",
       "932                              0                        0  \n",
       "2751                             0                        0  \n",
       "13126                            0                        0  \n",
       "15089                            0                        0  \n",
       "16992                            0                        0  \n",
       "...                            ...                      ...  \n",
       "27854                          410                      820  \n",
       "32683                          410                      820  \n",
       "32742                          410                      820  \n",
       "30641                          410                      824  \n",
       "30654                          410                      824  \n",
       "\n",
       "[55355 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b53512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2fa1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing values per column (count) ===\n",
      "subject_qid                        56\n",
      "object_qid                         40\n",
      "file                                0\n",
      "answer                              0\n",
      "question                            0\n",
      "subject                             0\n",
      "relation                            0\n",
      "best_gen_Llama_8b_Instract          0\n",
      "gen_recall_Llama_8b_Instract        0\n",
      "object                              0\n",
      "PPL_Llama3_1-8B_Instruct            0\n",
      "best_gen_Llama_3b_Instract          0\n",
      "gen_recall_Llama_3b_Instract        0\n",
      "bert_sim_Llama_3b_Instract          0\n",
      "PPL_Llama3_2-1B_Instruct            0\n",
      "best_gen_Llama_1b_Instract          0\n",
      "gen_recall_Llama_1b_Instract        0\n",
      "bert_sim_Llama_8b_Instract          0\n",
      "PPL_Llama3_2-3B_Instruct            0\n",
      "PPL_Gemma_7B_IT                     0\n",
      "bert_sim_Llama_1b_Instract          0\n",
      "best_gen_Gemma_7b_IT                0\n",
      "gen_recall_Gemma_7b_IT              0\n",
      "best_gen_Zephyr_7b_Beta             0\n",
      "gen_recall_Zephyr_7b_Beta           0\n",
      "bert_sim_Gemma_7b_IT                0\n",
      "PPL_Zephyr_7B_Beta                  0\n",
      "PPL_Phi3_5_mini_Instruct            0\n",
      "bert_sim_Zephyr_7b_Beta             0\n",
      "gen_recall_Phi3_5_mini_Instruct     0\n",
      "best_gen_Phi3_5_mini_Instruct       0\n",
      "bert_sim_Phi3_5_mini_Instruct       0\n",
      "subject_popularity_sitelinks        0\n",
      "object_popularity_sitelinks         0\n",
      "popularity_sitelinks_sum            0\n",
      "\n",
      "=== Missing values per column (count & %) ===\n",
      "                                 missing  missing_%\n",
      "subject_qid                           56       0.10\n",
      "object_qid                            40       0.07\n",
      "file                                   0       0.00\n",
      "answer                                 0       0.00\n",
      "question                               0       0.00\n",
      "subject                                0       0.00\n",
      "relation                               0       0.00\n",
      "best_gen_Llama_8b_Instract             0       0.00\n",
      "gen_recall_Llama_8b_Instract           0       0.00\n",
      "object                                 0       0.00\n",
      "PPL_Llama3_1-8B_Instruct               0       0.00\n",
      "best_gen_Llama_3b_Instract             0       0.00\n",
      "gen_recall_Llama_3b_Instract           0       0.00\n",
      "bert_sim_Llama_3b_Instract             0       0.00\n",
      "PPL_Llama3_2-1B_Instruct               0       0.00\n",
      "best_gen_Llama_1b_Instract             0       0.00\n",
      "gen_recall_Llama_1b_Instract           0       0.00\n",
      "bert_sim_Llama_8b_Instract             0       0.00\n",
      "PPL_Llama3_2-3B_Instruct               0       0.00\n",
      "PPL_Gemma_7B_IT                        0       0.00\n",
      "bert_sim_Llama_1b_Instract             0       0.00\n",
      "best_gen_Gemma_7b_IT                   0       0.00\n",
      "gen_recall_Gemma_7b_IT                 0       0.00\n",
      "best_gen_Zephyr_7b_Beta                0       0.00\n",
      "gen_recall_Zephyr_7b_Beta              0       0.00\n",
      "bert_sim_Gemma_7b_IT                   0       0.00\n",
      "PPL_Zephyr_7B_Beta                     0       0.00\n",
      "PPL_Phi3_5_mini_Instruct               0       0.00\n",
      "bert_sim_Zephyr_7b_Beta                0       0.00\n",
      "gen_recall_Phi3_5_mini_Instruct        0       0.00\n",
      "best_gen_Phi3_5_mini_Instruct          0       0.00\n",
      "bert_sim_Phi3_5_mini_Instruct          0       0.00\n",
      "subject_popularity_sitelinks           0       0.00\n",
      "object_popularity_sitelinks            0       0.00\n",
      "popularity_sitelinks_sum               0       0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Кол-во пропусков по колонкам\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"=== Missing values per column (count) ===\")\n",
    "print(na_counts.to_string())\n",
    "\n",
    "# Дополнительно: с долей в %\n",
    "missing_report = (\n",
    "    pd.DataFrame({\n",
    "        \"missing\": df.isna().sum(),\n",
    "        \"missing_%\": (df.isna().mean() * 100).round(2),\n",
    "    })\n",
    "    .sort_values(\"missing\", ascending=False)\n",
    ")\n",
    "print(\"\\n=== Missing values per column (count & %) ===\")\n",
    "print(missing_report.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3ff145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing counts ===\n",
      "subject_qid: 0\n",
      "object_qid : 0\n",
      "\n",
      "=== First few missing SUBJECT rows ===\n",
      "Empty DataFrame\n",
      "Columns: [row_id, subject, subject_qid]\n",
      "Index: []\n",
      "\n",
      "=== First few missing OBJECT rows ===\n",
      "Empty DataFrame\n",
      "Columns: [row_id, object, object_qid]\n",
      "Index: []\n",
      "\n",
      "=== Unique unresolved SUBJECT values (top 20) ===\n",
      "Empty DataFrame\n",
      "Columns: [subject, count]\n",
      "Index: []\n",
      "\n",
      "=== Unique unresolved OBJECT values (top 20) ===\n",
      "Empty DataFrame\n",
      "Columns: [object, count]\n",
      "Index: []\n",
      "\n",
      "✅ Saved:\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_rows.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_rows.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_unique.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_unique.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_indices.json\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_indices.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# поменяй путь при необходимости\n",
    "PATH = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "# что считаем пропуском для *_qid\n",
    "def is_missing_qid(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# базовые проверки\n",
    "for col in [\"subject\",\"object\",\"subject_qid\",\"object_qid\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Колонка отсутствует: {col}\")\n",
    "\n",
    "m_subj = df[\"subject_qid\"].apply(is_missing_qid)\n",
    "m_obj  = df[\"object_qid\"].apply(is_missing_qid)\n",
    "\n",
    "print(\"=== Missing counts ===\")\n",
    "print(f\"subject_qid: {int(m_subj.sum())}\")\n",
    "print(f\"object_qid : {int(m_obj.sum())}\")\n",
    "\n",
    "# ------- детальные таблицы по строкам -------\n",
    "subj_missing_rows = (\n",
    "    df.loc[m_subj, [\"subject\",\"subject_qid\"]]\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\":\"row_id\"})\n",
    ")\n",
    "obj_missing_rows = (\n",
    "    df.loc[m_obj, [\"object\",\"object_qid\"]]\n",
    "      .reset_index()\n",
    "      .rename(columns={\"index\":\"row_id\"})\n",
    ")\n",
    "\n",
    "print(\"\\n=== First few missing SUBJECT rows ===\")\n",
    "print(subj_missing_rows.head(10).to_string(index=False))\n",
    "print(\"\\n=== First few missing OBJECT rows ===\")\n",
    "print(obj_missing_rows.head(10).to_string(index=False))\n",
    "\n",
    "# ------- уникальные значения + частоты -------\n",
    "subj_unique = (\n",
    "    df.loc[m_subj, \"subject\"]\n",
    "      .astype(str).str.strip()\n",
    "      .value_counts(dropna=False)\n",
    "      .rename_axis(\"subject\")\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "obj_unique = (\n",
    "    df.loc[m_obj, \"object\"]\n",
    "      .astype(str).str.strip()\n",
    "      .value_counts(dropna=False)\n",
    "      .rename_axis(\"object\")\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Unique unresolved SUBJECT values (top 20) ===\")\n",
    "print(subj_unique.head(20).to_string(index=False))\n",
    "print(\"\\n=== Unique unresolved OBJECT values (top 20) ===\")\n",
    "print(obj_unique.head(20).to_string(index=False))\n",
    "\n",
    "# ------- маппинги: значение -> список индексов строк -------\n",
    "subj_map = (\n",
    "    df.loc[m_subj, [\"subject\"]]\n",
    "      .reset_index()\n",
    "      .groupby(\"subject\")[\"index\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "obj_map = (\n",
    "    df.loc[m_obj, [\"object\"]]\n",
    "      .reset_index()\n",
    "      .groupby(\"object\")[\"index\"]\n",
    "      .apply(list)\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# ------- сохранение результатов рядом с исходным файлом -------\n",
    "out_dir = Path(PATH).parent\n",
    "subj_rows_csv = out_dir / \"missing_subject_qid_rows.csv\"\n",
    "obj_rows_csv  = out_dir / \"missing_object_qid_rows.csv\"\n",
    "subj_unique_csv = out_dir / \"missing_subject_qid_unique.csv\"\n",
    "obj_unique_csv  = out_dir / \"missing_object_qid_unique.csv\"\n",
    "subj_idx_json = out_dir / \"missing_subject_qid_indices.json\"\n",
    "obj_idx_json  = out_dir / \"missing_object_qid_indices.json\"\n",
    "\n",
    "subj_missing_rows.to_csv(subj_rows_csv, index=False)\n",
    "obj_missing_rows.to_csv(obj_rows_csv, index=False)\n",
    "subj_unique.to_csv(subj_unique_csv, index=False)\n",
    "obj_unique.to_csv(obj_unique_csv, index=False)\n",
    "\n",
    "subj_idx_json.write_text(json.dumps(subj_map, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "obj_idx_json.write_text(json.dumps(obj_map, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n✅ Saved:\")\n",
    "print(f\"- {subj_rows_csv}\")\n",
    "print(f\"- {obj_rows_csv}\")\n",
    "print(f\"- {subj_unique_csv}\")\n",
    "print(f\"- {obj_unique_csv}\")\n",
    "print(f\"- {subj_idx_json}\")\n",
    "print(f\"- {obj_idx_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b580cdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing subject_qid: 71\n",
      "Missing object_qid : 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid: 100%|██████████| 71/71 [02:32<00:00,  2.15s/it]\n",
      "resolve object_qid: 100%|██████████| 46/46 [03:55<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitelinks for 19 new QIDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qid→sitelinks: 100%|██████████| 19/19 [00:08<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining subject_qid: 15\n",
      "Remaining object_qid : 24\n",
      "\n",
      "Saved unresolved subject lists to:\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_rows.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_unique.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_subject_qid_indices.json\n",
      "\n",
      "Saved unresolved object lists to:\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_rows.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_unique.csv\n",
      "- /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/missing_object_qid_indices.json\n",
      "\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ===== IO =====\n",
    "INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "OUT        = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "CACHE_LABEL2QID = Path(OUT).with_suffix(\".label2qid.json\")\n",
    "CACHE_SITELINKS = Path(OUT).with_suffix(\".sitelinks_cache.json\")\n",
    "\n",
    "# ===== SETTINGS =====\n",
    "LANGS_WB   = [\"en\", \"es\", \"ru\", \"de\", \"fr\", \"it\", \"uk\", \"pl\"]\n",
    "WIKI_SITES = [\"enwiki\", \"eswiki\", \"ruwiki\", \"dewiki\", \"frwiki\"]\n",
    "SLEEP_BETWEEN = 0.08\n",
    "TIMEOUT       = 20\n",
    "QID_RE = re.compile(r\"\\bQ\\d+\\b\", re.IGNORECASE)\n",
    "\n",
    "# ——— РУЧНЫЕ ПРАВКИ (добавляй сюда найденные тобой соответствия) ———\n",
    "# ключ — НОРМАЛИЗОВАННАЯ строка (см. canon_label), значение — QID\n",
    "MANUAL_QID = {\n",
    "    # Примеры (раскомментируй/поправь после проверки):\n",
    "    \"Bayamo, Cuba\" : \"Q115382\",\n",
    "    \"Konstantin Tsiolkovskii\" : \"Q41239\",\n",
    "    \n",
    "    # \"bayamo cuba\": \"Q155833\",                     # Bayamo\n",
    "    # \"konstantin tsiolkovskii\": \"Q6679\",          # Konstantin Tsiolkovsky (проверь QID)\n",
    "    # \"cayetana fitz james stuarthu\": \"Q155962\",   # Cayetana Fitz-James Stuart (проверь QID)\n",
    "    # \"milowice sosnowiec\": \"Q...?\",               # район в Sosnowiec\n",
    "    # \"tlajomulco de zuniga municipality\": \"Q...\", # муниципалитет (Mexico)\n",
    "    # \"liam pane\": \"Q340608\",                      # Liam Payne (если это опечатка)\n",
    "    # \"lhaj adam opel\": \"Q158251\",                 # Adam Opel (если «lhaj» это «haj»/титул)\n",
    "    # \"sleeprunning\": \"\",                          # если сущности нет — пусть останется пустым\n",
    "    # \"azulfina\": \"\",                              # если сущности нет\n",
    "}\n",
    "\n",
    "# ===== HTTP =====\n",
    "def build_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=6, backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"], respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"UNLamb-Wikidata/1.2 (contact: youremail@example.com)\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    })\n",
    "    return sess\n",
    "\n",
    "# ===== NORMALIZATION =====\n",
    "STOPWORDS_PREFIX = {\"el\",\"al\",\"haj\",\"lhaj\",\"sheikh\",\"shaikh\",\"mr\",\"mrs\",\"ms\",\"dr\"}\n",
    "DROP_TAIL_WORDS = {\"municipality\",\"city\",\"district\",\"county\",\"province\",\"state\",\"town\",\"village\"}\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    norm = unicodedata.normalize(\"NFKD\", s or \"\")\n",
    "    return \"\".join(ch for ch in norm if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_base(s: str) -> str:\n",
    "    s = str(s or \"\").strip()\n",
    "    s = s.replace(\"—\",\"-\").replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"‘\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "    s = re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", s)          # удалить скобки\n",
    "    s = re.sub(r\"[^\\w\\s,\\-]\", \" \", s)             # оставить буквы/цифры/пробел/дефис/зпт\n",
    "    s = s.replace(\"_\",\" \").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def canon_label(s: str) -> str:\n",
    "    s = normalize_base(s).lower().rstrip(\".\").strip()\n",
    "    s = strip_accents(s)\n",
    "    # убрать префиксные титулы\n",
    "    parts = [p for p in s.split() if p]\n",
    "    while parts and parts[0] in STOPWORDS_PREFIX:\n",
    "        parts = parts[1:]\n",
    "    s = \" \".join(parts)\n",
    "    # удалить завершающее служебное слово\n",
    "    tail = s.split()\n",
    "    if tail and tail[-1] in DROP_TAIL_WORDS:\n",
    "        s = \" \".join(tail[:-1])\n",
    "    return s\n",
    "\n",
    "def generate_variants(orig: str):\n",
    "    \"\"\"Порождаем набор кандидатных строк для поиска.\"\"\"\n",
    "    raw  = str(orig or \"\").strip()\n",
    "    norm = normalize_base(raw)\n",
    "    fold = strip_accents(norm)\n",
    "\n",
    "    base = canon_label(raw)\n",
    "    out = []\n",
    "    def add(x):\n",
    "        if x and x not in out:\n",
    "            out.append(x)\n",
    "\n",
    "    add(raw)\n",
    "    add(norm)\n",
    "    add(fold)\n",
    "    add(base)\n",
    "    # убрать точку\n",
    "    add(base.rstrip(\".\"))\n",
    "    # часть до запятой (часто 'City, Country')\n",
    "    if \",\" in base:\n",
    "        left = base.split(\",\", 1)[0].strip()\n",
    "        right = base.split(\",\", 1)[1].strip()\n",
    "        add(left)\n",
    "        add(right)\n",
    "        # перестановка\n",
    "        add(f\"{right} {left}\".strip())\n",
    "    # удалить слово 'municipality' в середине\n",
    "    add(re.sub(r\"\\bmunicipality\\b\", \" \", base).strip())\n",
    "    # исправление 'ii' -> 'y' для фамилий типа Tsiolkovskii\n",
    "    add(re.sub(r\"skii\\b\", \"sky\", base))\n",
    "    # убрать одиночные служебные слова в начале\n",
    "    toks = base.split()\n",
    "    if toks and toks[0] in STOPWORDS_PREFIX:\n",
    "        add(\" \".join(toks[1:]))\n",
    "\n",
    "    # уникальные\n",
    "    return [x for x in out if x]\n",
    "\n",
    "# ===== WIKIDATA =====\n",
    "def wbsearchentities(label, session, lang=\"en\", limit=10, timeout=TIMEOUT):\n",
    "    params = {\n",
    "        \"action\":\"wbsearchentities\",\"format\":\"json\",\n",
    "        \"language\":lang,\"uselang\":lang,\"type\":\"item\",\n",
    "        \"search\":label,\"limit\":limit,\n",
    "    }\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return (r.json() or {}).get(\"search\", []) or []\n",
    "\n",
    "def pick_qid_from_results(label_raw: str, results: list) -> str | None:\n",
    "    if not results: return None\n",
    "    lab_norm = strip_accents(normalize_base(label_raw)).lower()\n",
    "    # точное совпадение по label\n",
    "    for it in results:\n",
    "        lab = strip_accents(normalize_base(it.get(\"label\",\"\"))).lower()\n",
    "        if lab and lab == lab_norm:\n",
    "            return it.get(\"id\")\n",
    "    # точное совпадение по алиасам\n",
    "    for it in results:\n",
    "        for al in it.get(\"aliases\") or []:\n",
    "            if strip_accents(normalize_base(al)).lower() == lab_norm:\n",
    "                return it.get(\"id\")\n",
    "    # иначе первый\n",
    "    return results[0].get(\"id\")\n",
    "\n",
    "def wiki_search_title(term: str, session, lang=\"en\", timeout=TIMEOUT):\n",
    "    params = {\"action\":\"query\",\"list\":\"search\",\"format\":\"json\",\n",
    "              \"srsearch\":term,\"srlimit\":1,\"srwhat\":\"nearmatch\",\"srprop\":\"\"}\n",
    "    r = session.get(f\"https://{lang}.wikipedia.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    hits = ((r.json() or {}).get(\"query\", {}) or {}).get(\"search\", []) or []\n",
    "    return hits[0].get(\"title\") if hits else None\n",
    "\n",
    "def wiki_title_to_qid(site: str, title: str, session, timeout=TIMEOUT):\n",
    "    params = {\"action\":\"wbgetentities\",\"format\":\"json\",\"sites\":site,\"titles\":title,\"props\":\"info\"}\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    for k in (r.json() or {}).get(\"entities\", {}):\n",
    "        if k.startswith(\"Q\"): return k\n",
    "    return None\n",
    "\n",
    "def resolve_label_to_qid(label: str, session, manual_map: dict) -> str | None:\n",
    "    if not label or str(label).strip().lower() in {\"unknown\",\"uknown\",\"n/a\",\"-\",\"none\"}:\n",
    "        return None\n",
    "\n",
    "    q = extract_qid(label)\n",
    "    if q: return q\n",
    "\n",
    "    # 1) ручной словарь\n",
    "    key = canon_label(label)\n",
    "    if key in manual_map and manual_map[key]:\n",
    "        return manual_map[key]\n",
    "\n",
    "    # 2) варианты + wbsearchentities\n",
    "    variants = generate_variants(label)\n",
    "    for cand in variants:\n",
    "        for lang in LANGS_WB:\n",
    "            try:\n",
    "                res = wbsearchentities(cand, session, lang=lang, limit=10)\n",
    "                qid = pick_qid_from_results(cand, res)\n",
    "                if qid:\n",
    "                    return qid\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN/2)\n",
    "\n",
    "    # 3) wiki search → title → QID\n",
    "    for cand in variants:\n",
    "        for l, site in zip([\"en\",\"es\",\"ru\",\"de\",\"fr\"], WIKI_SITES):\n",
    "            try:\n",
    "                title = wiki_search_title(cand, session, lang=l)\n",
    "                if title:\n",
    "                    qid = wiki_title_to_qid(site, title, session)\n",
    "                    if qid:\n",
    "                        return qid\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_qid(val):\n",
    "    if pd.isna(val): return None\n",
    "    m = QID_RE.search(str(val))\n",
    "    return m.group(0).upper() if m else None\n",
    "\n",
    "def get_sitelinks_count(qid, session, timeout=TIMEOUT):\n",
    "    if not qid: return 0\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=timeout)\n",
    "        if r.status_code == 404: return 0\n",
    "        r.raise_for_status()\n",
    "        ent = (r.json() or {}).get(\"entities\", {}).get(qid, {})\n",
    "        return len(ent.get(\"sitelinks\", {}))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ===== MAIN =====\n",
    "def is_missing_qid(x) -> bool:\n",
    "    if pd.isna(x): return True\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    for col in [\"subject\",\"object\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Нет обязательной колонки: {col}\")\n",
    "\n",
    "    # гарантируем наличие целевых полей\n",
    "    for col in [\"subject_qid\",\"object_qid\",\"subject_popularity_sitelinks\",\"object_popularity_sitelinks\",\"popularity_sitelinks_sum\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    m_subj = df[\"subject_qid\"].apply(is_missing_qid)\n",
    "    m_obj  = df[\"object_qid\"].apply(is_missing_qid)\n",
    "    print(f\"Missing subject_qid: {int(m_subj.sum())}\")\n",
    "    print(f\"Missing object_qid : {int(m_obj.sum())}\")\n",
    "\n",
    "    if m_subj.sum() == 0 and m_obj.sum() == 0:\n",
    "        df.to_csv(OUT, index=False)\n",
    "        print(f\"✅ Nothing to fill. Saved: {OUT}\")\n",
    "        return\n",
    "\n",
    "    session = build_session()\n",
    "    label2qid = {}  # локальный кэш текущего прогона + manual\n",
    "    label2qid.update(MANUAL_QID)\n",
    "    # подгрузи предыдущий кэш, если был\n",
    "    if CACHE_LABEL2QID.exists():\n",
    "        try:\n",
    "            prev = json.loads(CACHE_LABEL2QID.read_text(\"utf-8\"))\n",
    "            label2qid.update(prev)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # SUBJECT\n",
    "    for idx in tqdm(df.index[m_subj], desc=\"resolve subject_qid\"):\n",
    "        lbl = str(df.at[idx, \"subject\"])\n",
    "        qid = resolve_label_to_qid(lbl, session, label2qid)\n",
    "        # сохраняем в label2qid по каноническому ключу, чтобы ускорять следующие попадания\n",
    "        label2qid[canon_label(lbl)] = qid or label2qid.get(canon_label(lbl), \"\")\n",
    "        if qid:\n",
    "            df.at[idx, \"subject_qid\"] = qid\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # OBJECT\n",
    "    for idx in tqdm(df.index[m_obj], desc=\"resolve object_qid\"):\n",
    "        lbl = str(df.at[idx, \"object\"])\n",
    "        qid = resolve_label_to_qid(lbl, session, label2qid)\n",
    "        label2qid[canon_label(lbl)] = qid or label2qid.get(canon_label(lbl), \"\")\n",
    "        if qid:\n",
    "            df.at[idx, \"object_qid\"] = qid\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # сохранить кэш маппинга\n",
    "    try:\n",
    "        CACHE_LABEL2QID.write_text(json.dumps(label2qid, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # подтянем sitelinks только для новых QID из строк, где ИЗНАЧАЛЬНО были пропуски\n",
    "    touched = (m_subj | m_obj)\n",
    "    # загрузим существующий кэш sitelinks\n",
    "    qid2sitelinks = {}\n",
    "    if CACHE_SITELINKS.exists():\n",
    "        try:\n",
    "            qid2sitelinks.update(json.loads(CACHE_SITELINKS.read_text(\"utf-8\")))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    new_qids = set()\n",
    "    for idx in df.index[touched]:\n",
    "        for col in [\"subject_qid\", \"object_qid\"]:\n",
    "            q = str(df.at[idx, col]).strip()\n",
    "            if q and q.upper().startswith(\"Q\"):\n",
    "                new_qids.add(q)\n",
    "\n",
    "    to_fetch = [q for q in sorted(new_qids) if q not in qid2sitelinks]\n",
    "    if to_fetch:\n",
    "        print(f\"Fetching sitelinks for {len(to_fetch)} new QIDs...\")\n",
    "        for q in tqdm(to_fetch, desc=\"qid→sitelinks\"):\n",
    "            qid2sitelinks[q] = get_sitelinks_count(q, session)\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        try:\n",
    "            CACHE_SITELINKS.write_text(json.dumps(qid2sitelinks, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # пересчёт популярностей только на touched\n",
    "    rows = df.index[touched]\n",
    "    if len(rows) > 0:\n",
    "        sub_vals = df.loc[rows, \"subject_qid\"].astype(str).str.strip()\n",
    "        obj_vals = df.loc[rows, \"object_qid\"].astype(str).str.strip()\n",
    "        df.loc[rows, \"subject_popularity_sitelinks\"] = sub_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"object_popularity_sitelinks\"]  = obj_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"popularity_sitelinks_sum\"]     = (\n",
    "            df.loc[rows, \"subject_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "            + df.loc[rows, \"object_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "    # отчёт по остаткам — и выгрузки для ручной правки\n",
    "    still_subj = df[\"subject_qid\"].apply(is_missing_qid)\n",
    "    still_obj  = df[\"object_qid\"].apply(is_missing_qid)\n",
    "    print(f\"\\nRemaining subject_qid: {int(still_subj.sum())}\")\n",
    "    print(f\"Remaining object_qid : {int(still_obj.sum())}\")\n",
    "\n",
    "    # выведем уникальные нерешённые значения и их индексы — чтобы ты дописала MANUAL_QID\n",
    "    def dump_missing(col_label, col_qid, fname_prefix):\n",
    "        miss = df[col_qid].apply(is_missing_qid)\n",
    "        rows = df.loc[miss, [col_label, col_qid]].reset_index().rename(columns={\"index\":\"row_id\"})\n",
    "        uniq = (\n",
    "            rows[col_label].astype(str).str.strip()\n",
    "            .value_counts(dropna=False)\n",
    "            .rename_axis(col_label)\n",
    "            .reset_index(name=\"count\")\n",
    "        )\n",
    "        out_dir = Path(OUT).parent\n",
    "        rows.to_csv(out_dir / f\"{fname_prefix}_rows.csv\", index=False)\n",
    "        uniq.to_csv(out_dir / f\"{fname_prefix}_unique.csv\", index=False)\n",
    "\n",
    "        # маппинг значение -> индексы\n",
    "        mapping = rows.groupby(col_label)[\"row_id\"].apply(list).to_dict()\n",
    "        (out_dir / f\"{fname_prefix}_indices.json\").write_text(json.dumps(mapping, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        print(f\"\\nSaved unresolved {col_label} lists to:\")\n",
    "        print(f\"- {(out_dir / f'{fname_prefix}_rows.csv')}\")\n",
    "        print(f\"- {(out_dir / f'{fname_prefix}_unique.csv')}\")\n",
    "        print(f\"- {(out_dir / f'{fname_prefix}_indices.json')}\")\n",
    "\n",
    "    if still_subj.any():\n",
    "        dump_missing(\"subject\", \"subject_qid\", \"missing_subject_qid\")\n",
    "    if still_obj.any():\n",
    "        dump_missing(\"object\", \"object_qid\", \"missing_object_qid\")\n",
    "\n",
    "    # save\n",
    "    Path(OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(f\"\\n✅ Saved: {OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2282ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing subject_qid: 71\n",
      "Missing object_qid : 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 2] 'San Agustín (Jalisco)'  ->  Q4407227  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:   3%|▎         | 2/71 [00:04<02:18,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 4] 'sleeprunning'  ->  NOT FOUND\n",
      "[SUBJ row 158] 'Bayamo, Cuba.'  ->  Q115382  | method=manual-canon cand=None lang=None\n",
      "[SUBJ row 618] 'lhaj Adam Opel'  ->  Q57479  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:   8%|▊         | 6/71 [00:04<00:33,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 619] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 761] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-canon cand=None lang=None\n",
      "[SUBJ row 887] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  11%|█▏        | 8/71 [00:04<00:21,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 969] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 1290] 'Bayamo, Cuba.'  ->  Q115382  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  14%|█▍        | 10/71 [00:07<00:49,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 1454] 'sleeprunning'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  15%|█▌        | 11/71 [00:13<01:54,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 1569] 'Azulfina'  ->  NOT FOUND\n",
      "[SUBJ row 1630] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 1730] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  20%|█▉        | 14/71 [00:17<01:33,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 1760] 'flat feet problems'  ->  NOT FOUND\n",
      "[SUBJ row 1817] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 1996] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  25%|██▌       | 18/71 [00:17<00:37,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 1997] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 2108] 'Ryn, Giżycko County'  ->  Q616895  | method=manual-canon cand=None lang=None\n",
      "[SUBJ row 2865] 'Concepción, Santa Cruz'  ->  Q751077  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  28%|██▊       | 20/71 [00:17<00:25,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 3426] 'Sonsonate Oeste (Acajutla)'  ->  Q127416243  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 4104] 'Aybak, Samangan'  ->  Q1020649  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  31%|███       | 22/71 [00:24<01:07,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 5994] 'Falun Mine Museum'  ->  NOT FOUND\n",
      "[SUBJ row 6132] 'Ryn, Giżycko County'  ->  Q616895  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  34%|███▍      | 24/71 [00:27<01:09,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 6836] 'sleeprunning'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  35%|███▌      | 25/71 [00:37<02:15,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 7627] 'Fox Glacier / Te Moeka o Tuawe'  ->  NOT FOUND\n",
      "[SUBJ row 7682] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 7683] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  39%|███▉      | 28/71 [00:40<01:35,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 8293] 'zahracleaning'  ->  NOT FOUND\n",
      "[SUBJ row 8679] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  42%|████▏     | 30/71 [00:47<01:47,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 9486] 'Band 4 Band'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  44%|████▎     | 31/71 [00:53<02:14,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 10082] 'Falun Mine Museum'  ->  NOT FOUND\n",
      "[SUBJ row 11454] 'The Iron Gate (Jerusalem)'  ->  Q119361435  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 11862] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  46%|████▋     | 33/71 [00:53<01:21,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 11915] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  49%|████▉     | 35/71 [01:03<01:50,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 12670] 'Fox Glacier / Te Moeka o Tuawe'  ->  NOT FOUND\n",
      "[SUBJ row 13727] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 14856] 'Anavatos, Chios'  ->  Q21682902  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  55%|█████▍    | 39/71 [01:03<00:46,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 15077] 'The Iron Gate (Jerusalem)'  ->  Q119361435  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 15496] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 16115] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  58%|█████▊    | 41/71 [01:03<00:30,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 16612] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 17310] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 19020] 'Naolinco, Veracruz'  ->  Q948343  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  63%|██████▎   | 45/71 [01:04<00:13,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 19875] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 20368] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 22199] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  66%|██████▌   | 47/71 [01:04<00:09,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 22531] 'Sonsonate Oeste (Acajutla)'  ->  Q127416243  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  69%|██████▉   | 49/71 [01:10<00:27,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 22592] 'Falun Mine Museum'  ->  NOT FOUND\n",
      "[SUBJ row 23938] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 24858] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  72%|███████▏  | 51/71 [01:10<00:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 26375] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 26376] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 28155] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  77%|███████▋  | 55/71 [01:17<00:22,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 28345] 'Falun Mine Museum'  ->  NOT FOUND\n",
      "[SUBJ row 28630] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 30724] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  80%|████████  | 57/71 [01:17<00:14,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 35183] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 35184] 'Konstantin Tsiolkovskii'  ->  Q41239  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 36318] 'The Iron Gate (Jerusalem)'  ->  Q119361435  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  86%|████████▌ | 61/71 [01:17<00:05,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 37140] 'San Matías, Santa Cruz'  ->  Q176823  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 37150] 'Concepción, Santa Cruz'  ->  Q751077  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  89%|████████▊ | 63/71 [01:24<00:10,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 38567] 'Pueblo Marron'  ->  NOT FOUND\n",
      "[SUBJ row 38690] 'Bayamo, Cuba.'  ->  Q115382  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 38691] 'Bayamo, Cuba.'  ->  Q115382  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  92%|█████████▏| 65/71 [01:24<00:05,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 39061] 'Lundu, Sarawak'  ->  Q6704167  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  93%|█████████▎| 66/71 [01:34<00:12,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 39332] 'Fox Glacier / Te Moeka o Tuawe'  ->  NOT FOUND\n",
      "[SUBJ row 41416] 'Aybak, Samangan'  ->  Q1020649  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 42525] 'The Iron Gate (Jerusalem)'  ->  Q119361435  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid:  99%|█████████▊| 70/71 [01:34<00:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUBJ row 44529] 'Naolinco, Veracruz'  ->  Q948343  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 44538] 'San Agustín (Jalisco)'  ->  Q4407227  | method=manual-exact cand=None lang=None\n",
      "[SUBJ row 44542] 'José Cardel, Veracruz'  ->  Q5938733  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid: 100%|██████████| 71/71 [01:34<00:00,  1.33s/it]\n",
      "resolve object_qid:   0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 2] 'Tlajomulco de Zúñiga Municipality'  ->  Q20249211  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:   4%|▍         | 2/46 [00:06<02:21,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 116] 'Liam Pane'  ->  NOT FOUND\n",
      "[OBJ  row 150] 'Bayamo, Cuba.'  ->  Q115382  | method=manual-exact cand=None lang=None\n",
      "[OBJ  row 215] 'Milowice, Sosnowiec'  ->  Q11781308  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:   9%|▊         | 4/46 [00:06<00:57,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 622] 'lhaj Adam Opel'  ->  Q57479  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  13%|█▎        | 6/46 [00:13<01:34,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 1241] 'Cayetana Fitz-James Stuarthu'  ->  NOT FOUND\n",
      "[OBJ  row 1942] 'Tlajomulco de Zúñiga Municipality'  ->  Q20249211  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  17%|█▋        | 8/46 [00:23<02:08,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 1981] 'Manufacture of machinery and equipment n.e.c.'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  20%|█▉        | 9/46 [00:30<02:32,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 1989] 'Princely House of Thurn and Taxis'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  22%|██▏       | 10/46 [00:36<02:50,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 5086] 'Santa Maria in Cosmedin Church'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  24%|██▍       | 11/46 [00:43<03:00,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 5087] 'Santa Maria in Cosmedin Church'  ->  NOT FOUND\n",
      "[OBJ  row 5231] 'Tinn Municipality'  ->  Q2365  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  28%|██▊       | 13/46 [00:49<02:22,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 5479] 'Saint Petersburg Eparchy'  ->  NOT FOUND\n",
      "[OBJ  row 6077] 'Hernando, Córdoba'  ->  Q18920947  | method=manual-canon cand=None lang=None\n",
      "[OBJ  row 6104] 'Ryn, Giżycko County'  ->  Q616895  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  37%|███▋      | 17/46 [00:50<00:52,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 6994] 'Los Cabos, México'  ->  Q5979421  | method=manual-canon cand=None lang=None\n",
      "[OBJ  row 7123] 'Los Cabos, México'  ->  Q5979421  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  39%|███▉      | 18/46 [00:56<01:16,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 7127] 'Saint Petersburg Eparchy'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  41%|████▏     | 19/46 [01:02<01:35,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 7619] 'Saint Petersburg Eparchy'  ->  NOT FOUND\n",
      "[OBJ  row 7688] 'Baalbek, Lebanon'  ->  Q178835  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  46%|████▌     | 21/46 [01:09<01:24,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 7833] 'Saint Petersburg Eparchy'  ->  NOT FOUND\n",
      "[OBJ  row 9930] 'BASF (Czechia)'  ->  Q9401  | method=manual-exact cand=None lang=None\n",
      "[OBJ  row 10012] 'Spijk, Groningen'  ->  Q2694306  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  52%|█████▏    | 24/46 [01:15<01:07,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 10878] 'Institution of Noble Ladies of the Prague Castle'  ->  NOT FOUND\n",
      "[OBJ  row 12325] 'Charlotte de Constant de Rebecque (1769-1845)'  ->  Q55901812  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  57%|█████▋    | 26/46 [01:22<01:02,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 12863] 'Mohammed Abdelmonem'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  59%|█████▊    | 27/46 [01:28<01:12,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 13730] 'Numidia Kingdom'  ->  NOT FOUND\n",
      "[OBJ  row 15396] 'Sielec, Sosnowiec'  ->  Q9336466  | method=manual-canon cand=None lang=None\n",
      "[OBJ  row 15397] 'Milowice, Sosnowiec'  ->  Q11781308  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  63%|██████▎   | 29/46 [01:28<00:41,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 15784] 'Stellantis Italy'  ->  Q3744458  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  67%|██████▋   | 31/46 [01:35<00:39,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 16444] 'Liam Pane'  ->  NOT FOUND\n",
      "[OBJ  row 17986] 'Lujhu District'  ->  Q713367  | method=manual-canon cand=None lang=None\n",
      "[OBJ  row 17987] 'Qieding District'  ->  Q713090  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  74%|███████▍  | 34/46 [01:42<00:33,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 18135] 'MAS IPSP'  ->  NOT FOUND\n",
      "[OBJ  row 18355] 'Chengguan District, Lanzhou'  ->  Q1069975  | method=manual-canon cand=None lang=None\n",
      "[OBJ  row 20087] 'Huamantla, Tlaxcala'  ->  Q576636  | method=manual-canon cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  80%|████████  | 37/46 [01:51<00:30,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 22486] 'Ralph H. Fowwlerr'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  83%|████████▎ | 38/46 [01:58<00:32,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 26940] 'Taking acetaminophen after smoking cannabis'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  85%|████████▍ | 39/46 [02:07<00:38,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 34525] 'Tuila'epa Sa'ilele Malielegaoi'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  87%|████████▋ | 40/46 [02:17<00:38,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 34954] 'Yeico Cáceres'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  89%|████████▉ | 41/46 [02:24<00:32,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 35031] 'flag of Dominican Republic'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  91%|█████████▏| 42/46 [02:33<00:29,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 36591] 'Isaías Fortis'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  93%|█████████▎| 43/46 [02:43<00:24,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 36592] 'Isaías Fortis'  ->  NOT FOUND\n",
      "[OBJ  row 36799] 'Centre-Nord Region'  ->  Q850064  | method=manual-exact cand=None lang=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid:  98%|█████████▊| 45/46 [02:50<00:05,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 36800] 'Est Region'  ->  NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve object_qid: 100%|██████████| 46/46 [02:56<00:00,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OBJ  row 41453] 'Brahui tribe'  ->  NOT FOUND\n",
      "\n",
      "Remaining subject_qid: 15\n",
      "Remaining object_qid : 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ===== IO =====\n",
    "INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "OUT        = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "CACHE_LABEL2QID = Path(OUT).with_suffix(\".label2qid.json\")\n",
    "CACHE_SITELINKS = Path(OUT).with_suffix(\".sitelinks_cache.json\")\n",
    "\n",
    "# ===== SETTINGS =====\n",
    "LANGS_WB   = [\"en\", \"es\", \"ru\", \"de\", \"fr\", \"it\", \"uk\", \"pl\"]\n",
    "WIKI_SITES = [\"enwiki\", \"eswiki\", \"ruwiki\", \"dewiki\", \"frwiki\"]\n",
    "SLEEP_BETWEEN = 0.08\n",
    "TIMEOUT       = 20\n",
    "QID_RE = re.compile(r\"\\bQ\\d+\\b\", re.IGNORECASE)\n",
    "\n",
    "# ——— РУЧНЫЕ ПРАВКИ (можно добавлять в любом регистре; работает и с канонич. ключом) ———\n",
    "MANUAL_QID = {\n",
    "    \"Bayamo, Cuba\"              : \"Q115382\",  # город Bayamo (Cuba)\n",
    "    \"Konstantin Tsiolkovskii\"   : \"Q41239\",   # Konstantin Tsiolkovsky\n",
    "    # примеры для дописывания:\n",
    "    # \"Milowice, Sosnowiec\": \"Q672082\",  # проверить реальный QID\n",
    "    # \"Tlajomulco de Zúñiga Municipality\": \"Q184216\",  # примерный, проверь\n",
    "    # \"Liam Pane\": \"Q340608\",  # если опечатка от Liam Payne\n",
    "    # \"lhaj Adam Opel\": \"Q158251\",  # Adam Opel — если \"lhaj\" это титул\n",
    "}\n",
    "\n",
    "# ===== HTTP =====\n",
    "def build_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=6, backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"], respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"UNLamb-Wikidata/1.2 (contact: youremail@example.com)\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    })\n",
    "    return sess\n",
    "\n",
    "# ===== NORMALIZATION =====\n",
    "STOPWORDS_PREFIX = {\"el\",\"al\",\"haj\",\"lhaj\",\"sheikh\",\"shaikh\",\"mr\",\"mrs\",\"ms\",\"dr\"}\n",
    "DROP_TAIL_WORDS = {\"municipality\",\"city\",\"district\",\"county\",\"province\",\"state\",\"town\",\"village\"}\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    norm = unicodedata.normalize(\"NFKD\", s or \"\")\n",
    "    return \"\".join(ch for ch in norm if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_base(s: str) -> str:\n",
    "    s = str(s or \"\").strip()\n",
    "    s = s.replace(\"—\",\"-\").replace(\"–\",\"-\").replace(\"’\",\"'\").replace(\"‘\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "    s = re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", s)          # удалить скобки\n",
    "    s = re.sub(r\"[^\\w\\s,\\-]\", \" \", s)             # оставить буквы/цифры/пробел/дефис/зпт\n",
    "    s = s.replace(\"_\",\" \").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def canon_label(s: str) -> str:\n",
    "    s = normalize_base(s).lower().rstrip(\".\").strip()\n",
    "    s = strip_accents(s)\n",
    "    # убрать префиксные титулы\n",
    "    parts = [p for p in s.split() if p]\n",
    "    while parts and parts[0] in STOPWORDS_PREFIX:\n",
    "        parts = parts[1:]\n",
    "    s = \" \".join(parts)\n",
    "    # удалить завершающее служебное слово\n",
    "    tail = s.split()\n",
    "    if tail and tail[-1] in DROP_TAIL_WORDS:\n",
    "        s = \" \".join(tail[:-1])\n",
    "    return s\n",
    "\n",
    "def generate_variants(orig: str):\n",
    "    \"\"\"Порождаем набор кандидатных строк для поиска.\"\"\"\n",
    "    raw  = str(orig or \"\").strip()\n",
    "    norm = normalize_base(raw)\n",
    "    fold = strip_accents(norm)\n",
    "\n",
    "    base = canon_label(raw)\n",
    "    out = []\n",
    "    def add(x):\n",
    "        if x and x not in out:\n",
    "            out.append(x)\n",
    "\n",
    "    add(raw)\n",
    "    add(norm)\n",
    "    add(fold)\n",
    "    add(base)\n",
    "    # убрать точку\n",
    "    add(base.rstrip(\".\"))\n",
    "    # часть до запятой (часто 'City, Country')\n",
    "    if \",\" in base:\n",
    "        left = base.split(\",\", 1)[0].strip()\n",
    "        right = base.split(\",\", 1)[1].strip()\n",
    "        add(left); add(right); add(f\"{right} {left}\".strip())\n",
    "    # удалить слово 'municipality' в середине\n",
    "    add(re.sub(r\"\\bmunicipality\\b\", \" \", base).strip())\n",
    "    # исправление 'ii' -> 'y' для фамилий типа Tsiolkovskii\n",
    "    add(re.sub(r\"skii\\b\", \"sky\", base))\n",
    "    # убрать титулы в начале\n",
    "    toks = base.split()\n",
    "    if toks and toks[0] in STOPWORDS_PREFIX:\n",
    "        add(\" \".join(toks[1:]))\n",
    "\n",
    "    return [x for x in out if x]\n",
    "\n",
    "# ===== WIKIDATA =====\n",
    "def wbsearchentities(label, session, lang=\"en\", limit=10, timeout=TIMEOUT):\n",
    "    params = {\n",
    "        \"action\":\"wbsearchentities\",\"format\":\"json\",\n",
    "        \"language\":lang,\"uselang\":lang,\"type\":\"item\",\n",
    "        \"search\":label,\"limit\":limit,\n",
    "    }\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return (r.json() or {}).get(\"search\", []) or []\n",
    "\n",
    "def pick_qid_from_results(label_raw: str, results: list):\n",
    "    \"\"\"\n",
    "    Возвращает (qid, match_type):\n",
    "      - 'wb-label-exact'  — точное совпадение по label (после нормализации/деакцента)\n",
    "      - 'wb-alias-exact'  — точное совпадение по alias\n",
    "      - 'wb-top'          — просто первый результат\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return None, None\n",
    "    lab_norm = strip_accents(normalize_base(label_raw)).lower()\n",
    "    for it in results:\n",
    "        lab = strip_accents(normalize_base(it.get(\"label\",\"\"))).lower()\n",
    "        if lab and lab == lab_norm:\n",
    "            return it.get(\"id\"), \"wb-label-exact\"\n",
    "    for it in results:\n",
    "        for al in it.get(\"aliases\") or []:\n",
    "            if strip_accents(normalize_base(al)).lower() == lab_norm:\n",
    "                return it.get(\"id\"), \"wb-alias-exact\"\n",
    "    return results[0].get(\"id\"), \"wb-top\"\n",
    "\n",
    "def wiki_search_title(term: str, session, lang=\"en\", timeout=TIMEOUT):\n",
    "    params = {\"action\":\"query\",\"list\":\"search\",\"format\":\"json\",\n",
    "              \"srsearch\":term,\"srlimit\":1,\"srwhat\":\"nearmatch\",\"srprop\":\"\"}\n",
    "    r = session.get(f\"https://{lang}.wikipedia.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    hits = ((r.json() or {}).get(\"query\", {}) or {}).get(\"search\", []) or []\n",
    "    return hits[0].get(\"title\") if hits else None\n",
    "\n",
    "def wiki_title_to_qid(site: str, title: str, session, timeout=TIMEOUT):\n",
    "    params = {\"action\":\"wbgetentities\",\"format\":\"json\",\"sites\":site,\"titles\":title,\"props\":\"info\"}\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    for k in (r.json() or {}).get(\"entities\", {}):\n",
    "        if k.startswith(\"Q\"):\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def extract_qid(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    m = QID_RE.search(str(val))\n",
    "    return m.group(0).upper() if m else None\n",
    "\n",
    "def resolve_label_to_qid(label: str, session, manual_map: dict):\n",
    "    \"\"\"\n",
    "    Возвращает (qid, meta) где meta ~ {'method':..., 'candidate':..., 'lang':...}\n",
    "    method ∈ {'manual-exact','manual-canon','wb-label-exact','wb-alias-exact','wb-top','wiki-title', None}\n",
    "    \"\"\"\n",
    "    meta = {\"method\": None, \"candidate\": None, \"lang\": None}\n",
    "    if not label or str(label).strip().lower() in {\"unknown\",\"uknown\",\"n/a\",\"-\",\"none\"}:\n",
    "        return None, meta\n",
    "\n",
    "    # уже QID?\n",
    "    q = extract_qid(label)\n",
    "    if q:\n",
    "        meta[\"method\"] = \"given-qid\"\n",
    "        return q, meta\n",
    "\n",
    "    # manual: пробуем разными ключами\n",
    "    raw_key   = label\n",
    "    canon_key = canon_label(label)\n",
    "    lower_key = str(label).lower().strip()\n",
    "    if raw_key in manual_map and manual_map[raw_key]:\n",
    "        meta[\"method\"] = \"manual-exact\"\n",
    "        return manual_map[raw_key], meta\n",
    "    if canon_key in manual_map and manual_map[canon_key]:\n",
    "        meta[\"method\"] = \"manual-canon\"\n",
    "        return manual_map[canon_key], meta\n",
    "    if lower_key in manual_map and manual_map[lower_key]:\n",
    "        meta[\"method\"] = \"manual-lower\"\n",
    "        return manual_map[lower_key], meta\n",
    "\n",
    "    # варианты + wbsearchentities\n",
    "    variants = generate_variants(label)\n",
    "    for cand in variants:\n",
    "        for lang in LANGS_WB:\n",
    "            try:\n",
    "                res = wbsearchentities(cand, session, lang=lang, limit=10)\n",
    "                qid, how = pick_qid_from_results(cand, res)\n",
    "                if qid:\n",
    "                    meta.update({\"method\": how, \"candidate\": cand, \"lang\": lang})\n",
    "                    return qid, meta\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN/2)\n",
    "\n",
    "    # wiki search → title → QID\n",
    "    for cand in variants:\n",
    "        for l, site in zip([\"en\",\"es\",\"ru\",\"de\",\"fr\"], WIKI_SITES):\n",
    "            try:\n",
    "                title = wiki_search_title(cand, session, lang=l)\n",
    "                if title:\n",
    "                    qid = wiki_title_to_qid(site, title, session)\n",
    "                    if qid:\n",
    "                        meta.update({\"method\": \"wiki-title\", \"candidate\": title, \"lang\": l})\n",
    "                        return qid, meta\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    return None, meta\n",
    "\n",
    "def get_sitelinks_count(qid, session, timeout=TIMEOUT):\n",
    "    if not qid:\n",
    "        return 0\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=timeout)\n",
    "        if r.status_code == 404:\n",
    "            return 0\n",
    "        r.raise_for_status()\n",
    "        ent = (r.json() or {}).get(\"entities\", {}).get(qid, {})\n",
    "        return len(ent.get(\"sitelinks\", {}))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ===== MISSING CHECK =====\n",
    "def is_missing_qid(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "# ===== MAIN =====\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    for col in [\"subject\",\"object\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Нет обязательной колонки: {col}\")\n",
    "\n",
    "    # гарантируем наличие целевых полей\n",
    "    for col in [\"subject_qid\",\"object_qid\",\"subject_popularity_sitelinks\",\"object_popularity_sitelinks\",\"popularity_sitelinks_sum\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    m_subj = df[\"subject_qid\"].apply(is_missing_qid)\n",
    "    m_obj  = df[\"object_qid\"].apply(is_missing_qid)\n",
    "    print(f\"Missing subject_qid: {int(m_subj.sum())}\")\n",
    "    print(f\"Missing object_qid : {int(m_obj.sum())}\")\n",
    "\n",
    "    if m_subj.sum() == 0 and m_obj.sum() == 0:\n",
    "        df.to_csv(OUT, index=False)\n",
    "        print(f\"✅ Nothing to fill. Saved: {OUT}\")\n",
    "        return\n",
    "\n",
    "    session = build_session()\n",
    "\n",
    "    # подгружаем предыдущий кэш label->qid (плюс manual)\n",
    "    label2qid = {}\n",
    "    label2qid.update(MANUAL_QID)\n",
    "    if CACHE_LABEL2QID.exists():\n",
    "        try:\n",
    "            prev = json.loads(CACHE_LABEL2QID.read_text(\"utf-8\"))\n",
    "            label2qid.update(prev)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ========= SUBJECT: только пропуски + печать найденных =========\n",
    "    for idx in tqdm(df.index[m_subj], desc=\"resolve subject_qid\"):\n",
    "        lbl = str(df.at[idx, \"subject\"])\n",
    "        qid, meta = resolve_label_to_qid(lbl, session, label2qid)\n",
    "\n",
    "        # кэшируем ключи для быстрого повторного попадания\n",
    "        label2qid[lbl] = qid or label2qid.get(lbl, \"\")\n",
    "        label2qid[canon_label(lbl)] = qid or label2qid.get(canon_label(lbl), \"\")\n",
    "\n",
    "        if qid:\n",
    "            df.at[idx, \"subject_qid\"] = qid\n",
    "            print(f\"[SUBJ row {idx}] '{lbl}'  ->  {qid}  | method={meta.get('method')} cand={meta.get('candidate')} lang={meta.get('lang')}\")\n",
    "        else:\n",
    "            print(f\"[SUBJ row {idx}] '{lbl}'  ->  NOT FOUND\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # ========= OBJECT: только пропуски + печать найденных =========\n",
    "    for idx in tqdm(df.index[m_obj], desc=\"resolve object_qid\"):\n",
    "        lbl = str(df.at[idx, \"object\"])\n",
    "        qid, meta = resolve_label_to_qid(lbl, session, label2qid)\n",
    "\n",
    "        label2qid[lbl] = qid or label2qid.get(lbl, \"\")\n",
    "        label2qid[canon_label(lbl)] = qid or label2qid.get(canon_label(lbl), \"\")\n",
    "\n",
    "        if qid:\n",
    "            df.at[idx, \"object_qid\"] = qid\n",
    "            print(f\"[OBJ  row {idx}] '{lbl}'  ->  {qid}  | method={meta.get('method')} cand={meta.get('candidate')} lang={meta.get('lang')}\")\n",
    "        else:\n",
    "            print(f\"[OBJ  row {idx}] '{lbl}'  ->  NOT FOUND\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # сохранить кэш маппинга\n",
    "    try:\n",
    "        CACHE_LABEL2QID.write_text(json.dumps(label2qid, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ——— sitelinks только для новых QID из строк, где ИЗНАЧАЛЬНО были пропуски ———\n",
    "    touched = (m_subj | m_obj)\n",
    "\n",
    "    qid2sitelinks = {}\n",
    "    if CACHE_SITELINKS.exists():\n",
    "        try:\n",
    "            qid2sitelinks.update(json.loads(CACHE_SITELINKS.read_text(\"utf-8\")))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    new_qids = set()\n",
    "    for idx in df.index[touched]:\n",
    "        for col in [\"subject_qid\", \"object_qid\"]:\n",
    "            q = str(df.at[idx, col]).strip()\n",
    "            if q and q.upper().startswith(\"Q\"):\n",
    "                new_qids.add(q)\n",
    "\n",
    "    to_fetch = [q for q in sorted(new_qids) if q not in qid2sitelinks]\n",
    "    if to_fetch:\n",
    "        print(f\"Fetching sitelinks for {len(to_fetch)} new QIDs...\")\n",
    "        for q in tqdm(to_fetch, desc=\"qid→sitelinks\"):\n",
    "            qid2sitelinks[q] = get_sitelinks_count(q, session)\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        try:\n",
    "            CACHE_SITELINKS.write_text(json.dumps(qid2sitelinks, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ——— пересчёт популярностей только на touched ———\n",
    "    rows = df.index[touched]\n",
    "    if len(rows) > 0:\n",
    "        sub_vals = df.loc[rows, \"subject_qid\"].astype(str).str.strip()\n",
    "        obj_vals = df.loc[rows, \"object_qid\"].astype(str).str.strip()\n",
    "        df.loc[rows, \"subject_popularity_sitelinks\"] = sub_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"object_popularity_sitelinks\"]  = obj_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"popularity_sitelinks_sum\"]     = (\n",
    "            df.loc[rows, \"subject_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "            + df.loc[rows, \"object_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "    # отчёт и сохранение\n",
    "    still_subj = int(df[\"subject_qid\"].apply(is_missing_qid).sum())\n",
    "    still_obj  = int(df[\"object_qid\"].apply(is_missing_qid).sum())\n",
    "    print(f\"\\nRemaining subject_qid: {still_subj}\")\n",
    "    print(f\"Remaining object_qid : {still_obj}\")\n",
    "\n",
    "    Path(OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(f\"\\n✅ Saved: {OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615cc437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 55355\n",
      "Удаляю (пустые subject_qid или object_qid): 39\n",
      "Останется: 55316\n",
      "✅ Перезаписано: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "def is_missing_qid(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# Проверим наличие колонок\n",
    "for col in [\"subject_qid\", \"object_qid\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Нет обязательной колонки: {col}\")\n",
    "\n",
    "mask_bad = df[\"subject_qid\"].apply(is_missing_qid) | df[\"object_qid\"].apply(is_missing_qid)\n",
    "\n",
    "removed = int(mask_bad.sum())\n",
    "kept = int(len(df) - removed)\n",
    "\n",
    "print(f\"Всего строк: {len(df)}\")\n",
    "print(f\"Удаляю (пустые subject_qid или object_qid): {removed}\")\n",
    "print(f\"Останется: {kept}\")\n",
    "\n",
    "df_clean = df.loc[~mask_bad].reset_index(drop=True)\n",
    "df_clean.to_csv(PATH, index=False)\n",
    "\n",
    "print(f\"✅ Перезаписано: {PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3e71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e1a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461c5b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NaN counts by best_gen_* column ===\n",
      "best_gen_Llama_1b_Instract           123\n",
      "best_gen_Phi3_5_mini_Instruct        56\n",
      "best_gen_Llama_3b_Instract           43\n",
      "best_gen_Llama_8b_Instract           30\n",
      "best_gen_Gemma_7b_IT                 2\n",
      "\n",
      "=== Pairwise intersection sizes (NaN indices) ===\n",
      "best_gen_Llama_1b_Instract ∩ best_gen_Phi3_5_mini_Instruct: 1\n",
      "best_gen_Llama_1b_Instract ∩ best_gen_Llama_3b_Instract: 1\n",
      "best_gen_Llama_1b_Instract ∩ best_gen_Llama_8b_Instract: 0\n",
      "best_gen_Llama_1b_Instract ∩ best_gen_Gemma_7b_IT: 0\n",
      "best_gen_Phi3_5_mini_Instruct ∩ best_gen_Llama_3b_Instract: 7\n",
      "best_gen_Phi3_5_mini_Instruct ∩ best_gen_Llama_8b_Instract: 10\n",
      "best_gen_Phi3_5_mini_Instruct ∩ best_gen_Gemma_7b_IT: 0\n",
      "best_gen_Llama_3b_Instract ∩ best_gen_Llama_8b_Instract: 1\n",
      "best_gen_Llama_3b_Instract ∩ best_gen_Gemma_7b_IT: 0\n",
      "best_gen_Llama_8b_Instract ∩ best_gen_Gemma_7b_IT: 0\n",
      "\n",
      "Rows NaN in ALL listed best_gen_* columns: 0\n",
      "\n",
      "=== Filled 'Uknown' & zeroed metrics for rows that had NaN in best_gen_* ===\n",
      "best_gen_Llama_1b_Instract           rows affected: 123\n",
      "best_gen_Phi3_5_mini_Instruct        rows affected: 56\n",
      "best_gen_Llama_3b_Instract           rows affected: 43\n",
      "best_gen_Llama_8b_Instract           rows affected: 30\n",
      "best_gen_Gemma_7b_IT                 rows affected: 2\n",
      "\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from pathlib import Path\n",
    "\n",
    "# === ФАЙЛЫ ===\n",
    "PATH = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "OUT  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "\n",
    "# Точные имена best_gen-колонок (как у тебя в данных)\n",
    "BEST_COLS = [\n",
    "    \"best_gen_Llama_1b_Instract\",\n",
    "    \"best_gen_Phi3_5_mini_Instruct\",\n",
    "    \"best_gen_Llama_3b_Instract\",\n",
    "    \"best_gen_Llama_8b_Instract\",\n",
    "    \"best_gen_Gemma_7b_IT\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "# Оставим только реально существующие (на всякий случай)\n",
    "best_existing = [c for c in BEST_COLS if c in df.columns]\n",
    "if not best_existing:\n",
    "    raise ValueError(\"Не нашёл ни одной колонки из BEST_COLS в файле.\")\n",
    "\n",
    "# === 1) Анализ пересечений пропусков ===\n",
    "nan_masks = {c: df[c].isna() for c in best_existing}\n",
    "nan_counts = {c: m.sum() for c, m in nan_masks.items()}\n",
    "\n",
    "print(\"=== NaN counts by best_gen_* column ===\")\n",
    "for c in best_existing:\n",
    "    print(f\"{c:35s}  {nan_counts[c]}\")\n",
    "\n",
    "# Попарные пересечения\n",
    "pairs = list(it.combinations(best_existing, 2))\n",
    "if pairs:\n",
    "    print(\"\\n=== Pairwise intersection sizes (NaN indices) ===\")\n",
    "    for a, b in pairs:\n",
    "        inter = (nan_masks[a] & nan_masks[b]).sum()\n",
    "        print(f\"{a} ∩ {b}: {inter}\")\n",
    "\n",
    "# Пересечение всех\n",
    "all_inter_mask = None\n",
    "for c in best_existing:\n",
    "    all_inter_mask = nan_masks[c] if all_inter_mask is None else (all_inter_mask & nan_masks[c])\n",
    "all_inter_cnt = int(all_inter_mask.sum())\n",
    "print(f\"\\nRows NaN in ALL listed best_gen_* columns: {all_inter_cnt}\")\n",
    "\n",
    "# === 2) Заполнение пропусков в best_gen_* -> \"Uknown\"\n",
    "# === 3) В этих же строках нули в bert_sim_* и gen_recall_*\n",
    "def suffix_from_best(colname: str) -> str:\n",
    "    # 'best_gen_' -> возвращаем хвост\n",
    "    return colname[len(\"best_gen_\"):] if colname.startswith(\"best_gen_\") else None\n",
    "\n",
    "changed_report = {}\n",
    "for best_col in best_existing:\n",
    "    mask_nan = nan_masks[best_col]  # запоминаем изначальные пропуски для этой колонки\n",
    "    n_rows = int(mask_nan.sum())\n",
    "    if n_rows == 0:\n",
    "        changed_report[best_col] = 0\n",
    "        continue\n",
    "\n",
    "    # 2) best_gen_* = \"Uknown\"\n",
    "    df.loc[mask_nan, best_col] = \"Uknown\"\n",
    "\n",
    "    # 3) найти соответствующие bert_sim_* и gen_recall_* и занулить в тех же индексах\n",
    "    suf = suffix_from_best(best_col)\n",
    "    if suf:\n",
    "        sim_col = f\"bert_sim_{suf}\"\n",
    "        rec_col = f\"gen_recall_{suf}\"\n",
    "        if sim_col in df.columns:\n",
    "            df.loc[mask_nan, sim_col] = 0.0\n",
    "        if rec_col in df.columns:\n",
    "            df.loc[mask_nan, rec_col] = 0.0\n",
    "\n",
    "    changed_report[best_col] = n_rows\n",
    "\n",
    "print(\"\\n=== Filled 'Uknown' & zeroed metrics for rows that had NaN in best_gen_* ===\")\n",
    "for c in best_existing:\n",
    "    print(f\"{c:35s}  rows affected: {changed_report[c]}\")\n",
    "\n",
    "# Сохранение\n",
    "Path(OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT, index=False)\n",
    "print(f\"\\n✅ Saved: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolve labels → QID (sample10): 100%|██████████| 12/12 [00:03<00:00,  3.47it/s]\n",
      "Fetch sitelinks (sample10): 100%|██████████| 12/12 [00:05<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved subject → QID: 100.0%  |  object → QID: 100.0%\n",
      "\n",
      "=== SAMPLE(10) RESULTS ===\n",
      "     subject                        object subject_qid object_qid  subject_popularity_sitelinks  object_popularity_sitelinks  popularity_sitelinks_sum\n",
      "  JavaScript               web development       Q2005    Q386275                           157                           38                       195\n",
      "  JavaScript   aspect-oriented programming       Q2005     Q30267                           157                           33                       190\n",
      "  JavaScript      event-driven programming       Q2005   Q1135914                           157                           29                       186\n",
      "  JavaScript        imperative programming       Q2005    Q275596                           157                           50                       207\n",
      "  JavaScript           generic programming       Q2005   Q1051282                           157                           33                       190\n",
      "  JavaScript        functional programming       Q2005    Q193076                           157                           56                       213\n",
      "  JavaScript          computer programming       Q2005     Q80006                           157                          120                       277\n",
      "  JavaScript                       JScript       Q2005    Q553514                           157                           27                       184\n",
      "Visual Basic Visual Basic for Applications       Q2378    Q667566                            89                           40                       129\n",
      "Visual Basic                     Microsoft       Q2378      Q2283                            89                          194                       283\n",
      "\n",
      "✅ Saved sample(10) file: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_SAMPLE10_with_popularity.csv\n",
      "ℹ️ Caches: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_SAMPLE10_with_popularity.label2qid.sample10.json, /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_SAMPLE10_with_popularity.sitelinks.sample10.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# import re\n",
    "# import json\n",
    "# import time\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# from tqdm import tqdm\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from urllib3.util.retry import Retry\n",
    "\n",
    "# # ======= CONFIG =======\n",
    "# INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics.csv\"\n",
    "# OUTPUT_CSV = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/zephyr_7b_beta_all_questions_with_metrics_SAMPLE10_with_popularity.csv\"\n",
    "\n",
    "# # Кэши для теста (отдельно, чтобы не мешали прод-запуску)\n",
    "# CACHE_LABEL2QID = Path(OUTPUT_CSV).with_suffix(\".label2qid.sample10.json\")\n",
    "# CACHE_SITELINKS = Path(OUTPUT_CSV).with_suffix(\".sitelinks.sample10.json\")\n",
    "\n",
    "# SAMPLE_N = 10            # берём 10 строк\n",
    "# LANGS = [\"en\", \"ru\", \"de\"]\n",
    "# STRICT_EXACT_ONLY = False  # если True — берём только точное совпадение метки\n",
    "# SLEEP_BETWEEN = 0.05\n",
    "# TIMEOUT = 20\n",
    "\n",
    "# QID_RE = re.compile(r\"Q\\d+\")\n",
    "\n",
    "# # ======= HTTP session =======\n",
    "# def build_session():\n",
    "#     sess = requests.Session()\n",
    "#     retries = Retry(\n",
    "#         total=5,\n",
    "#         backoff_factor=0.5,\n",
    "#         status_forcelist=[429, 500, 502, 503, 504],\n",
    "#         allowed_methods=[\"GET\"],\n",
    "#         respect_retry_after_header=True,\n",
    "#     )\n",
    "#     adapter = HTTPAdapter(max_retries=retries)\n",
    "#     sess.mount(\"https://\", adapter)\n",
    "#     sess.headers.update({\n",
    "#         \"User-Agent\": \"UNLamb-Wikidata/1.0 (contact: youremail@example.com)\",\n",
    "#         \"Accept\": \"application/json\",\n",
    "#     })\n",
    "#     return sess\n",
    "\n",
    "# # ======= utils =======\n",
    "# def load_json(path):\n",
    "#     p = Path(path)\n",
    "#     if p.exists():\n",
    "#         try:\n",
    "#             return json.loads(p.read_text(\"utf-8\"))\n",
    "#         except Exception:\n",
    "#             return {}\n",
    "#     return {}\n",
    "\n",
    "# def save_json(path, data):\n",
    "#     try:\n",
    "#         Path(path).write_text(json.dumps(data, ensure_ascii=False), encoding=\"utf-8\")\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# def extract_qid(val):\n",
    "#     if pd.isna(val):\n",
    "#         return None\n",
    "#     s = str(val)\n",
    "#     m = QID_RE.search(s)\n",
    "#     return m.group(0) if m else None\n",
    "\n",
    "# def search_wikidata_qid(label, session, langs=LANGS, timeout=TIMEOUT, strict_exact=STRICT_EXACT_ONLY):\n",
    "#     label_stripped = str(label).strip()\n",
    "#     if not label_stripped:\n",
    "#         return None\n",
    "\n",
    "#     # если уже есть QID в строке\n",
    "#     q = extract_qid(label_stripped)\n",
    "#     if q:\n",
    "#         return q\n",
    "\n",
    "#     for lang in langs:\n",
    "#         params = {\n",
    "#             \"action\": \"wbsearchentities\",\n",
    "#             \"format\": \"json\",\n",
    "#             \"language\": lang,\n",
    "#             \"uselang\": lang,\n",
    "#             \"type\": \"item\",\n",
    "#             \"search\": label_stripped,\n",
    "#             \"limit\": 5,\n",
    "#         }\n",
    "#         try:\n",
    "#             r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "#             r.raise_for_status()\n",
    "#             results = (r.json() or {}).get(\"search\", []) or []\n",
    "#             if not results:\n",
    "#                 continue\n",
    "\n",
    "#             lower = label_stripped.lower()\n",
    "\n",
    "#             # 1) точное совпадение по label (case-insensitive)\n",
    "#             for item in results:\n",
    "#                 if item.get(\"label\", \"\").lower() == lower:\n",
    "#                     return item.get(\"id\")\n",
    "\n",
    "#             # 2) если строгий режим выключен — берём топ-результат\n",
    "#             if not strict_exact:\n",
    "#                 return results[0].get(\"id\")\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "#     return None\n",
    "\n",
    "# def get_sitelinks_count(qid, session, timeout=TIMEOUT):\n",
    "#     if not qid:\n",
    "#         return 0\n",
    "#     url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "#     try:\n",
    "#         r = session.get(url, timeout=timeout)\n",
    "#         if r.status_code == 404:\n",
    "#             return 0\n",
    "#         r.raise_for_status()\n",
    "#         ent = (r.json() or {}).get(\"entities\", {}).get(qid, {})\n",
    "#         return len(ent.get(\"sitelinks\", {}))\n",
    "#     except Exception:\n",
    "#         return 0\n",
    "\n",
    "# # ======= main =======\n",
    "# def main():\n",
    "#     df = pd.read_csv(INPUT_CSV)\n",
    "#     if \"subject\" not in df.columns or \"object\" not in df.columns:\n",
    "#         raise ValueError(\"Ожидаю колонки 'subject' и 'object'.\")\n",
    "\n",
    "#     # берём первые 10 (или случайные 10 — раскомментируй следующую строку)\n",
    "#     df_test = df.head(SAMPLE_N).copy()\n",
    "#     # df_test = df.sample(n=SAMPLE_N, random_state=42).copy()\n",
    "\n",
    "#     session = build_session()\n",
    "#     label2qid = load_json(CACHE_LABEL2QID)\n",
    "#     qid2sitelinks = load_json(CACHE_SITELINKS)\n",
    "\n",
    "#     subj_vals = df_test[\"subject\"].fillna(\"\").astype(str)\n",
    "#     obj_vals  = df_test[\"object\"].fillna(\"\").astype(str)\n",
    "#     unique_labels = sorted(set(subj_vals.tolist() + obj_vals.tolist()))\n",
    "\n",
    "#     # 1) label -> QID\n",
    "#     to_resolve = [lbl for lbl in unique_labels if lbl and lbl not in label2qid]\n",
    "#     for lbl in tqdm(to_resolve, desc=\"Resolve labels → QID (sample10)\"):\n",
    "#         qid = search_wikidata_qid(lbl, session)\n",
    "#         label2qid[lbl] = qid or \"\"\n",
    "#         time.sleep(SLEEP_BETWEEN)\n",
    "#     save_json(CACHE_LABEL2QID, label2qid)\n",
    "\n",
    "#     df_test[\"subject_qid\"] = subj_vals.map(lambda s: label2qid.get(s, \"\") or extract_qid(s) or \"\")\n",
    "#     df_test[\"object_qid\"]  = obj_vals.map(lambda s: label2qid.get(s, \"\") or extract_qid(s) or \"\")\n",
    "\n",
    "#     # 2) QID -> sitelinks\n",
    "#     qids = sorted({q for q in pd.concat([df_test[\"subject_qid\"], df_test[\"object_qid\"]]).tolist() if q})\n",
    "#     to_fetch = [q for q in qids if q not in qid2sitelinks]\n",
    "#     for q in tqdm(to_fetch, desc=\"Fetch sitelinks (sample10)\"):\n",
    "#         qid2sitelinks[q] = get_sitelinks_count(q, session)\n",
    "#         time.sleep(SLEEP_BETWEEN)\n",
    "#     save_json(CACHE_SITELINKS, qid2sitelinks)\n",
    "\n",
    "#     df_test[\"subject_popularity_sitelinks\"] = df_test[\"subject_qid\"].map(lambda q: int(qid2sitelinks.get(q, 0)))\n",
    "#     df_test[\"object_popularity_sitelinks\"]  = df_test[\"object_qid\"].map(lambda q: int(qid2sitelinks.get(q, 0)))\n",
    "#     df_test[\"popularity_sitelinks_sum\"]     = df_test[\"subject_popularity_sitelinks\"] + df_test[\"object_popularity_sitelinks\"]\n",
    "\n",
    "#     # краткий отчёт\n",
    "#     res_subj = (df_test[\"subject_qid\"] != \"\").mean() * 100\n",
    "#     res_obj  = (df_test[\"object_qid\"]  != \"\").mean() * 100\n",
    "#     print(f\"Resolved subject → QID: {res_subj:.1f}%  |  object → QID: {res_obj:.1f}%\")\n",
    "\n",
    "#     cols = [\"subject\",\"object\",\"subject_qid\",\"object_qid\",\n",
    "#             \"subject_popularity_sitelinks\",\"object_popularity_sitelinks\",\"popularity_sitelinks_sum\"]\n",
    "#     print(\"\\n=== SAMPLE(10) RESULTS ===\")\n",
    "#     print(df_test[cols].to_string(index=False))\n",
    "\n",
    "#     df_test.to_csv(OUTPUT_CSV, index=False)\n",
    "#     print(f\"\\n✅ Saved sample(10) file: {OUTPUT_CSV}\")\n",
    "#     print(f\"ℹ️ Caches: {CACHE_LABEL2QID}, {CACHE_SITELINKS}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852bde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8e5653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To fill subject_qid: 71\n",
      "To fill object_qid : 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resolve subject_qid: 100%|██████████| 71/71 [03:52<00:00,  3.27s/it]\n",
      "resolve object_qid: 100%|██████████| 46/46 [03:26<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching sitelinks for 108 new QIDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qid→sitelinks: 100%|██████████| 108/108 [00:57<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining empty subject_qid: 56\n",
      "Remaining empty object_qid : 40\n",
      "\n",
      "✅ Saved: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\n",
      "ℹ️ Caches: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.label2qid.json, /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.sitelinks_cache.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import unicodedata\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ========= IO =========\n",
    "INPUT_CSV  = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/final_QA_triplets.csv\"\n",
    "OUT        = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "# Кэши рядом с OUT\n",
    "CACHE_LABEL2QID = Path(OUT).with_suffix(\".label2qid.json\")\n",
    "CACHE_SITELINKS = Path(OUT).with_suffix(\".sitelinks_cache.json\")\n",
    "\n",
    "# ========= SETTINGS =========\n",
    "LANGS_WB   = [\"en\", \"ru\", \"de\", \"fr\", \"es\", \"it\", \"uk\", \"pl\"]\n",
    "WIKI_SITES = [\"enwiki\", \"ruwiki\", \"dewiki\", \"frwiki\", \"eswiki\"]\n",
    "SLEEP_BETWEEN = 0.08\n",
    "TIMEOUT       = 20\n",
    "QID_RE = re.compile(r\"\\bQ\\d+\\b\", re.IGNORECASE)\n",
    "\n",
    "# ========= HTTP =========\n",
    "def build_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=6,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        respect_retry_after_header=True,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"UNLamb-Wikidata/1.1 (contact: youremail@example.com)\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    })\n",
    "    return sess\n",
    "\n",
    "# ========= UTILS =========\n",
    "def strip_accents(text: str) -> str:\n",
    "    norm = unicodedata.normalize(\"NFKD\", text or \"\")\n",
    "    return \"\".join(ch for ch in norm if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    s = str(label or \"\").strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    s = s.replace(\"—\", \"-\").replace(\"–\", \"-\").replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = re.sub(r\"\\s*\\(.*?\\)\\s*\", \" \", s)       # убрать пояснения в скобках\n",
    "    s = re.sub(r\"[^\\w\\s\\-]\", \" \", s)           # только буквы/цифры/пробел/дефис/_\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_qid(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    m = QID_RE.search(str(val))\n",
    "    return m.group(0).upper() if m else None\n",
    "\n",
    "def is_missing_qid(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "def is_meaningful_label(s: str) -> bool:\n",
    "    lo = str(s or \"\").strip().lower()\n",
    "    return lo not in {\"\", \"unknown\", \"uknown\", \"n/a\", \"-\", \"none\"}\n",
    "\n",
    "# ========= CACHE I/O =========\n",
    "def load_json(path):\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text(\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_json(path, data):\n",
    "    try:\n",
    "        Path(path).write_text(json.dumps(data, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ========= WIKIDATA =========\n",
    "def wbsearchentities(label, session, lang=\"en\", limit=10, timeout=TIMEOUT):\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": lang,\n",
    "        \"uselang\": lang,\n",
    "        \"type\": \"item\",\n",
    "        \"search\": label,\n",
    "        \"limit\": limit,\n",
    "    }\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return (r.json() or {}).get(\"search\", []) or []\n",
    "\n",
    "def pick_qid_from_results(label_raw: str, results: list) -> str | None:\n",
    "    if not results:\n",
    "        return None\n",
    "    lab_norm = strip_accents(normalize_label(label_raw)).lower()\n",
    "    # точное совпадение по label\n",
    "    for it in results:\n",
    "        lab = strip_accents(normalize_label(it.get(\"label\", \"\"))).lower()\n",
    "        if lab and lab == lab_norm:\n",
    "            return it.get(\"id\")\n",
    "    # точное совпадение по aliases\n",
    "    for it in results:\n",
    "        for al in it.get(\"aliases\") or []:\n",
    "            al_norm = strip_accents(normalize_label(al)).lower()\n",
    "            if al_norm and al_norm == lab_norm:\n",
    "                return it.get(\"id\")\n",
    "    # иначе первый\n",
    "    return results[0].get(\"id\")\n",
    "\n",
    "def wiki_search_title(term: str, session, lang=\"en\", timeout=TIMEOUT):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"format\": \"json\",\n",
    "        \"srsearch\": term,\n",
    "        \"srlimit\": 1,\n",
    "        \"srwhat\": \"nearmatch\",\n",
    "        \"srprop\": \"\",\n",
    "    }\n",
    "    r = session.get(f\"https://{lang}.wikipedia.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    hits = ((r.json() or {}).get(\"query\", {}) or {}).get(\"search\", []) or []\n",
    "    return hits[0].get(\"title\") if hits else None\n",
    "\n",
    "def wiki_title_to_qid(site: str, title: str, session, timeout=TIMEOUT):\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"sites\": site,\n",
    "        \"titles\": title,\n",
    "        \"props\": \"info\",\n",
    "    }\n",
    "    r = session.get(\"https://www.wikidata.org/w/api.php\", params=params, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    for k in (r.json() or {}).get(\"entities\", {}):\n",
    "        if k.startswith(\"Q\"):\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "def resolve_label_to_qid(label: str, session) -> str | None:\n",
    "    if not is_meaningful_label(label):\n",
    "        return None\n",
    "    # уже QID?\n",
    "    q = extract_qid(label)\n",
    "    if q:\n",
    "        return q\n",
    "\n",
    "    raw  = str(label).strip()\n",
    "    norm = normalize_label(raw)\n",
    "    fold = strip_accents(norm)\n",
    "\n",
    "    candidates = [raw]\n",
    "    if norm and norm != raw: candidates.append(norm)\n",
    "    if fold and fold not in candidates: candidates.append(fold)\n",
    "\n",
    "    # A) wbsearchentities\n",
    "    for cand in candidates:\n",
    "        for lang in LANGS_WB:\n",
    "            try:\n",
    "                res = wbsearchentities(cand, session, lang=lang, limit=10)\n",
    "                qid = pick_qid_from_results(cand, res)\n",
    "                if qid:\n",
    "                    return qid\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN/2)\n",
    "\n",
    "    # B) wiki search → title → wbgetentities\n",
    "    for cand in candidates:\n",
    "        for l, site in zip([\"en\", \"ru\", \"de\", \"fr\", \"es\"], WIKI_SITES):\n",
    "            try:\n",
    "                title = wiki_search_title(cand, session, lang=l)\n",
    "                if title:\n",
    "                    qid = wiki_title_to_qid(site, title, session)\n",
    "                    if qid:\n",
    "                        return qid\n",
    "            except Exception:\n",
    "                pass\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_sitelinks_count(qid, session, timeout=TIMEOUT):\n",
    "    if not qid:\n",
    "        return 0\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=timeout)\n",
    "        if r.status_code == 404:\n",
    "            return 0\n",
    "        r.raise_for_status()\n",
    "        ent = (r.json() or {}).get(\"entities\", {}).get(qid, {})\n",
    "        return len(ent.get(\"sitelinks\", {}))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# ========= MAIN =========\n",
    "def main():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    for col in [\"subject\", \"object\"]:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Нет обязательной колонки: {col}\")\n",
    "\n",
    "    # убедимся, что целевые колонки существуют\n",
    "    for col in [\"subject_qid\",\"object_qid\",\"subject_popularity_sitelinks\",\"object_popularity_sitelinks\",\"popularity_sitelinks_sum\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    # маски «только пропуски»\n",
    "    m_subj = df[\"subject_qid\"].apply(is_missing_qid)\n",
    "    m_obj  = df[\"object_qid\"].apply(is_missing_qid)\n",
    "    print(f\"To fill subject_qid: {int(m_subj.sum())}\")\n",
    "    print(f\"To fill object_qid : {int(m_obj.sum())}\")\n",
    "\n",
    "    if m_subj.sum() == 0 and m_obj.sum() == 0:\n",
    "        df.to_csv(OUT, index=False)\n",
    "        print(f\"✅ Nothing to fill. Saved as-is: {OUT}\")\n",
    "        return\n",
    "\n",
    "    session = build_session()\n",
    "    label2qid = load_json(CACHE_LABEL2QID)\n",
    "    qid2sitelinks = load_json(CACHE_SITELINKS)\n",
    "\n",
    "    # —— SUBJECT only-missing\n",
    "    for idx in tqdm(df.index[m_subj], desc=\"resolve subject_qid\"):\n",
    "        lbl = str(df.at[idx, \"subject\"])\n",
    "        if not is_meaningful_label(lbl):\n",
    "            continue\n",
    "        if lbl in label2qid and label2qid[lbl]:\n",
    "            qid = label2qid[lbl]\n",
    "        else:\n",
    "            qid = resolve_label_to_qid(lbl, session)\n",
    "            label2qid[lbl] = qid or \"\"\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        if qid:\n",
    "            df.at[idx, \"subject_qid\"] = qid\n",
    "\n",
    "    # —— OBJECT only-missing\n",
    "    for idx in tqdm(df.index[m_obj], desc=\"resolve object_qid\"):\n",
    "        lbl = str(df.at[idx, \"object\"])\n",
    "        if not is_meaningful_label(lbl):\n",
    "            continue\n",
    "        if lbl in label2qid and label2qid[lbl]:\n",
    "            qid = label2qid[lbl]\n",
    "        else:\n",
    "            qid = resolve_label_to_qid(lbl, session)\n",
    "            label2qid[lbl] = qid or \"\"\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        if qid:\n",
    "            df.at[idx, \"object_qid\"] = qid\n",
    "\n",
    "    # сохранить кэш маппинга\n",
    "    save_json(CACHE_LABEL2QID, label2qid)\n",
    "\n",
    "    # строки, где ИЗНАЧАЛЬНО были пропуски (их и обновляем)\n",
    "    touched = (m_subj | m_obj)\n",
    "\n",
    "    # QID из затронутых строк → тянуть sitelinks только для новых\n",
    "    new_qids = set()\n",
    "    for idx in df.index[touched]:\n",
    "        for col in [\"subject_qid\", \"object_qid\"]:\n",
    "            q = str(df.at[idx, col]).strip()\n",
    "            if q and q.upper().startswith(\"Q\"):\n",
    "                new_qids.add(q)\n",
    "\n",
    "    to_fetch = [q for q in sorted(new_qids) if q not in qid2sitelinks]\n",
    "    if to_fetch:\n",
    "        print(f\"Fetching sitelinks for {len(to_fetch)} new QIDs...\")\n",
    "        for q in tqdm(to_fetch, desc=\"qid→sitelinks\"):\n",
    "            qid2sitelinks[q] = get_sitelinks_count(q, session)\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "        save_json(CACHE_SITELINKS, qid2sitelinks)\n",
    "\n",
    "    # пересчёт популярностей только на touched\n",
    "    rows = df.index[touched]\n",
    "    if len(rows) > 0:\n",
    "        sub_vals = df.loc[rows, \"subject_qid\"].astype(str).str.strip()\n",
    "        obj_vals = df.loc[rows, \"object_qid\"].astype(str).str.strip()\n",
    "\n",
    "        df.loc[rows, \"subject_popularity_sitelinks\"] = sub_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"object_popularity_sitelinks\"]  = obj_vals.map(lambda q: int(qid2sitelinks.get(q, 0)) if q.upper().startswith(\"Q\") else 0)\n",
    "        df.loc[rows, \"popularity_sitelinks_sum\"]     = (\n",
    "            df.loc[rows, \"subject_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "            + df.loc[rows, \"object_popularity_sitelinks\"].fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "    # отчёт\n",
    "    still_subj = int(df[\"subject_qid\"].apply(is_missing_qid).sum())\n",
    "    still_obj  = int(df[\"object_qid\"].apply(is_missing_qid).sum())\n",
    "    print(f\"\\nRemaining empty subject_qid: {still_subj}\")\n",
    "    print(f\"Remaining empty object_qid : {still_obj}\")\n",
    "\n",
    "    # save\n",
    "    Path(OUT).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(OUT, index=False)\n",
    "    print(f\"\\n✅ Saved: {OUT}\")\n",
    "    print(f\"ℹ️ Caches: {CACHE_LABEL2QID}, {CACHE_SITELINKS}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5287eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed414374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadb5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c07e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Отсортировано и сохранено: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>PPL_Llama3_1-8B_Instruct</th>\n",
       "      <th>best_gen_Llama_8b_Instract</th>\n",
       "      <th>gen_recall_Llama_8b_Instract</th>\n",
       "      <th>bert_sim_Llama_8b_Instract</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_sim_Zephyr_7b_Beta</th>\n",
       "      <th>PPL_Phi3_5_mini_Instruct</th>\n",
       "      <th>best_gen_Phi3_5_mini_Instruct</th>\n",
       "      <th>gen_recall_Phi3_5_mini_Instruct</th>\n",
       "      <th>bert_sim_Phi3_5_mini_Instruct</th>\n",
       "      <th>subject_qid</th>\n",
       "      <th>object_qid</th>\n",
       "      <th>subject_popularity_sitelinks</th>\n",
       "      <th>object_popularity_sitelinks</th>\n",
       "      <th>popularity_sitelinks_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business_industry.csv</td>\n",
       "      <td>What did book industry said to be the same as?</td>\n",
       "      <td>book publishing</td>\n",
       "      <td>book industry</td>\n",
       "      <td>said to be the same as</td>\n",
       "      <td>book publishing</td>\n",
       "      <td>1865.235713</td>\n",
       "      <td>Publishing was said to be like running a river</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.512184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468156</td>\n",
       "      <td>1.587546e+17</td>\n",
       "      <td>Digital distribution platforms Book industry o...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.615013</td>\n",
       "      <td>Q56560668</td>\n",
       "      <td>Q112165919</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>places_city.csv</td>\n",
       "      <td>What did Calais twinned administrative body?</td>\n",
       "      <td>Xiangtan</td>\n",
       "      <td>Calais</td>\n",
       "      <td>twinned administrative body</td>\n",
       "      <td>Xiangtan</td>\n",
       "      <td>46.282780</td>\n",
       "      <td>France – via Lille and Kortrijk (</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016905</td>\n",
       "      <td>1.862347e+15</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205666</td>\n",
       "      <td>Q87300250</td>\n",
       "      <td>Q113491656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health_symptom.csv</td>\n",
       "      <td>What is the drug or therapy used for treatment...</td>\n",
       "      <td>propantheline</td>\n",
       "      <td>spasm</td>\n",
       "      <td>drug or therapy used for treatment</td>\n",
       "      <td>propantheline</td>\n",
       "      <td>5.538411</td>\n",
       "      <td>Examples include baclofen, dantrolene</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355749</td>\n",
       "      <td>4.429714e+12</td>\n",
       "      <td>Beta-blockers, muscle relaxants</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.342819</td>\n",
       "      <td>Q65632660</td>\n",
       "      <td>Q95594627</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health_disease.csv</td>\n",
       "      <td>What is the health specialty of cerebrovascula...</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>cerebrovascular trauma</td>\n",
       "      <td>health specialty</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>139.951523</td>\n",
       "      <td>Neurosurgery and Vascular Surgery</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296120</td>\n",
       "      <td>2.191506e+17</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407211</td>\n",
       "      <td>Q55092798</td>\n",
       "      <td>Q15751978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>health_disease.csv</td>\n",
       "      <td>What is the health specialty of traumatic suba...</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>traumatic subarachnoid hemorrhage</td>\n",
       "      <td>health specialty</td>\n",
       "      <td>cardiology</td>\n",
       "      <td>49.679041</td>\n",
       "      <td>Neurocritical care</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135701</td>\n",
       "      <td>8.355436e+13</td>\n",
       "      <td>Neurosurgery and Neurology</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.323818</td>\n",
       "      <td>Q55093271</td>\n",
       "      <td>Q15751978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55311</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the country of Japan?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>country</td>\n",
       "      <td>Japan</td>\n",
       "      <td>12709.168794</td>\n",
       "      <td>Japan is a country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.744425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636226</td>\n",
       "      <td>4.784381e+14</td>\n",
       "      <td>Japan is a country in East Asia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697189</td>\n",
       "      <td>Q17</td>\n",
       "      <td>Q17</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55312</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the country of Russia?</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>country</td>\n",
       "      <td>Russia</td>\n",
       "      <td>11398.500244</td>\n",
       "      <td>Russia is a country in Eastern Europe and Nort...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.672289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595697</td>\n",
       "      <td>8.400571e+15</td>\n",
       "      <td>Russia is a transcontinental country in Northe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605403</td>\n",
       "      <td>Q159</td>\n",
       "      <td>Q159</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55313</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Russia?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Russia</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Japan</td>\n",
       "      <td>1651.651733</td>\n",
       "      <td>Russia has relations with:</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202271</td>\n",
       "      <td>7.640358e+17</td>\n",
       "      <td>Multifaceted and complex, with partners</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091473</td>\n",
       "      <td>Q159</td>\n",
       "      <td>Q17</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55314</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Turkey?</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Japan</td>\n",
       "      <td>3224.247831</td>\n",
       "      <td>Turkey has diplomatic relations with 195 UN re...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057623</td>\n",
       "      <td>2.014042e+18</td>\n",
       "      <td>Relations vary with each country; please speci...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371126</td>\n",
       "      <td>Q43</td>\n",
       "      <td>Q17</td>\n",
       "      <td>414</td>\n",
       "      <td>410</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55315</th>\n",
       "      <td>places_country.csv</td>\n",
       "      <td>What is the diplomatic relation of Turkey?</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>diplomatic relation</td>\n",
       "      <td>Russia</td>\n",
       "      <td>1523.578146</td>\n",
       "      <td>Turkey has a non-NATO ally relationship with R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.424669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268692</td>\n",
       "      <td>5.048042e+17</td>\n",
       "      <td>Friendly, though sometimes strained, due to</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211534</td>\n",
       "      <td>Q43</td>\n",
       "      <td>Q159</td>\n",
       "      <td>414</td>\n",
       "      <td>410</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55316 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        file  \\\n",
       "0      business_industry.csv   \n",
       "1            places_city.csv   \n",
       "3         health_symptom.csv   \n",
       "4         health_disease.csv   \n",
       "5         health_disease.csv   \n",
       "...                      ...   \n",
       "55311     places_country.csv   \n",
       "55312     places_country.csv   \n",
       "55313     places_country.csv   \n",
       "55314     places_country.csv   \n",
       "55315     places_country.csv   \n",
       "\n",
       "                                                question           answer  \\\n",
       "0         What did book industry said to be the same as?  book publishing   \n",
       "1           What did Calais twinned administrative body?         Xiangtan   \n",
       "3      What is the drug or therapy used for treatment...    propantheline   \n",
       "4      What is the health specialty of cerebrovascula...       cardiology   \n",
       "5      What is the health specialty of traumatic suba...       cardiology   \n",
       "...                                                  ...              ...   \n",
       "55311                      What is the country of Japan?            Japan   \n",
       "55312                     What is the country of Russia?           Russia   \n",
       "55313         What is the diplomatic relation of Russia?            Japan   \n",
       "55314         What is the diplomatic relation of Turkey?            Japan   \n",
       "55315         What is the diplomatic relation of Turkey?           Russia   \n",
       "\n",
       "                                 subject                            relation  \\\n",
       "0                          book industry              said to be the same as   \n",
       "1                                 Calais         twinned administrative body   \n",
       "3                                  spasm  drug or therapy used for treatment   \n",
       "4                 cerebrovascular trauma                    health specialty   \n",
       "5      traumatic subarachnoid hemorrhage                    health specialty   \n",
       "...                                  ...                                 ...   \n",
       "55311                              Japan                             country   \n",
       "55312                             Russia                             country   \n",
       "55313                             Russia                 diplomatic relation   \n",
       "55314                             Turkey                 diplomatic relation   \n",
       "55315                             Turkey                 diplomatic relation   \n",
       "\n",
       "                object  PPL_Llama3_1-8B_Instruct  \\\n",
       "0      book publishing               1865.235713   \n",
       "1             Xiangtan                 46.282780   \n",
       "3        propantheline                  5.538411   \n",
       "4           cardiology                139.951523   \n",
       "5           cardiology                 49.679041   \n",
       "...                ...                       ...   \n",
       "55311            Japan              12709.168794   \n",
       "55312           Russia              11398.500244   \n",
       "55313            Japan               1651.651733   \n",
       "55314            Japan               3224.247831   \n",
       "55315           Russia               1523.578146   \n",
       "\n",
       "                              best_gen_Llama_8b_Instract  \\\n",
       "0         Publishing was said to be like running a river   \n",
       "1                      France – via Lille and Kortrijk (   \n",
       "3                  Examples include baclofen, dantrolene   \n",
       "4                      Neurosurgery and Vascular Surgery   \n",
       "5                                     Neurocritical care   \n",
       "...                                                  ...   \n",
       "55311                                 Japan is a country   \n",
       "55312  Russia is a country in Eastern Europe and Nort...   \n",
       "55313                         Russia has relations with:   \n",
       "55314  Turkey has diplomatic relations with 195 UN re...   \n",
       "55315  Turkey has a non-NATO ally relationship with R...   \n",
       "\n",
       "       gen_recall_Llama_8b_Instract  bert_sim_Llama_8b_Instract  ...  \\\n",
       "0                               0.5                    0.512184  ...   \n",
       "1                               0.0                    0.260455  ...   \n",
       "3                               0.0                    0.279883  ...   \n",
       "4                               0.0                    0.357400  ...   \n",
       "5                               0.0                    0.258862  ...   \n",
       "...                             ...                         ...  ...   \n",
       "55311                           1.0                    0.744425  ...   \n",
       "55312                           1.0                    0.672289  ...   \n",
       "55313                           0.0                    0.302459  ...   \n",
       "55314                           0.0                    0.119216  ...   \n",
       "55315                           1.0                    0.424669  ...   \n",
       "\n",
       "       bert_sim_Zephyr_7b_Beta PPL_Phi3_5_mini_Instruct  \\\n",
       "0                     0.468156             1.587546e+17   \n",
       "1                    -0.016905             1.862347e+15   \n",
       "3                     0.355749             4.429714e+12   \n",
       "4                     0.296120             2.191506e+17   \n",
       "5                     0.135701             8.355436e+13   \n",
       "...                        ...                      ...   \n",
       "55311                 0.636226             4.784381e+14   \n",
       "55312                 0.595697             8.400571e+15   \n",
       "55313                 0.202271             7.640358e+17   \n",
       "55314                 0.057623             2.014042e+18   \n",
       "55315                 0.268692             5.048042e+17   \n",
       "\n",
       "                           best_gen_Phi3_5_mini_Instruct  \\\n",
       "0      Digital distribution platforms Book industry o...   \n",
       "1                                         Not applicable   \n",
       "3                        Beta-blockers, muscle relaxants   \n",
       "4                                              Neurology   \n",
       "5                             Neurosurgery and Neurology   \n",
       "...                                                  ...   \n",
       "55311                    Japan is a country in East Asia   \n",
       "55312  Russia is a transcontinental country in Northe...   \n",
       "55313            Multifaceted and complex, with partners   \n",
       "55314  Relations vary with each country; please speci...   \n",
       "55315        Friendly, though sometimes strained, due to   \n",
       "\n",
       "       gen_recall_Phi3_5_mini_Instruct  bert_sim_Phi3_5_mini_Instruct  \\\n",
       "0                                  0.5                       0.615013   \n",
       "1                                  0.0                       0.205666   \n",
       "3                                  0.0                       0.342819   \n",
       "4                                  0.0                       0.407211   \n",
       "5                                  0.0                       0.323818   \n",
       "...                                ...                            ...   \n",
       "55311                              1.0                       0.697189   \n",
       "55312                              1.0                       0.605403   \n",
       "55313                              0.0                       0.091473   \n",
       "55314                              0.0                       0.371126   \n",
       "55315                              0.0                       0.211534   \n",
       "\n",
       "      subject_qid  object_qid  subject_popularity_sitelinks  \\\n",
       "0       Q56560668  Q112165919                             0   \n",
       "1       Q87300250  Q113491656                             0   \n",
       "3       Q65632660   Q95594627                             0   \n",
       "4       Q55092798   Q15751978                             0   \n",
       "5       Q55093271   Q15751978                             0   \n",
       "...           ...         ...                           ...   \n",
       "55311         Q17         Q17                           410   \n",
       "55312        Q159        Q159                           410   \n",
       "55313        Q159         Q17                           410   \n",
       "55314         Q43         Q17                           414   \n",
       "55315         Q43        Q159                           414   \n",
       "\n",
       "       object_popularity_sitelinks popularity_sitelinks_sum  \n",
       "0                                0                        0  \n",
       "1                                0                        0  \n",
       "3                                0                        0  \n",
       "4                                0                        0  \n",
       "5                                0                        0  \n",
       "...                            ...                      ...  \n",
       "55311                          410                      820  \n",
       "55312                          410                      820  \n",
       "55313                          410                      820  \n",
       "55314                          410                      824  \n",
       "55315                          410                      824  \n",
       "\n",
       "[55316 rows x 35 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = \"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv\"\n",
    "\n",
    "df = pd.read_csv(PATH)\n",
    "\n",
    "if \"popularity_sitelinks_sum\" not in df.columns:\n",
    "    raise ValueError(\"Нет колонки 'popularity_sitelinks_sum'.\")\n",
    "\n",
    "# стабильная сортировка (сохраняет порядок строк с одинаковым значением)\n",
    "df = df.sort_values(\"popularity_sitelinks_sum\", ascending=True, kind=\"mergesort\")\n",
    "\n",
    "df.to_csv(PATH, index=False)\n",
    "print(f\"✅ Отсортировано и сохранено: {PATH}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a6664f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'question', 'answer', 'subject', 'relation', 'object',\n",
       "       'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract',\n",
       "       'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract',\n",
       "       'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract',\n",
       "       'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract',\n",
       "       'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract',\n",
       "       'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract',\n",
       "       'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT',\n",
       "       'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta',\n",
       "       'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta',\n",
       "       'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct',\n",
       "       'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct',\n",
       "       'subject_qid', 'object_qid', 'subject_popularity_sitelinks',\n",
       "       'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20c49752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/full.csv  (rows=55316)\n",
      "Total rows: 55316\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_llama3_1b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_1b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_llama3_1b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_1b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_llama3_1b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_1b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_llama3_1b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_1b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_llama3_1b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_1b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_llama3_1b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_1b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_llama3_1b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_1b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_llama3_1b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_1b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_llama3_3b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_3b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_llama3_3b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_3b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_llama3_3b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_3b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_llama3_3b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_3b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_llama3_3b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_3b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_llama3_3b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_3b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_llama3_3b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_3b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_llama3_3b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_3b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_llama3_8b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_8b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_llama3_8b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_8b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_llama3_8b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_8b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_llama3_8b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_8b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_llama3_8b.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_llama3_8b.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_llama3_8b.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_llama3_8b.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_llama3_8b.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_llama3_8b.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_llama3_8b.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_llama3_8b.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_gemma7.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_gemma7.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_gemma7.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_gemma7.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_gemma7.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_gemma7.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_gemma7.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_gemma7.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_gemma7.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_gemma7.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_gemma7.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_gemma7.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_gemma7.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_gemma7.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_gemma7.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_gemma7.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_zephyr7.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_zephyr7.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_zephyr7.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_zephyr7.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_zephyr7.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_zephyr7.csv  (rows=49785)\n",
      "⚠️ [zephyr7][popular][15%] нужно 8297, но нашлось только 6234 подходящих (recall&sim>0.5)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_zephyr7.csv  (rows=6234)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_zephyr7.csv  (rows=49082)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_zephyr7.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_zephyr7.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_zephyr7.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_zephyr7.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_zephyr7.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_zephyr7.csv  (rows=49785)\n",
      "⚠️ [zephyr7][rare][15%] нужно 8297, но нашлось только 6234 подходящих (recall&sim>0.5)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_zephyr7.csv  (rows=6234)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_zephyr7.csv  (rows=49082)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_1_phi3.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_phi3.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_5_phi3.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_phi3.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_10_phi3.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_phi3.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/popular_forget_15_phi3.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_phi3.csv  (rows=47019)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_1_phi3.csv  (rows=553)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_1_phi3.csv  (rows=54763)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_5_phi3.csv  (rows=2765)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_5_phi3.csv  (rows=52551)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_10_phi3.csv  (rows=5531)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_10_phi3.csv  (rows=49785)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/rare_forget_15_phi3.csv  (rows=8297)\n",
      "saved: /mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits/retain_15_phi3.csv  (rows=47019)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== входные данные ====\n",
    "# df = pd.read_csv(\"...\")  # у тебя df уже есть в памяти, просто убери эту строку\n",
    "OUTPUT_DIR = \"/mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits\"\n",
    "\n",
    "# соответствия моделей их колонкам\n",
    "MODEL_COLS = {\n",
    "    \"llama3_1b\": (\"gen_recall_Llama_1b_Instract\", \"bert_sim_Llama_1b_Instract\"),\n",
    "    \"llama3_3b\": (\"gen_recall_Llama_3b_Instract\", \"bert_sim_Llama_3b_Instract\"),\n",
    "    \"llama3_8b\": (\"gen_recall_Llama_8b_Instract\", \"bert_sim_Llama_8b_Instract\"),\n",
    "    \"gemma7\"   : (\"gen_recall_Gemma_7b_IT\",       \"bert_sim_Gemma_7b_IT\"),\n",
    "    \"zephyr7\"  : (\"gen_recall_Zephyr_7b_Beta\",    \"bert_sim_Zephyr_7b_Beta\"),\n",
    "    \"phi3\"     : (\"gen_recall_Phi3_5_mini_Instruct\",\"bert_sim_Phi3_5_mini_Instruct\"),\n",
    "}\n",
    "\n",
    "PCTS = [1, 5, 10, 15]\n",
    "TYPES = [\"popular\", \"rare\"]  # popular: с конца; rare: с начала\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def select_forget_indices(df: pd.DataFrame, side: str, k: int,\n",
    "                          recall_col: str, sim_col: str) -> list[int]:\n",
    "    \"\"\"Идём от нужного конца и набираем k индексов, где recall>0.5 и sim>0.5.\"\"\"\n",
    "    # валидные строки для модели\n",
    "    cond = (df[recall_col] > 0.5) & (df[sim_col] > 0.5)\n",
    "    cond = cond.fillna(False)\n",
    "\n",
    "    order_idx = df.index if side == \"rare\" else df.index[::-1]  # rare: с начала, popular: с конца\n",
    "    chosen = []\n",
    "    for i in order_idx:\n",
    "        if cond.at[i]:\n",
    "            chosen.append(i)\n",
    "            if len(chosen) >= k:\n",
    "                break\n",
    "\n",
    "    # вернуть в исходном порядке датафрейма (стабильно)\n",
    "    chosen.sort()\n",
    "    return chosen\n",
    "\n",
    "def save_split(df: pd.DataFrame, name: str):\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    path = Path(OUTPUT_DIR) / f\"{name}.csv\"\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"saved: {path}  (rows={len(df)})\")\n",
    "\n",
    "def make_splits(df: pd.DataFrame):\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "    # full\n",
    "    save_split(df, \"full\")\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"Total rows: {total}\")\n",
    "\n",
    "    for model, (rec_col, sim_col) in MODEL_COLS.items():\n",
    "        # sanity check\n",
    "        for col in (rec_col, sim_col):\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Нет колонки '{col}' для модели '{model}'\")\n",
    "\n",
    "        for side in TYPES:\n",
    "            for p in PCTS:\n",
    "                k_target = int(total * p / 100)  # ⌊N% * |df|⌋\n",
    "                if k_target <= 0:\n",
    "                    print(f\"[{model}][{side}][{p}%] target=0 — пропускаю\")\n",
    "                    continue\n",
    "\n",
    "                forget_idx = select_forget_indices(df, side, k_target, rec_col, sim_col)\n",
    "                if len(forget_idx) < k_target:\n",
    "                    print(f\"⚠️ [{model}][{side}][{p}%] нужно {k_target}, но нашлось только {len(forget_idx)} подходящих (recall&sim>0.5)\")\n",
    "\n",
    "                forget_df = df.loc[forget_idx].copy()\n",
    "                retain_df = df.drop(index=forget_idx).copy()\n",
    "\n",
    "                # имена сплитов\n",
    "                forget_name = f\"{side}_forget_{p}_{model}\"\n",
    "                retain_name = f\"retain_{p}_{model}\"  # комплемент к forget\n",
    "\n",
    "                save_split(forget_df, forget_name)\n",
    "                save_split(retain_df, retain_name)\n",
    "\n",
    "# === вызов ===\n",
    "make_splits(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6496c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ae81705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded split:                           full  rows=55316\n",
      "Loaded split:       popular_forget_10_gemma7  rows=5531\n",
      "Loaded split:    popular_forget_10_llama3_1b  rows=5531\n",
      "Loaded split:    popular_forget_10_llama3_3b  rows=5531\n",
      "Loaded split:    popular_forget_10_llama3_8b  rows=5531\n",
      "Loaded split:         popular_forget_10_phi3  rows=5531\n",
      "Loaded split:      popular_forget_10_zephyr7  rows=5531\n",
      "Loaded split:       popular_forget_15_gemma7  rows=8297\n",
      "Loaded split:    popular_forget_15_llama3_1b  rows=8297\n",
      "Loaded split:    popular_forget_15_llama3_3b  rows=8297\n",
      "Loaded split:    popular_forget_15_llama3_8b  rows=8297\n",
      "Loaded split:         popular_forget_15_phi3  rows=8297\n",
      "Loaded split:      popular_forget_15_zephyr7  rows=6234\n",
      "Loaded split:        popular_forget_1_gemma7  rows=553\n",
      "Loaded split:     popular_forget_1_llama3_1b  rows=553\n",
      "Loaded split:     popular_forget_1_llama3_3b  rows=553\n",
      "Loaded split:     popular_forget_1_llama3_8b  rows=553\n",
      "Loaded split:          popular_forget_1_phi3  rows=553\n",
      "Loaded split:       popular_forget_1_zephyr7  rows=553\n",
      "Loaded split:        popular_forget_5_gemma7  rows=2765\n",
      "Loaded split:     popular_forget_5_llama3_1b  rows=2765\n",
      "Loaded split:     popular_forget_5_llama3_3b  rows=2765\n",
      "Loaded split:     popular_forget_5_llama3_8b  rows=2765\n",
      "Loaded split:          popular_forget_5_phi3  rows=2765\n",
      "Loaded split:       popular_forget_5_zephyr7  rows=2765\n",
      "Loaded split:          rare_forget_10_gemma7  rows=5531\n",
      "Loaded split:       rare_forget_10_llama3_1b  rows=5531\n",
      "Loaded split:       rare_forget_10_llama3_3b  rows=5531\n",
      "Loaded split:       rare_forget_10_llama3_8b  rows=5531\n",
      "Loaded split:            rare_forget_10_phi3  rows=5531\n",
      "Loaded split:         rare_forget_10_zephyr7  rows=5531\n",
      "Loaded split:          rare_forget_15_gemma7  rows=8297\n",
      "Loaded split:       rare_forget_15_llama3_1b  rows=8297\n",
      "Loaded split:       rare_forget_15_llama3_3b  rows=8297\n",
      "Loaded split:       rare_forget_15_llama3_8b  rows=8297\n",
      "Loaded split:            rare_forget_15_phi3  rows=8297\n",
      "Loaded split:         rare_forget_15_zephyr7  rows=6234\n",
      "Loaded split:           rare_forget_1_gemma7  rows=553\n",
      "Loaded split:        rare_forget_1_llama3_1b  rows=553\n",
      "Loaded split:        rare_forget_1_llama3_3b  rows=553\n",
      "Loaded split:        rare_forget_1_llama3_8b  rows=553\n",
      "Loaded split:             rare_forget_1_phi3  rows=553\n",
      "Loaded split:          rare_forget_1_zephyr7  rows=553\n",
      "Loaded split:           rare_forget_5_gemma7  rows=2765\n",
      "Loaded split:        rare_forget_5_llama3_1b  rows=2765\n",
      "Loaded split:        rare_forget_5_llama3_3b  rows=2765\n",
      "Loaded split:        rare_forget_5_llama3_8b  rows=2765\n",
      "Loaded split:             rare_forget_5_phi3  rows=2765\n",
      "Loaded split:          rare_forget_5_zephyr7  rows=2765\n",
      "Loaded split:               retain_10_gemma7  rows=49785\n",
      "Loaded split:            retain_10_llama3_1b  rows=49785\n",
      "Loaded split:            retain_10_llama3_3b  rows=49785\n",
      "Loaded split:            retain_10_llama3_8b  rows=49785\n",
      "Loaded split:                 retain_10_phi3  rows=49785\n",
      "Loaded split:              retain_10_zephyr7  rows=49785\n",
      "Loaded split:               retain_15_gemma7  rows=47019\n",
      "Loaded split:            retain_15_llama3_1b  rows=47019\n",
      "Loaded split:            retain_15_llama3_3b  rows=47019\n",
      "Loaded split:            retain_15_llama3_8b  rows=47019\n",
      "Loaded split:                 retain_15_phi3  rows=47019\n",
      "Loaded split:              retain_15_zephyr7  rows=49082\n",
      "Loaded split:                retain_1_gemma7  rows=54763\n",
      "Loaded split:             retain_1_llama3_1b  rows=54763\n",
      "Loaded split:             retain_1_llama3_3b  rows=54763\n",
      "Loaded split:             retain_1_llama3_8b  rows=54763\n",
      "Loaded split:                  retain_1_phi3  rows=54763\n",
      "Loaded split:               retain_1_zephyr7  rows=54763\n",
      "Loaded split:                retain_5_gemma7  rows=52551\n",
      "Loaded split:             retain_5_llama3_1b  rows=52551\n",
      "Loaded split:             retain_5_llama3_3b  rows=52551\n",
      "Loaded split:             retain_5_llama3_8b  rows=52551\n",
      "Loaded split:                  retain_5_phi3  rows=52551\n",
      "Loaded split:               retain_5_zephyr7  rows=52551\n",
      "\n",
      "=== DatasetDict summary ===\n",
      "                          full: 55316 rows, 35 cols\n",
      "      popular_forget_10_gemma7: 5531 rows, 35 cols\n",
      "   popular_forget_10_llama3_1b: 5531 rows, 35 cols\n",
      "   popular_forget_10_llama3_3b: 5531 rows, 35 cols\n",
      "   popular_forget_10_llama3_8b: 5531 rows, 35 cols\n",
      "        popular_forget_10_phi3: 5531 rows, 35 cols\n",
      "     popular_forget_10_zephyr7: 5531 rows, 35 cols\n",
      "      popular_forget_15_gemma7: 8297 rows, 35 cols\n",
      "   popular_forget_15_llama3_1b: 8297 rows, 35 cols\n",
      "   popular_forget_15_llama3_3b: 8297 rows, 35 cols\n",
      "   popular_forget_15_llama3_8b: 8297 rows, 35 cols\n",
      "        popular_forget_15_phi3: 8297 rows, 35 cols\n",
      "     popular_forget_15_zephyr7: 6234 rows, 35 cols\n",
      "       popular_forget_1_gemma7: 553 rows, 35 cols\n",
      "    popular_forget_1_llama3_1b: 553 rows, 35 cols\n",
      "    popular_forget_1_llama3_3b: 553 rows, 35 cols\n",
      "    popular_forget_1_llama3_8b: 553 rows, 35 cols\n",
      "         popular_forget_1_phi3: 553 rows, 35 cols\n",
      "      popular_forget_1_zephyr7: 553 rows, 35 cols\n",
      "       popular_forget_5_gemma7: 2765 rows, 35 cols\n",
      "    popular_forget_5_llama3_1b: 2765 rows, 35 cols\n",
      "    popular_forget_5_llama3_3b: 2765 rows, 35 cols\n",
      "    popular_forget_5_llama3_8b: 2765 rows, 35 cols\n",
      "         popular_forget_5_phi3: 2765 rows, 35 cols\n",
      "      popular_forget_5_zephyr7: 2765 rows, 35 cols\n",
      "         rare_forget_10_gemma7: 5531 rows, 35 cols\n",
      "      rare_forget_10_llama3_1b: 5531 rows, 35 cols\n",
      "      rare_forget_10_llama3_3b: 5531 rows, 35 cols\n",
      "      rare_forget_10_llama3_8b: 5531 rows, 35 cols\n",
      "           rare_forget_10_phi3: 5531 rows, 35 cols\n",
      "        rare_forget_10_zephyr7: 5531 rows, 35 cols\n",
      "         rare_forget_15_gemma7: 8297 rows, 35 cols\n",
      "      rare_forget_15_llama3_1b: 8297 rows, 35 cols\n",
      "      rare_forget_15_llama3_3b: 8297 rows, 35 cols\n",
      "      rare_forget_15_llama3_8b: 8297 rows, 35 cols\n",
      "           rare_forget_15_phi3: 8297 rows, 35 cols\n",
      "        rare_forget_15_zephyr7: 6234 rows, 35 cols\n",
      "          rare_forget_1_gemma7: 553 rows, 35 cols\n",
      "       rare_forget_1_llama3_1b: 553 rows, 35 cols\n",
      "       rare_forget_1_llama3_3b: 553 rows, 35 cols\n",
      "       rare_forget_1_llama3_8b: 553 rows, 35 cols\n",
      "            rare_forget_1_phi3: 553 rows, 35 cols\n",
      "         rare_forget_1_zephyr7: 553 rows, 35 cols\n",
      "          rare_forget_5_gemma7: 2765 rows, 35 cols\n",
      "       rare_forget_5_llama3_1b: 2765 rows, 35 cols\n",
      "       rare_forget_5_llama3_3b: 2765 rows, 35 cols\n",
      "       rare_forget_5_llama3_8b: 2765 rows, 35 cols\n",
      "            rare_forget_5_phi3: 2765 rows, 35 cols\n",
      "         rare_forget_5_zephyr7: 2765 rows, 35 cols\n",
      "              retain_10_gemma7: 49785 rows, 35 cols\n",
      "           retain_10_llama3_1b: 49785 rows, 35 cols\n",
      "           retain_10_llama3_3b: 49785 rows, 35 cols\n",
      "           retain_10_llama3_8b: 49785 rows, 35 cols\n",
      "                retain_10_phi3: 49785 rows, 35 cols\n",
      "             retain_10_zephyr7: 49785 rows, 35 cols\n",
      "              retain_15_gemma7: 47019 rows, 35 cols\n",
      "           retain_15_llama3_1b: 47019 rows, 35 cols\n",
      "           retain_15_llama3_3b: 47019 rows, 35 cols\n",
      "           retain_15_llama3_8b: 47019 rows, 35 cols\n",
      "                retain_15_phi3: 47019 rows, 35 cols\n",
      "             retain_15_zephyr7: 49082 rows, 35 cols\n",
      "               retain_1_gemma7: 54763 rows, 35 cols\n",
      "            retain_1_llama3_1b: 54763 rows, 35 cols\n",
      "            retain_1_llama3_3b: 54763 rows, 35 cols\n",
      "            retain_1_llama3_8b: 54763 rows, 35 cols\n",
      "                 retain_1_phi3: 54763 rows, 35 cols\n",
      "              retain_1_zephyr7: 54763 rows, 35 cols\n",
      "               retain_5_gemma7: 52551 rows, 35 cols\n",
      "            retain_5_llama3_1b: 52551 rows, 35 cols\n",
      "            retain_5_llama3_3b: 52551 rows, 35 cols\n",
      "            retain_5_llama3_8b: 52551 rows, 35 cols\n",
      "                 retain_5_phi3: 52551 rows, 35 cols\n",
      "              retain_5_zephyr7: 52551 rows, 35 cols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 55316/55316 [00:00<00:00, 314035.32 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 213917.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 218810.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 218435.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 217631.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 217932.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 218211.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 249710.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 250665.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 250420.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 249771.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 239281.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6234/6234 [00:00<00:00, 221728.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45434.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45043.11 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45458.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 46003.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 46417.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 43811.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 146969.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 155317.55 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 151726.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 152487.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 154898.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 150484.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 209652.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 209622.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 206778.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 214288.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 213816.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5531/5531 [00:00<00:00, 215615.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 246779.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 247088.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 236954.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 243252.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8297/8297 [00:00<00:00, 245845.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6234/6234 [00:00<00:00, 221895.61 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 44732.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45169.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45530.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45636.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 45894.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 553/553 [00:00<00:00, 46398.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 158037.29 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 152111.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 150556.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 156594.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 153532.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2765/2765 [00:00<00:00, 156766.21 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 344313.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 342785.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 347289.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 344486.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 343910.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49785/49785 [00:00<00:00, 345666.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47019/47019 [00:00<00:00, 339882.94 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47019/47019 [00:00<00:00, 277881.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47019/47019 [00:00<00:00, 322203.36 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47019/47019 [00:00<00:00, 328951.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 47019/47019 [00:00<00:00, 334473.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 49082/49082 [00:00<00:00, 332522.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 345561.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 345266.48 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 341282.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 341209.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 342398.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 54763/54763 [00:00<00:00, 343150.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 339567.88 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 242071.03 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 342389.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 342641.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 342962.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52551/52551 [00:00<00:00, 343001.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved DatasetDict to: /mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/tripunlamb_hfds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 56/56 [00:00<00:00, 241.88ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.08s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 273.31ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 272.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.47s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 275.18ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.39s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 279.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 279.88ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 245.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.35s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 192.12ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.18s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 268.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.18s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 285.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.23s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 201.33ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.20s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 253.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.39s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 257.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 332.27ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.01 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 318.06ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.06 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 311.01ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 329.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.02s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 374.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.04s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 344.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.16s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 262.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 265.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 262.96ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.02s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 267.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 266.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.09s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 253.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.13s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 244.45ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.34s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 252.06ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.15s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 247.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.16s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 249.53ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 238.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.10s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 249.99ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.03s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 264.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.13s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 266.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.24s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 202.20ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.19s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 251.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.57s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 251.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.18s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 268.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.95 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 328.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.02s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 316.79ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.00s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 332.64ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.00 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 290.40ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.03s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 312.17ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 300.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.07s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 152.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.12s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 157.35ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.48s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 159.77ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.13s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 256.65ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.09s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 150.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.27s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 238.92ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 237.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.74s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 251.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.87s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 246.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.75s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 254.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.78s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 247.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 249.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.75s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:00<00:00, 247.44ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.73s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:00<00:00, 255.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.71s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:00<00:00, 252.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.81s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:00<00:00, 256.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.18s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 48/48 [00:00<00:00, 259.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.94s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 252.50ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.83s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 249.15ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.73s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 229.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.45s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 241.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 250.42ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.86s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 241.91ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.80s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 237.43ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.02s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 249.00ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.66s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 233.09ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.63s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 252.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.91s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 236.83ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.56s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 250.80ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.18s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 238.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.58s/ shards]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict  # pip install datasets\n",
    "\n",
    "# Папка со сплитами, которые создавали ранее\n",
    "SPLITS_DIR = Path(\"/mnt/extremessd10tb/borisiuk/open-unlearning/tripunlamb_splits\")\n",
    "OUT_DIR    = Path(\"/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/tripunlamb_hfds\")  # куда сохранить HF-датасет\n",
    "\n",
    "def read_header_cols(csv_path: Path) -> list[str]:\n",
    "    return list(pd.read_csv(csv_path, nrows=0).columns)\n",
    "\n",
    "def load_split(csv_path: Path, all_cols: list[str]) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # досоздать недостающие колонки и привести к единому порядку\n",
    "    for c in all_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    df = df.reindex(columns=all_cols)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    assert SPLITS_DIR.exists(), f\"Не найдена папка со сплитами: {SPLITS_DIR}\"\n",
    "\n",
    "    csvs = sorted([p for p in SPLITS_DIR.glob(\"*.csv\") if p.is_file()])\n",
    "    if not csvs:\n",
    "        raise RuntimeError(f\"В {SPLITS_DIR} нет .csv файлов\")\n",
    "\n",
    "    # базовая схема: из full.csv, иначе — по первому файлу\n",
    "    full_csv = next((p for p in csvs if p.stem == \"full\"), None)\n",
    "    schema_src = full_csv or csvs[0]\n",
    "    all_cols = read_header_cols(schema_src)\n",
    "\n",
    "    # собираем датасет дикт\n",
    "    dsets = {}\n",
    "    for p in csvs:\n",
    "        df = load_split(p, all_cols)\n",
    "        dsets[p.stem] = Dataset.from_pandas(df, preserve_index=False)\n",
    "        print(f\"Loaded split: {p.stem:>30s}  rows={len(df)}\")\n",
    "\n",
    "    dd = DatasetDict(dsets)\n",
    "\n",
    "    # краткий отчёт\n",
    "    print(\"\\n=== DatasetDict summary ===\")\n",
    "    for name, ds in dd.items():\n",
    "        print(f\"{name:>30s}: {ds.num_rows} rows, {len(ds.features)} cols\")\n",
    "\n",
    "    # сохранить на диск в формате HF datasets (Arrow)\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    dd.save_to_disk(str(OUT_DIR))\n",
    "    print(f\"\\n✅ Saved DatasetDict to: {OUT_DIR}\")\n",
    "\n",
    "    # (опционально) пуш на Hugging Face Hub:\n",
    "    dd.push_to_hub(\"SwetieePawsss/TripUNLamb\")\n",
    "    # dd.push_to_hub(\"user_or_org/tripunlamb\", private=True)  # потребуются токены HF\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef854ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056e31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dff5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b3e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170a4cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/extremessd10tb/borisiuk/miniconda3/envs/triplets/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!export HF_HOME=\"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\"\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Задаем токен как переменную окружения в коде\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Ваш токен, полученный в настройках Hugging Face\n",
    "token = \"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\"\n",
    "\n",
    "# Авторизация с использованием токена\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5d7be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d505e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== gen_recall (>0.5) per column ===\n",
      "gen_recall_Llama_8b_Instract           22126 / 55316  ( 40.0%)   NaN: 0\n",
      "gen_recall_Llama_3b_Instract           18305 / 55316  ( 33.1%)   NaN: 0\n",
      "gen_recall_Llama_1b_Instract           14098 / 55316  ( 25.5%)   NaN: 0\n",
      "gen_recall_Gemma_7b_IT                 12416 / 55316  ( 22.4%)   NaN: 0\n",
      "gen_recall_Zephyr_7b_Beta               9729 / 55316  ( 17.6%)   NaN: 0\n",
      "gen_recall_Phi3_5_mini_Instruct        17930 / 55316  ( 32.4%)   NaN: 0\n",
      "\n",
      "=== bert_sim (>0.5) per column ===\n",
      "bert_sim_Llama_8b_Instract             25493 / 55316  ( 46.1%)   NaN: 0\n",
      "bert_sim_Llama_3b_Instract             23070 / 55316  ( 41.7%)   NaN: 0\n",
      "bert_sim_Llama_1b_Instract             20703 / 55316  ( 37.4%)   NaN: 0\n",
      "bert_sim_Gemma_7b_IT                   13382 / 55316  ( 24.2%)   NaN: 0\n",
      "bert_sim_Zephyr_7b_Beta                 8915 / 55316  ( 16.1%)   NaN: 0\n",
      "bert_sim_Phi3_5_mini_Instruct          20151 / 55316  ( 36.4%)   NaN: 0\n",
      "\n",
      "=== Intersections (>0.5) ===\n",
      "All gen_recalls: 4807 / 55316 (8.7%)\n",
      "All bert_sims : 4628 / 55316 (8.4%)\n",
      "ALL (rec & sim): 2780 / 55316 (5.0%)\n",
      "\n",
      "=== Cumulative intersection: gen_recalls ===\n",
      "start -> 55316\n",
      "+ gen_recall_Llama_8b_Instract        => 22126  (Δ -33190)\n",
      "+ gen_recall_Llama_3b_Instract        => 16104  (Δ -6022)\n",
      "+ gen_recall_Llama_1b_Instract        => 11062  (Δ -5042)\n",
      "+ gen_recall_Gemma_7b_IT              => 8049  (Δ -3013)\n",
      "+ gen_recall_Zephyr_7b_Beta           => 4937  (Δ -3112)\n",
      "+ gen_recall_Phi3_5_mini_Instruct     => 4807  (Δ -130)\n",
      "\n",
      "=== Cumulative intersection: bert_sims ===\n",
      "start -> 55316\n",
      "+ bert_sim_Llama_8b_Instract          => 25493  (Δ -29823)\n",
      "+ bert_sim_Llama_3b_Instract          => 19489  (Δ -6004)\n",
      "+ bert_sim_Llama_1b_Instract          => 14732  (Δ -4757)\n",
      "+ bert_sim_Gemma_7b_IT                => 9116  (Δ -5616)\n",
      "+ bert_sim_Zephyr_7b_Beta             => 4853  (Δ -4263)\n",
      "+ bert_sim_Phi3_5_mini_Instruct       => 4628  (Δ -225)\n",
      "\n",
      "=== Cumulative intersection: ALL (recalls then sims) ===\n",
      "start -> 55316\n",
      "+ gen_recall_Llama_8b_Instract        => 22126  (Δ -33190)\n",
      "+ gen_recall_Llama_3b_Instract        => 16104  (Δ -6022)\n",
      "+ gen_recall_Llama_1b_Instract        => 11062  (Δ -5042)\n",
      "+ gen_recall_Gemma_7b_IT              => 8049  (Δ -3013)\n",
      "+ gen_recall_Zephyr_7b_Beta           => 4937  (Δ -3112)\n",
      "+ gen_recall_Phi3_5_mini_Instruct     => 4807  (Δ -130)\n",
      "+ bert_sim_Llama_8b_Instract          => 4557  (Δ -250)\n",
      "+ bert_sim_Llama_3b_Instract          => 4382  (Δ -175)\n",
      "+ bert_sim_Llama_1b_Instract          => 4291  (Δ -91)\n",
      "+ bert_sim_Gemma_7b_IT                => 3623  (Δ -668)\n",
      "+ bert_sim_Zephyr_7b_Beta             => 2824  (Δ -799)\n",
      "+ bert_sim_Phi3_5_mini_Instruct       => 2780  (Δ -44)\n",
      "\n",
      "Naive expected ALL-pass (product of per-column rates): ~0 rows (0.0%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# df уже в памяти\n",
    "\n",
    "RECALL_COLS = [\n",
    "    \"gen_recall_Llama_8b_Instract\",\n",
    "    \"gen_recall_Llama_3b_Instract\",\n",
    "    \"gen_recall_Llama_1b_Instract\",\n",
    "    \"gen_recall_Gemma_7b_IT\",\n",
    "    \"gen_recall_Zephyr_7b_Beta\",\n",
    "    \"gen_recall_Phi3_5_mini_Instruct\",\n",
    "]\n",
    "SIM_COLS = [\n",
    "    \"bert_sim_Llama_8b_Instract\",\n",
    "    \"bert_sim_Llama_3b_Instract\",\n",
    "    \"bert_sim_Llama_1b_Instract\",\n",
    "    \"bert_sim_Gemma_7b_IT\",\n",
    "    \"bert_sim_Zephyr_7b_Beta\",\n",
    "    \"bert_sim_Phi3_5_mini_Instruct\",\n",
    "]\n",
    "\n",
    "# sanity\n",
    "missing = [c for c in RECALL_COLS + SIM_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"В df отсутствуют колонки: {missing}\")\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "def per_column_report(cols, title):\n",
    "    print(f\"\\n=== {title} (>0.5) per column ===\")\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        n = int((s > 0.5).sum())\n",
    "        pct = 100.0 * n / total if total else 0.0\n",
    "        na = int(s.isna().sum())\n",
    "        print(f\"{c:35s}  {n:7d} / {total}  ({pct:5.1f}%)   NaN: {na}\")\n",
    "\n",
    "per_column_report(RECALL_COLS, \"gen_recall\")\n",
    "per_column_report(SIM_COLS,    \"bert_sim\")\n",
    "\n",
    "rec_all_mask  = (df[RECALL_COLS] > 0.5).all(axis=1)\n",
    "sim_all_mask  = (df[SIM_COLS] > 0.5).all(axis=1)\n",
    "both_all_mask = rec_all_mask & sim_all_mask\n",
    "\n",
    "def p(n):  # формат процентов\n",
    "    return f\"{(100.0*n/total if total else 0.0):.1f}%\"\n",
    "\n",
    "print(\"\\n=== Intersections (>0.5) ===\")\n",
    "print(f\"All gen_recalls: {int(rec_all_mask.sum())} / {total} ({p(rec_all_mask.sum())})\")\n",
    "print(f\"All bert_sims : {int(sim_all_mask.sum())} / {total} ({p(sim_all_mask.sum())})\")\n",
    "print(f\"ALL (rec & sim): {int(both_all_mask.sum())} / {total} ({p(both_all_mask.sum())})\")\n",
    "\n",
    "# Пошаговое (кумулятивное) сужение пересечения, чтобы увидеть «узкие» колонки\n",
    "def cumulative_intersection(cols, title):\n",
    "    print(f\"\\n=== Cumulative intersection: {title} ===\")\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    prev = int(mask.sum())\n",
    "    print(f\"start -> {prev}\")\n",
    "    for c in cols:\n",
    "        mask = mask & (df[c] > 0.5)\n",
    "        now = int(mask.sum())\n",
    "        print(f\"+ {c:35s} => {now}  (Δ {now - prev})\")\n",
    "        prev = now\n",
    "    return mask\n",
    "\n",
    "_ = cumulative_intersection(RECALL_COLS, \"gen_recalls\")\n",
    "_ = cumulative_intersection(SIM_COLS,    \"bert_sims\")\n",
    "_ = cumulative_intersection(RECALL_COLS + SIM_COLS, \"ALL (recalls then sims)\")\n",
    "\n",
    "# Наивная оценка: произведение индивидуальных долей (чтобы понять порядок величины)\n",
    "rates = [(df[c] > 0.5).mean() for c in (RECALL_COLS + SIM_COLS)]\n",
    "naive_expected = int(round(total * reduce(mul, rates, 1.0)))\n",
    "print(f\"\\nNaive expected ALL-pass (product of per-column rates): ~{naive_expected} rows \"\n",
    "      f\"({(100.0*naive_expected/total if total else 0.0):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ce429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection rare 1%] total=55316 target=553 chosen_forget=553 | boundary popularity_sitelinks_sum=133\n",
      "[know_intersection popular 1%] total=55316 target=553 chosen_forget=553 | boundary popularity_sitelinks_sum=820\n",
      "[know_intersection retain 98%] size=54210\n",
      "[know_intersection rare 5%] total=55316 target=2765 chosen_forget=2765 | boundary popularity_sitelinks_sum=745\n",
      "[know_intersection popular 5%] total=55316 target=2765 chosen_forget=2765 | boundary popularity_sitelinks_sum=820\n",
      "[know_intersection retain 90%] size=52536\n",
      "[ranking rare 1%] total=55316 target=553 chosen_forget=553 | boundary popularity_sitelinks_sum=6\n",
      "[ranking popular 1%] total=55316 target=553 chosen_forget=553 | boundary popularity_sitelinks_sum=824\n",
      "[ranking retain 98%] size=54210\n",
      "[ranking rare 5%] total=55316 target=2765 chosen_forget=2765 | boundary popularity_sitelinks_sum=20\n",
      "[ranking popular 5%] total=55316 target=2765 chosen_forget=2765 | boundary popularity_sitelinks_sum=824\n",
      "[ranking retain 90%] size=49786\n",
      "[ranking rare 10%] total=55316 target=5531 chosen_forget=5531 | boundary popularity_sitelinks_sum=35\n",
      "[ranking popular 10%] total=55316 target=5531 chosen_forget=5531 | boundary popularity_sitelinks_sum=824\n",
      "[ranking retain 80%] size=44254\n",
      "[ranking rare 15%] total=55316 target=8297 chosen_forget=8297 | boundary popularity_sitelinks_sum=49\n",
      "[ranking popular 15%] total=55316 target=8297 chosen_forget=8297 | boundary popularity_sitelinks_sum=824\n",
      "[ranking retain 70%] size=38722\n",
      "DatasetDict({\n",
      "    full: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 55316\n",
      "    })\n",
      "    know_intersection_rare_forget_1: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 553\n",
      "    })\n",
      "    know_intersection_popular_forget_1: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 553\n",
      "    })\n",
      "    know_intersection_retain_98: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 54210\n",
      "    })\n",
      "    know_intersection_rare_forget_5: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 2765\n",
      "    })\n",
      "    know_intersection_popular_forget_5: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 2765\n",
      "    })\n",
      "    know_intersection_retain_90: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 52536\n",
      "    })\n",
      "    rare_forget_1: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 553\n",
      "    })\n",
      "    popular_forget_1: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 553\n",
      "    })\n",
      "    retain_98: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 54210\n",
      "    })\n",
      "    rare_forget_5: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 2765\n",
      "    })\n",
      "    popular_forget_5: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 2765\n",
      "    })\n",
      "    retain_90: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 49786\n",
      "    })\n",
      "    rare_forget_10: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 5531\n",
      "    })\n",
      "    popular_forget_10: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 5531\n",
      "    })\n",
      "    retain_80: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 44254\n",
      "    })\n",
      "    rare_forget_15: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 8297\n",
      "    })\n",
      "    popular_forget_15: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 8297\n",
      "    })\n",
      "    retain_70: Dataset({\n",
      "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
      "        num_rows: 38722\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# df: pd.DataFrame уже в памяти и ОТСОРТИРОВАН по популярности (вниз -> rare ... вверх -> popular)\n",
    "df = pd.read_csv('/mnt/extremessd10tb/borisiuk/open-unlearning/constuction_full_triplets_ds/FIXED_final_QA_triplets.csv')\n",
    "\n",
    "RECALL_COLS = [\n",
    "    \"gen_recall_Llama_8b_Instract\",\n",
    "    \"gen_recall_Llama_3b_Instract\",\n",
    "    \"gen_recall_Llama_1b_Instract\",\n",
    "    \"gen_recall_Gemma_7b_IT\",\n",
    "    \"gen_recall_Zephyr_7b_Beta\",\n",
    "    \"gen_recall_Phi3_5_mini_Instruct\",\n",
    "]\n",
    "SIM_COLS = [\n",
    "    \"bert_sim_Llama_8b_Instract\",\n",
    "    \"bert_sim_Llama_3b_Instract\",\n",
    "    \"bert_sim_Llama_1b_Instract\",\n",
    "    \"bert_sim_Gemma_7b_IT\",\n",
    "    \"bert_sim_Zephyr_7b_Beta\",\n",
    "    \"bert_sim_Phi3_5_mini_Instruct\",\n",
    "]\n",
    "\n",
    "need = RECALL_COLS + SIM_COLS + [\"popularity_sitelinks_sum\"]\n",
    "miss = [c for c in need if c not in df.columns]\n",
    "if miss:\n",
    "    raise ValueError(f\"В df отсутствуют колонки: {miss}\")\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "# маска «знания» по всем моделям\n",
    "quality_mask = (df[RECALL_COLS] > 0.5).all(axis=1) & (df[SIM_COLS] > 0.5).all(axis=1)\n",
    "\n",
    "def target_n(pct: int) -> int:\n",
    "    return int(total * pct / 100)  # floor\n",
    "\n",
    "def subset_dataset(dataframe: pd.DataFrame, idx_seq) -> Dataset:\n",
    "    return Dataset.from_pandas(dataframe.iloc[idx_seq], preserve_index=False)\n",
    "\n",
    "def forget_indices_ranking(kind: str, pct: int) -> list[int]:\n",
    "    \"\"\"Без пересечений знаний: rare = первые n; popular = последние n (в обратном порядке).\"\"\"\n",
    "    n = target_n(pct)\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    if 2*n > total:\n",
    "        raise ValueError(f\"N слишком велик: 2*N% > 100% (pct={pct}, total={total})\")\n",
    "    if kind == \"rare\":\n",
    "        return list(range(0, n))\n",
    "    if kind == \"popular\":\n",
    "        return list(range(total - 1, total - 1 - n, -1))  # с конца к началу\n",
    "    raise ValueError(\"kind должен быть 'popular' или 'rare'\")\n",
    "\n",
    "def forget_indices_intersection(kind: str, pct: int) -> list[int]:\n",
    "    \"\"\"Полное пересечение знаний: набираем ровно n, двигаясь от края и пропуская некачественные.\"\"\"\n",
    "    n = target_n(pct)\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    if 2*n > total:\n",
    "        raise ValueError(f\"N слишком велик: 2*N% > 100% (pct={pct}, total={total})\")\n",
    "\n",
    "    out = []\n",
    "    if kind == \"rare\":\n",
    "        for i in range(total):\n",
    "            if quality_mask.iat[i]:\n",
    "                out.append(i)\n",
    "                if len(out) == n: break\n",
    "    elif kind == \"popular\":\n",
    "        for i in range(total-1, -1, -1):\n",
    "            if quality_mask.iat[i]:\n",
    "                out.append(i)\n",
    "                if len(out) == n: break\n",
    "    else:\n",
    "        raise ValueError(\"kind должен быть 'popular' или 'rare'\")\n",
    "\n",
    "    if len(out) != n:\n",
    "        have = int(quality_mask.sum())\n",
    "        raise ValueError(\n",
    "            f\"[know_intersection {kind} {pct}%] Недостаточно качественных строк: \"\n",
    "            f\"нужно {n}, доступно {have}.\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def complement_retain_idx(rare_idx: list[int], popular_idx: list[int]) -> list[int]:\n",
    "    \"\"\"Ретейн — строки, которые не попали ни в rare, ни в popular (т.е. середина).\n",
    "       Размер ≈ total-2n (точно для ранжирования; может быть > при пересечении наборов в know_intersection).\"\"\"\n",
    "    forget_union = set(rare_idx) | set(popular_idx)\n",
    "    retain_idx = [i for i in range(total) if i not in forget_union]\n",
    "    return retain_idx\n",
    "\n",
    "def report_forget(kind: str, pct: int, forget_idx: list[int], tag: str):\n",
    "    pop_vals = df.iloc[forget_idx][\"popularity_sitelinks_sum\"].tolist()\n",
    "    boundary = pop_vals[-1] if kind == \"rare\" else pop_vals[0]\n",
    "    print(\n",
    "        f\"[{tag} {kind} {pct}%] total={total} target={target_n(pct)} \"\n",
    "        f\"chosen_forget={len(forget_idx)} | boundary popularity_sitelinks_sum={boundary}\"\n",
    "    )\n",
    "\n",
    "# ---- строим DatasetDict ----\n",
    "splits = {}\n",
    "splits[\"full\"] = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "# A) know_intersection: только 1% и 5%\n",
    "for pct in [1, 5]:\n",
    "    rare_f = forget_indices_intersection(\"rare\", pct)\n",
    "    pop_f  = forget_indices_intersection(\"popular\", pct)\n",
    "    retain = complement_retain_idx(rare_f, pop_f)\n",
    "\n",
    "    report_forget(\"rare\", pct, rare_f,  tag=\"know_intersection\")\n",
    "    report_forget(\"popular\", pct, pop_f, tag=\"know_intersection\")\n",
    "    print(f\"[know_intersection retain {100-2*pct}%] size={len(retain)}\")\n",
    "\n",
    "    splits[f\"know_intersection_rare_forget_{pct}\"]      = subset_dataset(df, rare_f)\n",
    "    splits[f\"know_intersection_popular_forget_{pct}\"]   = subset_dataset(df, pop_f)\n",
    "    splits[f\"know_intersection_retain_{100-2*pct}\"]     = subset_dataset(df, retain)\n",
    "\n",
    "# B) Ранжирование: 1,5,10,15%\n",
    "for pct in [1, 5, 10, 15]:\n",
    "    rare_f = forget_indices_ranking(\"rare\", pct)\n",
    "    pop_f  = forget_indices_ranking(\"popular\", pct)\n",
    "    retain = complement_retain_idx(rare_f, pop_f)\n",
    "\n",
    "    report_forget(\"rare\", pct, rare_f,  tag=\"ranking\")\n",
    "    report_forget(\"popular\", pct, pop_f, tag=\"ranking\")\n",
    "    print(f\"[ranking retain {100-2*pct}%] size={len(retain)}\")\n",
    "\n",
    "    splits[f\"rare_forget_{pct}\"]    = subset_dataset(df, rare_f)\n",
    "    splits[f\"popular_forget_{pct}\"] = subset_dataset(df, pop_f)\n",
    "    splits[f\"retain_{100-2*pct}\"]   = subset_dataset(df, retain)\n",
    "\n",
    "# Готовый датадикт (в памяти)\n",
    "tripunlamb = DatasetDict(splits)\n",
    "print(tripunlamb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2672b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 56/56 [00:00<00:00, 234.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.68s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 302.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.08 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 383.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.17s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 256.42ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.35s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 265.29ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.73s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 262.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 255.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.95s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 311.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.08 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 237.85ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.01s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 257.13ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.77s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 191.63ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.01s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 268.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.02 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 245.78ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.81s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 255.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.05 shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 279.81ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.01s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 45/45 [00:00<00:00, 249.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.77s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 251.11ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:00<00:00, 279.08ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.06s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 39/39 [00:00<00:00, 246.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.48s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/SwetieePawsss/TripUNLamb/commit/e39802a67535fae8cada8fab45d67cbccecf2c73', commit_message='Upload dataset', commit_description='', oid='e39802a67535fae8cada8fab45d67cbccecf2c73', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/SwetieePawsss/TripUNLamb', endpoint='https://huggingface.co', repo_type='dataset', repo_id='SwetieePawsss/TripUNLamb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_splits.push_to_hub(\"SwetieePawsss/TripUNLamb\")\n",
    "tripunlamb.push_to_hub(\"SwetieePawsss/TripUNLamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88c8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b25505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc36314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4296230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits found: ['full', 'know_intersection_rare_forget_1', 'know_intersection_popular_forget_1', 'know_intersection_retain_98', 'know_intersection_rare_forget_5', 'know_intersection_popular_forget_5', 'know_intersection_retain_90', 'rare_forget_1', 'popular_forget_1', 'retain_98', 'rare_forget_5', 'popular_forget_5', 'retain_90', 'rare_forget_10', 'popular_forget_10', 'retain_80', 'rare_forget_15', 'popular_forget_15', 'retain_70']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 55316/55316 [00:01<00:00, 46508.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[full] before=55316, after=54975, removed=341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 553/553 [00:00<00:00, 33253.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_rare_forget_1] before=553, after=516, removed=37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 553/553 [00:00<00:00, 34160.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_popular_forget_1] before=553, after=415, removed=138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 54210/54210 [00:01<00:00, 47031.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_retain_98] before=54210, after=54044, removed=166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2765/2765 [00:00<00:00, 42605.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_rare_forget_5] before=2765, after=2548, removed=217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2765/2765 [00:00<00:00, 43104.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_popular_forget_5] before=2765, after=2540, removed=225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 52536/52536 [00:01<00:00, 46751.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[know_intersection_retain_90] before=52536, after=52424, removed=112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 553/553 [00:00<00:00, 33593.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rare_forget_1] before=553, after=542, removed=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 553/553 [00:00<00:00, 34256.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[popular_forget_1] before=553, after=540, removed=13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 54210/54210 [00:01<00:00, 47298.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retain_98] before=54210, after=53893, removed=317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2765/2765 [00:00<00:00, 42368.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rare_forget_5] before=2765, after=2739, removed=26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2765/2765 [00:00<00:00, 43218.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[popular_forget_5] before=2765, after=2720, removed=45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 49786/49786 [00:01<00:00, 46724.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retain_90] before=49786, after=49516, removed=270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5531/5531 [00:00<00:00, 44149.22 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rare_forget_10] before=5531, after=5495, removed=36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5531/5531 [00:00<00:00, 44976.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[popular_forget_10] before=5531, after=5388, removed=143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 44254/44254 [00:00<00:00, 46776.88 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retain_80] before=44254, after=44092, removed=162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 8297/8297 [00:00<00:00, 45069.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rare_forget_15] before=8297, after=8253, removed=44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 8297/8297 [00:00<00:00, 46264.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[popular_forget_15] before=8297, after=8147, removed=150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 38722/38722 [00:00<00:00, 46831.75 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retain_70] before=38722, after=38575, removed=147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 54044/54044 [00:04<00:00, 12065.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates in 'know_intersection_retain_98' with gen_recall_Llama_8b_Instract>0.6: 20814\n",
      "Added split 'fast_retain' with 500 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# --- параметры ---\n",
    "REPO_IN = \"SwetieePawsss/TripUNLamb\"\n",
    "SOURCE_SPLIT = \"know_intersection_retain_98\"\n",
    "METRIC_COL = \"gen_recall_Llama_8b_Instract\"\n",
    "FAST_SIZE = 500\n",
    "SEED = 42\n",
    "\n",
    "# --- загрузка ---\n",
    "ds: DatasetDict = load_dataset(REPO_IN)\n",
    "print(\"Splits found:\", list(ds.keys()))\n",
    "\n",
    "# проверим наличие нужных колонок (ровно по именам)\n",
    "for split_name, dset in ds.items():\n",
    "    cols = set(dset.column_names)\n",
    "    if not {\"subject\", \"object\"} <= cols:\n",
    "        raise KeyError(f\"В сплите '{split_name}' нет колонок 'subject' и/или 'object'. Есть: {sorted(cols)}\")\n",
    "\n",
    "# --- 1) фильтрация во всех сплитах: оставляем только где subject != object ---\n",
    "filtered_splits = {}\n",
    "for split_name, dset in ds.items():\n",
    "    n_before = len(dset)\n",
    "    dset2 = dset.filter(lambda ex: ex[\"subject\"] != ex[\"object\"])\n",
    "    n_after = len(dset2)\n",
    "    print(f\"[{split_name}] before={n_before}, after={n_after}, removed={n_before - n_after}\")\n",
    "    filtered_splits[split_name] = dset2\n",
    "\n",
    "filtered = DatasetDict(filtered_splits)\n",
    "\n",
    "# --- 2) fast_retain из know_intersection_retain_98 с метрикой > 0.6 ---\n",
    "if SOURCE_SPLIT not in filtered:\n",
    "    raise KeyError(f\"Нет сплита '{SOURCE_SPLIT}'. Доступные: {list(filtered.keys())}\")\n",
    "\n",
    "if METRIC_COL not in filtered[SOURCE_SPLIT].column_names:\n",
    "    raise KeyError(f\"Нет колонки '{METRIC_COL}'. Есть: {filtered[SOURCE_SPLIT].column_names}\")\n",
    "\n",
    "# Оставляем кандидатов с метрикой > 0.6 (без нормализации, просто приводим к float для надёжности)\n",
    "def pred_gt_06(ex):\n",
    "    v = ex[METRIC_COL]\n",
    "    try:\n",
    "        v = float(v)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return v > 0.6\n",
    "\n",
    "cands = filtered[SOURCE_SPLIT].filter(pred_gt_06)\n",
    "print(f\"Candidates in '{SOURCE_SPLIT}' with {METRIC_COL}>0.6: {len(cands)}\")\n",
    "\n",
    "cands = cands.shuffle(seed=SEED)\n",
    "take_n = min(FAST_SIZE, len(cands))\n",
    "fast_retain = cands.select(range(take_n))\n",
    "\n",
    "filtered[\"fast_retain\"] = fast_retain\n",
    "print(f\"Added split 'fast_retain' with {len(fast_retain)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b98945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    full: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 54975\n",
       "    })\n",
       "    know_intersection_rare_forget_1: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 516\n",
       "    })\n",
       "    know_intersection_popular_forget_1: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 415\n",
       "    })\n",
       "    know_intersection_retain_98: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 54044\n",
       "    })\n",
       "    know_intersection_rare_forget_5: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 2548\n",
       "    })\n",
       "    know_intersection_popular_forget_5: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 2540\n",
       "    })\n",
       "    know_intersection_retain_90: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 52424\n",
       "    })\n",
       "    rare_forget_1: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 542\n",
       "    })\n",
       "    popular_forget_1: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 540\n",
       "    })\n",
       "    retain_98: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 53893\n",
       "    })\n",
       "    rare_forget_5: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 2739\n",
       "    })\n",
       "    popular_forget_5: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 2720\n",
       "    })\n",
       "    retain_90: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 49516\n",
       "    })\n",
       "    rare_forget_10: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 5495\n",
       "    })\n",
       "    popular_forget_10: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 5388\n",
       "    })\n",
       "    retain_80: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 44092\n",
       "    })\n",
       "    rare_forget_15: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 8253\n",
       "    })\n",
       "    popular_forget_15: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 8147\n",
       "    })\n",
       "    retain_70: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 38575\n",
       "    })\n",
       "    fast_retain: Dataset({\n",
       "        features: ['file', 'question', 'answer', 'subject', 'relation', 'object', 'PPL_Llama3_1-8B_Instruct', 'best_gen_Llama_8b_Instract', 'gen_recall_Llama_8b_Instract', 'bert_sim_Llama_8b_Instract', 'PPL_Llama3_2-3B_Instruct', 'best_gen_Llama_3b_Instract', 'gen_recall_Llama_3b_Instract', 'bert_sim_Llama_3b_Instract', 'PPL_Llama3_2-1B_Instruct', 'best_gen_Llama_1b_Instract', 'gen_recall_Llama_1b_Instract', 'bert_sim_Llama_1b_Instract', 'PPL_Gemma_7B_IT', 'best_gen_Gemma_7b_IT', 'gen_recall_Gemma_7b_IT', 'bert_sim_Gemma_7b_IT', 'PPL_Zephyr_7B_Beta', 'best_gen_Zephyr_7b_Beta', 'gen_recall_Zephyr_7b_Beta', 'bert_sim_Zephyr_7b_Beta', 'PPL_Phi3_5_mini_Instruct', 'best_gen_Phi3_5_mini_Instruct', 'gen_recall_Phi3_5_mini_Instruct', 'bert_sim_Phi3_5_mini_Instruct', 'subject_qid', 'object_qid', 'subject_popularity_sitelinks', 'object_popularity_sitelinks', 'popularity_sitelinks_sum'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd9a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:11<00:00,  4.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:13<00:00, 13.95s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 11.58ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.32s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 14.67ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:11<00:00,  4.85ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:13<00:00, 13.14s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  6.25ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.50s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  6.18ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.88s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 53/53 [00:10<00:00,  4.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.64s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 10.92ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.36s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 11.03ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 54/54 [00:11<00:00,  4.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.92s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  5.59ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  5.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.71s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 50/50 [00:10<00:00,  4.70ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.11s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:01<00:00,  5.68ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.25s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:01<00:00,  5.79ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.20s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 45/45 [00:09<00:00,  4.92ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.54s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:01<00:00,  5.48ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.88s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 9/9 [00:01<00:00,  5.52ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.39s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 39/39 [00:07<00:00,  4.89ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.41s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 11.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.30s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/SwetieePawsss/TripUNLamb/commit/97ef7a76641ad08b4e927058d8b905fa4290d248', commit_message='Upload dataset', commit_description='', oid='97ef7a76641ad08b4e927058d8b905fa4290d248', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/SwetieePawsss/TripUNLamb', endpoint='https://huggingface.co', repo_type='dataset', repo_id='SwetieePawsss/TripUNLamb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_splits.push_to_hub(\"SwetieePawsss/TripUNLamb\")\n",
    "filtered.push_to_hub(\"SwetieePawsss/TripUNLamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a1184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triplets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
