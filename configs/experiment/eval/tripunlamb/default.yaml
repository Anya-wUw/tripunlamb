# @package _global_

defaults:
  - override /model: Llama-3.1-8B-Instruct #Phi-3.5-mini-instruct #gemma-7b-it #Llama-3.2-1B-Instruct
  - override /eval: tripunlamb

forget_split: know_intersection #rare_forget10 #popular_forget10 #rare_forget5
retain_logs_path: ${retain_logs_path}
task_name: ${task_name} 

model:
  model_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct #microsoft/Phi-3.5-mini-instruct # #google/gemma-7b-it #open-unlearning/tofu_Llama-3.2-1B-Instruct_full #TODO add own pretrain

eval:
  tripunlamb:
    forget_split: ${forget_split}
    retain_logs_path: ${retain_logs_path}

# task_name: ???