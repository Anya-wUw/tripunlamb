# @package _global_

defaults:
  - override /model: Llama-3.1-8B-Instruct #Phi-3.5-mini-instruct #gemma-7b-it #Llama-3.1-8B-Instruct
  - override /trainer: finetune
  - override /data/datasets@data.train: tripunlamb_full #_full
  - override /eval: tripunlamb

mode: finetune
trainer:
  generation_kwargs:
    max_new_tokens: 8
    temperature: 0.0
    top_p: 1.0
    do_sample: false
    eos_token_id: ${model.tokenizer_args.pretrained_model_name_or_path}/<|eot_id|>  # опционально
  until: ["<|eot_id|>", "\n"]

  args:
    learning_rate: 2e-4 #2e-4
    num_train_epochs: 3 #2
    # warmup_epochs: 0.2
    # weight_decay: 0.01
    # max_grad_norm: 1.0

peft:
  lora:
    enabled: true
    r: 32 #32
    alpha: 64 #64
    dropout: 0.0 #0.05
    target_modules:
      - q_proj 
      - k_proj 
      - v_proj 
      - o_proj 
      

forget_split: know_intersection
# holdout_split: holdout10
retain_logs_path: null

eval:
  tripunlamb:
    forget_split: ${forget_split}
    # holdout_split: ${holdout_split}
    retain_logs_path: ${retain_logs_path}
    overwrite: true


task_name: tripunlamb_full_llama3-instruct 