# @package _global_

defaults:
  - override /model: Llama-3.1-8B-Instruct
  - override /trainer: finetune
  - override /data/datasets@data.train: PopQA_full #_full
  - override /eval: popqa

mode: finetune
trainer:
  args:
    learning_rate: 2e-4
    num_train_epochs: 3
    warmup_epochs: 0.2
    weight_decay: 0.01
    max_grad_norm: 1.0
    # embedding_learning_rate: 0.00003
    # lr_scheduler_type: cosine
    # warmup_steps: 16

peft:
  lora:
    enabled: true
    r: 32 #32
    alpha: 16 #64
    dropout: 0.0
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      # - gate_proj
      # - up_proj
      # - down_proj


forget_split: rare_forget10
# holdout_split: holdout10
retain_logs_path: null

eval:
  popqa:
    forget_split: ${forget_split}
    # holdout_split: ${holdout_split}
    retain_logs_path: ${retain_logs_path}
    overwrite: true


task_name: popqa_Llama-3.1-8B-Instruct_full