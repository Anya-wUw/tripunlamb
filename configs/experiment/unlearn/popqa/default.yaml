# @package _global_

defaults:
  - override /model: Llama-3.1-8B-Instruct #Phi-3.5-mini-instruct #gemma-7b-it #Llama-3.1-8B-Instruct #Llama-3.2-3B-Instruct
  - override /trainer: GradAscent
  - override /data: unlearn
  - override /data/datasets@data.forget: PopQA_popular_forget #PopQA_rare_forget
  - override /data/datasets@data.retain: PopQA_popular_retain #PopQA_rare_retain
  - override /eval: popqa

model:
  model_args:
    pretrained_model_name_or_path: meta-llama/Llama-3.1-8B-Instruct  #microsoft/Phi-3.5-mini-instruct #google/gemma-7b-it #/mnt/extremessd10tb/borisiuk/open-unlearning/saves/finetune/llama3.1-8b_full_2ep_ft_popqa #TODO: load popQA pretrain to  HF

forget_split: popular_forget10 #rare_forget10
retain_split: retain_intersection80 #rare_retain95
# holdout_split: holdout10
retain_logs_path: null
question_key: "question"

eval:
  popqa:
    forget_split: ${forget_split}
    retain_split: ${retain_split}
    retain_logs_path: ${retain_logs_path}
    overwrite: true
    question_key: ${question_key}
    
data:
  anchor: forget
  forget:
    PopQA_popular_forget:
      args:
        hf_args:
          split: ${forget_split}
  retain:
    PopQA_popular_retain:
      args:
        hf_args:
          split: ${retain_split}



trainer:
  generation_kwargs:
        max_new_tokens: 8
        top_p: 1.0
        temperature: 0.0
        do_sample: false
        eos_token_id: ${model.tokenizer_args.pretrained_model_name_or_path}/<|eot_id|> 
  until: ["<|eot_id|>", "\n"]
  args:
    warmup_epochs: 1.0 # custom parameter
    learning_rate: 3e-5 #1e-5
    weight_decay: 0.01
    num_train_epochs: 2

peft:
  lora:
    enabled: true
    r: 32 #32
    alpha: 64 #64
    dropout: 0.0
    target_modules:
      - q_proj 
      - k_proj 
      - v_proj 
      - o_proj 

task_name: ???